# Chapter 5: The mechanics of learning

## Key Concepts Learned

- Backpropagation implementation from scratch
- Different activation functions and when to use them
- Weight initialization strategies

## Implementations

- [✓] Basic MLP from scratch
- [✓] Book's CNN example
- [✓] My extension: Added dropout and batch normalization

## Experiments & Findings

1. **Learning Rate Sensitivity**: Compared SGD with different LRs on MNIST

   - Finding: LR=0.01 worked best for this architecture
   - [See notebook](experiments/lr-analysis.ipynb)

2. **Custom Dataset**: Applied the concepts to [specific dataset]
   - Modified architecture because...
   - Achieved X% accuracy

## Resources & References

- Book: Chapter 3, pages 45-78
- Additional reading: [Original batch norm paper](link)
- My blog post explaining the concepts: [link]
