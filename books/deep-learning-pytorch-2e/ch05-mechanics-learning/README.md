# Chapter 5: The Mechanics of Learning

## Key Concepts Learned

- PyTorch's autograd system and computational graphs
- Parameter updates and gradient descent mechanics
- Optimizers: SGD, Adam
- Learning rate
- How PyTorch tracks operations for backpropagation

## Implementations

- [✓] Manual gradient descent implementation (no optimizer)
- [✓] Custom optimizer from scratch mimicking SGD
- [✓] Comparison of different PyTorch optimizers

## Key Takeaways

- Understanding `loss.backward()` and what happens under the hood
- When to use `model.eval()` vs `torch.no_grad()`
- Why we need `optimizer.zero_grad()`
- The importance of the learning rate

## Resources & References

- Book: Chapter 5,
