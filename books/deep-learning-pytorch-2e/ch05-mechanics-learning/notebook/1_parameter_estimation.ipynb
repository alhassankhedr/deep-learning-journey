{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "142213bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0153d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7c89c",
   "metadata": {},
   "source": [
    "### Drawing the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31d06b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Measurement\")\n",
    "plt.ylabel(\"Temperature (Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "\n",
    "plt.savefig(\"temp_data_plot.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea2e27",
   "metadata": {},
   "source": [
    "The linear model y = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a058096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c88d0b",
   "metadata": {},
   "source": [
    "The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14bf3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diff = (t_p - t_c) ** 2\n",
    "    return squared_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71a3b4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9214db07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb6eb60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2979)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w + delta, b), t_c) - \n",
    "     loss_fn(model(t_u, w - delta, b), t_c)) / (2 * delta)\n",
    "\n",
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "068c0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eadf9672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4602.5000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b + delta), t_c) - \n",
    "     loss_fn(model(t_u, w, b - delta), t_c)) / (2 * delta)\n",
    "\n",
    "loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7be19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce922cd7",
   "metadata": {},
   "source": [
    "Derivative of the loss compared to its input (tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f810066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(tp, tc):\n",
    "    dsq_diffs = 2 * (tp - tc) / tp.size(0)\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cdc5c",
   "metadata": {},
   "source": [
    "Derivative of the model compared to its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18b3a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9347e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d2f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss {float(loss):.6f}')\n",
    "        print(f'Params: {params}')\n",
    "        print(f'Gradient: {grad}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c777a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Params: tensor([1.7761, 0.1064])\n",
      "Gradient: tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574917\n",
      "Params: tensor([2.0848, 0.1303])\n",
      "Gradient: tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "Params: tensor([2.2094, 0.1217])\n",
      "Gradient: tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "Params: tensor([2.2616, 0.1004])\n",
      "Gradient: tensor([-5.2218,  2.1327])\n",
      "Epoch 5, Loss 29.507149\n",
      "Params: tensor([2.2853, 0.0740])\n",
      "Gradient: tensor([-2.3715,  2.6310])\n",
      "Epoch 6, Loss 29.392456\n",
      "Params: tensor([2.2978, 0.0458])\n",
      "Gradient: tensor([-1.2492,  2.8241])\n",
      "Epoch 7, Loss 29.298828\n",
      "Params: tensor([2.3059, 0.0168])\n",
      "Gradient: tensor([-0.8071,  2.8970])\n",
      "Epoch 8, Loss 29.208717\n",
      "Params: tensor([ 2.3122, -0.0124])\n",
      "Gradient: tensor([-0.6325,  2.9227])\n",
      "Epoch 9, Loss 29.119413\n",
      "Params: tensor([ 2.3178, -0.0417])\n",
      "Gradient: tensor([-0.5633,  2.9298])\n",
      "Epoch 10, Loss 29.030489\n",
      "Params: tensor([ 2.3232, -0.0710])\n",
      "Gradient: tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941875\n",
      "Params: tensor([ 2.3284, -0.1003])\n",
      "Gradient: tensor([-0.5240,  2.9264])\n",
      "Epoch 12, Loss 28.853565\n",
      "Params: tensor([ 2.3336, -0.1295])\n",
      "Gradient: tensor([-0.5190,  2.9222])\n",
      "Epoch 13, Loss 28.765553\n",
      "Params: tensor([ 2.3388, -0.1587])\n",
      "Gradient: tensor([-0.5165,  2.9175])\n",
      "Epoch 14, Loss 28.677851\n",
      "Params: tensor([ 2.3439, -0.1878])\n",
      "Gradient: tensor([-0.5150,  2.9126])\n",
      "Epoch 15, Loss 28.590435\n",
      "Params: tensor([ 2.3491, -0.2169])\n",
      "Gradient: tensor([-0.5138,  2.9077])\n",
      "Epoch 16, Loss 28.503319\n",
      "Params: tensor([ 2.3542, -0.2459])\n",
      "Gradient: tensor([-0.5129,  2.9028])\n",
      "Epoch 17, Loss 28.416496\n",
      "Params: tensor([ 2.3593, -0.2749])\n",
      "Gradient: tensor([-0.5120,  2.8979])\n",
      "Epoch 18, Loss 28.329973\n",
      "Params: tensor([ 2.3644, -0.3038])\n",
      "Gradient: tensor([-0.5111,  2.8930])\n",
      "Epoch 19, Loss 28.243742\n",
      "Params: tensor([ 2.3695, -0.3327])\n",
      "Gradient: tensor([-0.5102,  2.8881])\n",
      "Epoch 20, Loss 28.157804\n",
      "Params: tensor([ 2.3746, -0.3615])\n",
      "Gradient: tensor([-0.5093,  2.8832])\n",
      "Epoch 21, Loss 28.072155\n",
      "Params: tensor([ 2.3797, -0.3903])\n",
      "Gradient: tensor([-0.5084,  2.8783])\n",
      "Epoch 22, Loss 27.986797\n",
      "Params: tensor([ 2.3848, -0.4190])\n",
      "Gradient: tensor([-0.5076,  2.8734])\n",
      "Epoch 23, Loss 27.901728\n",
      "Params: tensor([ 2.3899, -0.4477])\n",
      "Gradient: tensor([-0.5067,  2.8685])\n",
      "Epoch 24, Loss 27.816950\n",
      "Params: tensor([ 2.3949, -0.4763])\n",
      "Gradient: tensor([-0.5059,  2.8636])\n",
      "Epoch 25, Loss 27.732460\n",
      "Params: tensor([ 2.4000, -0.5049])\n",
      "Gradient: tensor([-0.5050,  2.8588])\n",
      "Epoch 26, Loss 27.648256\n",
      "Params: tensor([ 2.4050, -0.5335])\n",
      "Gradient: tensor([-0.5042,  2.8539])\n",
      "Epoch 27, Loss 27.564342\n",
      "Params: tensor([ 2.4101, -0.5620])\n",
      "Gradient: tensor([-0.5033,  2.8490])\n",
      "Epoch 28, Loss 27.480707\n",
      "Params: tensor([ 2.4151, -0.5904])\n",
      "Gradient: tensor([-0.5024,  2.8442])\n",
      "Epoch 29, Loss 27.397364\n",
      "Params: tensor([ 2.4201, -0.6188])\n",
      "Gradient: tensor([-0.5016,  2.8394])\n",
      "Epoch 30, Loss 27.314299\n",
      "Params: tensor([ 2.4251, -0.6471])\n",
      "Gradient: tensor([-0.5007,  2.8346])\n",
      "Epoch 31, Loss 27.231508\n",
      "Params: tensor([ 2.4301, -0.6754])\n",
      "Gradient: tensor([-0.4999,  2.8297])\n",
      "Epoch 32, Loss 27.149010\n",
      "Params: tensor([ 2.4351, -0.7037])\n",
      "Gradient: tensor([-0.4990,  2.8249])\n",
      "Epoch 33, Loss 27.066790\n",
      "Params: tensor([ 2.4401, -0.7319])\n",
      "Gradient: tensor([-0.4982,  2.8201])\n",
      "Epoch 34, Loss 26.984844\n",
      "Params: tensor([ 2.4450, -0.7600])\n",
      "Gradient: tensor([-0.4973,  2.8153])\n",
      "Epoch 35, Loss 26.903175\n",
      "Params: tensor([ 2.4500, -0.7881])\n",
      "Gradient: tensor([-0.4965,  2.8106])\n",
      "Epoch 36, Loss 26.821791\n",
      "Params: tensor([ 2.4550, -0.8162])\n",
      "Gradient: tensor([-0.4957,  2.8058])\n",
      "Epoch 37, Loss 26.740679\n",
      "Params: tensor([ 2.4599, -0.8442])\n",
      "Gradient: tensor([-0.4948,  2.8010])\n",
      "Epoch 38, Loss 26.659840\n",
      "Params: tensor([ 2.4649, -0.8722])\n",
      "Gradient: tensor([-0.4940,  2.7963])\n",
      "Epoch 39, Loss 26.579279\n",
      "Params: tensor([ 2.4698, -0.9001])\n",
      "Gradient: tensor([-0.4931,  2.7915])\n",
      "Epoch 40, Loss 26.498987\n",
      "Params: tensor([ 2.4747, -0.9280])\n",
      "Gradient: tensor([-0.4923,  2.7868])\n",
      "Epoch 41, Loss 26.418974\n",
      "Params: tensor([ 2.4796, -0.9558])\n",
      "Gradient: tensor([-0.4915,  2.7820])\n",
      "Epoch 42, Loss 26.339231\n",
      "Params: tensor([ 2.4845, -0.9836])\n",
      "Gradient: tensor([-0.4906,  2.7773])\n",
      "Epoch 43, Loss 26.259754\n",
      "Params: tensor([ 2.4894, -1.0113])\n",
      "Gradient: tensor([-0.4898,  2.7726])\n",
      "Epoch 44, Loss 26.180548\n",
      "Params: tensor([ 2.4943, -1.0390])\n",
      "Gradient: tensor([-0.4890,  2.7679])\n",
      "Epoch 45, Loss 26.101616\n",
      "Params: tensor([ 2.4992, -1.0666])\n",
      "Gradient: tensor([-0.4881,  2.7632])\n",
      "Epoch 46, Loss 26.022949\n",
      "Params: tensor([ 2.5041, -1.0942])\n",
      "Gradient: tensor([-0.4873,  2.7585])\n",
      "Epoch 47, Loss 25.944544\n",
      "Params: tensor([ 2.5089, -1.1217])\n",
      "Gradient: tensor([-0.4865,  2.7538])\n",
      "Epoch 48, Loss 25.866417\n",
      "Params: tensor([ 2.5138, -1.1492])\n",
      "Gradient: tensor([-0.4856,  2.7491])\n",
      "Epoch 49, Loss 25.788546\n",
      "Params: tensor([ 2.5186, -1.1766])\n",
      "Gradient: tensor([-0.4848,  2.7444])\n",
      "Epoch 50, Loss 25.710938\n",
      "Params: tensor([ 2.5235, -1.2040])\n",
      "Gradient: tensor([-0.4840,  2.7398])\n",
      "Epoch 51, Loss 25.633600\n",
      "Params: tensor([ 2.5283, -1.2314])\n",
      "Gradient: tensor([-0.4832,  2.7351])\n",
      "Epoch 52, Loss 25.556524\n",
      "Params: tensor([ 2.5331, -1.2587])\n",
      "Gradient: tensor([-0.4823,  2.7305])\n",
      "Epoch 53, Loss 25.479700\n",
      "Params: tensor([ 2.5379, -1.2860])\n",
      "Gradient: tensor([-0.4815,  2.7258])\n",
      "Epoch 54, Loss 25.403149\n",
      "Params: tensor([ 2.5428, -1.3132])\n",
      "Gradient: tensor([-0.4807,  2.7212])\n",
      "Epoch 55, Loss 25.326851\n",
      "Params: tensor([ 2.5476, -1.3403])\n",
      "Gradient: tensor([-0.4799,  2.7166])\n",
      "Epoch 56, Loss 25.250811\n",
      "Params: tensor([ 2.5523, -1.3675])\n",
      "Gradient: tensor([-0.4791,  2.7120])\n",
      "Epoch 57, Loss 25.175032\n",
      "Params: tensor([ 2.5571, -1.3945])\n",
      "Gradient: tensor([-0.4783,  2.7074])\n",
      "Epoch 58, Loss 25.099512\n",
      "Params: tensor([ 2.5619, -1.4216])\n",
      "Gradient: tensor([-0.4775,  2.7028])\n",
      "Epoch 59, Loss 25.024250\n",
      "Params: tensor([ 2.5667, -1.4485])\n",
      "Gradient: tensor([-0.4766,  2.6982])\n",
      "Epoch 60, Loss 24.949236\n",
      "Params: tensor([ 2.5714, -1.4755])\n",
      "Gradient: tensor([-0.4758,  2.6936])\n",
      "Epoch 61, Loss 24.874483\n",
      "Params: tensor([ 2.5762, -1.5024])\n",
      "Gradient: tensor([-0.4750,  2.6890])\n",
      "Epoch 62, Loss 24.799976\n",
      "Params: tensor([ 2.5809, -1.5292])\n",
      "Gradient: tensor([-0.4742,  2.6845])\n",
      "Epoch 63, Loss 24.725737\n",
      "Params: tensor([ 2.5857, -1.5560])\n",
      "Gradient: tensor([-0.4734,  2.6799])\n",
      "Epoch 64, Loss 24.651735\n",
      "Params: tensor([ 2.5904, -1.5828])\n",
      "Gradient: tensor([-0.4726,  2.6753])\n",
      "Epoch 65, Loss 24.577986\n",
      "Params: tensor([ 2.5951, -1.6095])\n",
      "Gradient: tensor([-0.4718,  2.6708])\n",
      "Epoch 66, Loss 24.504494\n",
      "Params: tensor([ 2.5998, -1.6361])\n",
      "Gradient: tensor([-0.4710,  2.6663])\n",
      "Epoch 67, Loss 24.431252\n",
      "Params: tensor([ 2.6045, -1.6628])\n",
      "Gradient: tensor([-0.4702,  2.6617])\n",
      "Epoch 68, Loss 24.358253\n",
      "Params: tensor([ 2.6092, -1.6893])\n",
      "Gradient: tensor([-0.4694,  2.6572])\n",
      "Epoch 69, Loss 24.285505\n",
      "Params: tensor([ 2.6139, -1.7159])\n",
      "Gradient: tensor([-0.4686,  2.6527])\n",
      "Epoch 70, Loss 24.212996\n",
      "Params: tensor([ 2.6186, -1.7423])\n",
      "Gradient: tensor([-0.4678,  2.6482])\n",
      "Epoch 71, Loss 24.140741\n",
      "Params: tensor([ 2.6232, -1.7688])\n",
      "Gradient: tensor([-0.4670,  2.6437])\n",
      "Epoch 72, Loss 24.068733\n",
      "Params: tensor([ 2.6279, -1.7952])\n",
      "Gradient: tensor([-0.4662,  2.6392])\n",
      "Epoch 73, Loss 23.996971\n",
      "Params: tensor([ 2.6326, -1.8215])\n",
      "Gradient: tensor([-0.4654,  2.6347])\n",
      "Epoch 74, Loss 23.925447\n",
      "Params: tensor([ 2.6372, -1.8478])\n",
      "Gradient: tensor([-0.4646,  2.6302])\n",
      "Epoch 75, Loss 23.854164\n",
      "Params: tensor([ 2.6418, -1.8741])\n",
      "Gradient: tensor([-0.4638,  2.6258])\n",
      "Epoch 76, Loss 23.783125\n",
      "Params: tensor([ 2.6465, -1.9003])\n",
      "Gradient: tensor([-0.4631,  2.6213])\n",
      "Epoch 77, Loss 23.712330\n",
      "Params: tensor([ 2.6511, -1.9265])\n",
      "Gradient: tensor([-0.4623,  2.6169])\n",
      "Epoch 78, Loss 23.641771\n",
      "Params: tensor([ 2.6557, -1.9526])\n",
      "Gradient: tensor([-0.4615,  2.6124])\n",
      "Epoch 79, Loss 23.571455\n",
      "Params: tensor([ 2.6603, -1.9787])\n",
      "Gradient: tensor([-0.4607,  2.6080])\n",
      "Epoch 80, Loss 23.501381\n",
      "Params: tensor([ 2.6649, -2.0047])\n",
      "Gradient: tensor([-0.4599,  2.6035])\n",
      "Epoch 81, Loss 23.431538\n",
      "Params: tensor([ 2.6695, -2.0307])\n",
      "Gradient: tensor([-0.4591,  2.5991])\n",
      "Epoch 82, Loss 23.361937\n",
      "Params: tensor([ 2.6741, -2.0566])\n",
      "Gradient: tensor([-0.4584,  2.5947])\n",
      "Epoch 83, Loss 23.292570\n",
      "Params: tensor([ 2.6787, -2.0825])\n",
      "Gradient: tensor([-0.4576,  2.5903])\n",
      "Epoch 84, Loss 23.223436\n",
      "Params: tensor([ 2.6832, -2.1084])\n",
      "Gradient: tensor([-0.4568,  2.5859])\n",
      "Epoch 85, Loss 23.154541\n",
      "Params: tensor([ 2.6878, -2.1342])\n",
      "Gradient: tensor([-0.4560,  2.5815])\n",
      "Epoch 86, Loss 23.085882\n",
      "Params: tensor([ 2.6923, -2.1600])\n",
      "Gradient: tensor([-0.4553,  2.5771])\n",
      "Epoch 87, Loss 23.017445\n",
      "Params: tensor([ 2.6969, -2.1857])\n",
      "Gradient: tensor([-0.4545,  2.5727])\n",
      "Epoch 88, Loss 22.949249\n",
      "Params: tensor([ 2.7014, -2.2114])\n",
      "Gradient: tensor([-0.4537,  2.5684])\n",
      "Epoch 89, Loss 22.881283\n",
      "Params: tensor([ 2.7060, -2.2370])\n",
      "Gradient: tensor([-0.4529,  2.5640])\n",
      "Epoch 90, Loss 22.813549\n",
      "Params: tensor([ 2.7105, -2.2626])\n",
      "Gradient: tensor([-0.4522,  2.5597])\n",
      "Epoch 91, Loss 22.746044\n",
      "Params: tensor([ 2.7150, -2.2882])\n",
      "Gradient: tensor([-0.4514,  2.5553])\n",
      "Epoch 92, Loss 22.678768\n",
      "Params: tensor([ 2.7195, -2.3137])\n",
      "Gradient: tensor([-0.4506,  2.5510])\n",
      "Epoch 93, Loss 22.611717\n",
      "Params: tensor([ 2.7240, -2.3392])\n",
      "Gradient: tensor([-0.4499,  2.5466])\n",
      "Epoch 94, Loss 22.544899\n",
      "Params: tensor([ 2.7285, -2.3646])\n",
      "Gradient: tensor([-0.4491,  2.5423])\n",
      "Epoch 95, Loss 22.478308\n",
      "Params: tensor([ 2.7330, -2.3900])\n",
      "Gradient: tensor([-0.4483,  2.5380])\n",
      "Epoch 96, Loss 22.411936\n",
      "Params: tensor([ 2.7374, -2.4153])\n",
      "Gradient: tensor([-0.4476,  2.5337])\n",
      "Epoch 97, Loss 22.345793\n",
      "Params: tensor([ 2.7419, -2.4406])\n",
      "Gradient: tensor([-0.4468,  2.5294])\n",
      "Epoch 98, Loss 22.279875\n",
      "Params: tensor([ 2.7464, -2.4658])\n",
      "Gradient: tensor([-0.4461,  2.5251])\n",
      "Epoch 99, Loss 22.214186\n",
      "Params: tensor([ 2.7508, -2.4910])\n",
      "Gradient: tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148712\n",
      "Params: tensor([ 2.7553, -2.5162])\n",
      "Gradient: tensor([-0.4446,  2.5165])\n",
      "Epoch 101, Loss 22.083464\n",
      "Params: tensor([ 2.7597, -2.5413])\n",
      "Gradient: tensor([-0.4438,  2.5122])\n",
      "Epoch 102, Loss 22.018438\n",
      "Params: tensor([ 2.7641, -2.5664])\n",
      "Gradient: tensor([-0.4430,  2.5080])\n",
      "Epoch 103, Loss 21.953630\n",
      "Params: tensor([ 2.7686, -2.5914])\n",
      "Gradient: tensor([-0.4423,  2.5037])\n",
      "Epoch 104, Loss 21.889046\n",
      "Params: tensor([ 2.7730, -2.6164])\n",
      "Gradient: tensor([-0.4415,  2.4994])\n",
      "Epoch 105, Loss 21.824677\n",
      "Params: tensor([ 2.7774, -2.6414])\n",
      "Gradient: tensor([-0.4408,  2.4952])\n",
      "Epoch 106, Loss 21.760530\n",
      "Params: tensor([ 2.7818, -2.6663])\n",
      "Gradient: tensor([-0.4400,  2.4910])\n",
      "Epoch 107, Loss 21.696600\n",
      "Params: tensor([ 2.7862, -2.6912])\n",
      "Gradient: tensor([-0.4393,  2.4867])\n",
      "Epoch 108, Loss 21.632881\n",
      "Params: tensor([ 2.7906, -2.7160])\n",
      "Gradient: tensor([-0.4385,  2.4825])\n",
      "Epoch 109, Loss 21.569391\n",
      "Params: tensor([ 2.7949, -2.7408])\n",
      "Gradient: tensor([-0.4378,  2.4783])\n",
      "Epoch 110, Loss 21.506104\n",
      "Params: tensor([ 2.7993, -2.7655])\n",
      "Gradient: tensor([-0.4370,  2.4741])\n",
      "Epoch 111, Loss 21.443037\n",
      "Params: tensor([ 2.8037, -2.7902])\n",
      "Gradient: tensor([-0.4363,  2.4699])\n",
      "Epoch 112, Loss 21.380188\n",
      "Params: tensor([ 2.8080, -2.8149])\n",
      "Gradient: tensor([-0.4356,  2.4657])\n",
      "Epoch 113, Loss 21.317547\n",
      "Params: tensor([ 2.8124, -2.8395])\n",
      "Gradient: tensor([-0.4348,  2.4615])\n",
      "Epoch 114, Loss 21.255117\n",
      "Params: tensor([ 2.8167, -2.8641])\n",
      "Gradient: tensor([-0.4341,  2.4573])\n",
      "Epoch 115, Loss 21.192905\n",
      "Params: tensor([ 2.8211, -2.8886])\n",
      "Gradient: tensor([-0.4334,  2.4531])\n",
      "Epoch 116, Loss 21.130899\n",
      "Params: tensor([ 2.8254, -2.9131])\n",
      "Gradient: tensor([-0.4326,  2.4490])\n",
      "Epoch 117, Loss 21.069105\n",
      "Params: tensor([ 2.8297, -2.9375])\n",
      "Gradient: tensor([-0.4319,  2.4448])\n",
      "Epoch 118, Loss 21.007524\n",
      "Params: tensor([ 2.8340, -2.9619])\n",
      "Gradient: tensor([-0.4311,  2.4407])\n",
      "Epoch 119, Loss 20.946150\n",
      "Params: tensor([ 2.8383, -2.9863])\n",
      "Gradient: tensor([-0.4304,  2.4365])\n",
      "Epoch 120, Loss 20.884983\n",
      "Params: tensor([ 2.8426, -3.0106])\n",
      "Gradient: tensor([-0.4297,  2.4324])\n",
      "Epoch 121, Loss 20.824024\n",
      "Params: tensor([ 2.8469, -3.0349])\n",
      "Gradient: tensor([-0.4290,  2.4282])\n",
      "Epoch 122, Loss 20.763273\n",
      "Params: tensor([ 2.8512, -3.0592])\n",
      "Gradient: tensor([-0.4282,  2.4241])\n",
      "Epoch 123, Loss 20.702726\n",
      "Params: tensor([ 2.8555, -3.0834])\n",
      "Gradient: tensor([-0.4275,  2.4200])\n",
      "Epoch 124, Loss 20.642384\n",
      "Params: tensor([ 2.8597, -3.1075])\n",
      "Gradient: tensor([-0.4268,  2.4159])\n",
      "Epoch 125, Loss 20.582251\n",
      "Params: tensor([ 2.8640, -3.1316])\n",
      "Gradient: tensor([-0.4261,  2.4118])\n",
      "Epoch 126, Loss 20.522322\n",
      "Params: tensor([ 2.8682, -3.1557])\n",
      "Gradient: tensor([-0.4253,  2.4077])\n",
      "Epoch 127, Loss 20.462593\n",
      "Params: tensor([ 2.8725, -3.1797])\n",
      "Gradient: tensor([-0.4246,  2.4036])\n",
      "Epoch 128, Loss 20.403069\n",
      "Params: tensor([ 2.8767, -3.2037])\n",
      "Gradient: tensor([-0.4239,  2.3995])\n",
      "Epoch 129, Loss 20.343744\n",
      "Params: tensor([ 2.8810, -3.2277])\n",
      "Gradient: tensor([-0.4232,  2.3954])\n",
      "Epoch 130, Loss 20.284622\n",
      "Params: tensor([ 2.8852, -3.2516])\n",
      "Gradient: tensor([-0.4224,  2.3914])\n",
      "Epoch 131, Loss 20.225702\n",
      "Params: tensor([ 2.8894, -3.2755])\n",
      "Gradient: tensor([-0.4217,  2.3873])\n",
      "Epoch 132, Loss 20.166979\n",
      "Params: tensor([ 2.8936, -3.2993])\n",
      "Gradient: tensor([-0.4210,  2.3832])\n",
      "Epoch 133, Loss 20.108459\n",
      "Params: tensor([ 2.8978, -3.3231])\n",
      "Gradient: tensor([-0.4203,  2.3792])\n",
      "Epoch 134, Loss 20.050137\n",
      "Params: tensor([ 2.9020, -3.3469])\n",
      "Gradient: tensor([-0.4196,  2.3752])\n",
      "Epoch 135, Loss 19.992012\n",
      "Params: tensor([ 2.9062, -3.3706])\n",
      "Gradient: tensor([-0.4189,  2.3711])\n",
      "Epoch 136, Loss 19.934086\n",
      "Params: tensor([ 2.9104, -3.3942])\n",
      "Gradient: tensor([-0.4182,  2.3671])\n",
      "Epoch 137, Loss 19.876350\n",
      "Params: tensor([ 2.9146, -3.4179])\n",
      "Gradient: tensor([-0.4174,  2.3631])\n",
      "Epoch 138, Loss 19.818823\n",
      "Params: tensor([ 2.9187, -3.4415])\n",
      "Gradient: tensor([-0.4167,  2.3591])\n",
      "Epoch 139, Loss 19.761478\n",
      "Params: tensor([ 2.9229, -3.4650])\n",
      "Gradient: tensor([-0.4160,  2.3551])\n",
      "Epoch 140, Loss 19.704334\n",
      "Params: tensor([ 2.9270, -3.4885])\n",
      "Gradient: tensor([-0.4153,  2.3510])\n",
      "Epoch 141, Loss 19.647387\n",
      "Params: tensor([ 2.9312, -3.5120])\n",
      "Gradient: tensor([-0.4146,  2.3471])\n",
      "Epoch 142, Loss 19.590626\n",
      "Params: tensor([ 2.9353, -3.5354])\n",
      "Gradient: tensor([-0.4139,  2.3431])\n",
      "Epoch 143, Loss 19.534063\n",
      "Params: tensor([ 2.9395, -3.5588])\n",
      "Gradient: tensor([-0.4132,  2.3391])\n",
      "Epoch 144, Loss 19.477690\n",
      "Params: tensor([ 2.9436, -3.5822])\n",
      "Gradient: tensor([-0.4125,  2.3351])\n",
      "Epoch 145, Loss 19.421507\n",
      "Params: tensor([ 2.9477, -3.6055])\n",
      "Gradient: tensor([-0.4118,  2.3311])\n",
      "Epoch 146, Loss 19.365515\n",
      "Params: tensor([ 2.9518, -3.6287])\n",
      "Gradient: tensor([-0.4111,  2.3272])\n",
      "Epoch 147, Loss 19.309715\n",
      "Params: tensor([ 2.9559, -3.6520])\n",
      "Gradient: tensor([-0.4104,  2.3232])\n",
      "Epoch 148, Loss 19.254107\n",
      "Params: tensor([ 2.9600, -3.6752])\n",
      "Gradient: tensor([-0.4097,  2.3193])\n",
      "Epoch 149, Loss 19.198685\n",
      "Params: tensor([ 2.9641, -3.6983])\n",
      "Gradient: tensor([-0.4090,  2.3153])\n",
      "Epoch 150, Loss 19.143448\n",
      "Params: tensor([ 2.9682, -3.7214])\n",
      "Gradient: tensor([-0.4083,  2.3114])\n",
      "Epoch 151, Loss 19.088402\n",
      "Params: tensor([ 2.9723, -3.7445])\n",
      "Gradient: tensor([-0.4076,  2.3075])\n",
      "Epoch 152, Loss 19.033545\n",
      "Params: tensor([ 2.9763, -3.7675])\n",
      "Gradient: tensor([-0.4069,  2.3036])\n",
      "Epoch 153, Loss 18.978870\n",
      "Params: tensor([ 2.9804, -3.7905])\n",
      "Gradient: tensor([-0.4062,  2.2997])\n",
      "Epoch 154, Loss 18.924377\n",
      "Params: tensor([ 2.9844, -3.8135])\n",
      "Gradient: tensor([-0.4056,  2.2957])\n",
      "Epoch 155, Loss 18.870081\n",
      "Params: tensor([ 2.9885, -3.8364])\n",
      "Gradient: tensor([-0.4049,  2.2918])\n",
      "Epoch 156, Loss 18.815960\n",
      "Params: tensor([ 2.9925, -3.8593])\n",
      "Gradient: tensor([-0.4042,  2.2880])\n",
      "Epoch 157, Loss 18.762024\n",
      "Params: tensor([ 2.9966, -3.8821])\n",
      "Gradient: tensor([-0.4035,  2.2841])\n",
      "Epoch 158, Loss 18.708271\n",
      "Params: tensor([ 3.0006, -3.9049])\n",
      "Gradient: tensor([-0.4028,  2.2802])\n",
      "Epoch 159, Loss 18.654699\n",
      "Params: tensor([ 3.0046, -3.9277])\n",
      "Gradient: tensor([-0.4021,  2.2763])\n",
      "Epoch 160, Loss 18.601315\n",
      "Params: tensor([ 3.0086, -3.9504])\n",
      "Gradient: tensor([-0.4014,  2.2724])\n",
      "Epoch 161, Loss 18.548109\n",
      "Params: tensor([ 3.0126, -3.9731])\n",
      "Gradient: tensor([-0.4007,  2.2686])\n",
      "Epoch 162, Loss 18.495085\n",
      "Params: tensor([ 3.0166, -3.9958])\n",
      "Gradient: tensor([-0.4001,  2.2647])\n",
      "Epoch 163, Loss 18.442236\n",
      "Params: tensor([ 3.0206, -4.0184])\n",
      "Gradient: tensor([-0.3994,  2.2609])\n",
      "Epoch 164, Loss 18.389568\n",
      "Params: tensor([ 3.0246, -4.0409])\n",
      "Gradient: tensor([-0.3987,  2.2570])\n",
      "Epoch 165, Loss 18.337080\n",
      "Params: tensor([ 3.0286, -4.0635])\n",
      "Gradient: tensor([-0.3980,  2.2532])\n",
      "Epoch 166, Loss 18.284777\n",
      "Params: tensor([ 3.0326, -4.0860])\n",
      "Gradient: tensor([-0.3974,  2.2494])\n",
      "Epoch 167, Loss 18.232643\n",
      "Params: tensor([ 3.0365, -4.1084])\n",
      "Gradient: tensor([-0.3967,  2.2456])\n",
      "Epoch 168, Loss 18.180687\n",
      "Params: tensor([ 3.0405, -4.1308])\n",
      "Gradient: tensor([-0.3960,  2.2417])\n",
      "Epoch 169, Loss 18.128904\n",
      "Params: tensor([ 3.0445, -4.1532])\n",
      "Gradient: tensor([-0.3953,  2.2379])\n",
      "Epoch 170, Loss 18.077301\n",
      "Params: tensor([ 3.0484, -4.1756])\n",
      "Gradient: tensor([-0.3947,  2.2341])\n",
      "Epoch 171, Loss 18.025877\n",
      "Params: tensor([ 3.0523, -4.1979])\n",
      "Gradient: tensor([-0.3940,  2.2303])\n",
      "Epoch 172, Loss 17.974623\n",
      "Params: tensor([ 3.0563, -4.2201])\n",
      "Gradient: tensor([-0.3933,  2.2266])\n",
      "Epoch 173, Loss 17.923546\n",
      "Params: tensor([ 3.0602, -4.2424])\n",
      "Gradient: tensor([-0.3927,  2.2228])\n",
      "Epoch 174, Loss 17.872641\n",
      "Params: tensor([ 3.0641, -4.2646])\n",
      "Gradient: tensor([-0.3920,  2.2190])\n",
      "Epoch 175, Loss 17.821909\n",
      "Params: tensor([ 3.0680, -4.2867])\n",
      "Gradient: tensor([-0.3913,  2.2152])\n",
      "Epoch 176, Loss 17.771345\n",
      "Params: tensor([ 3.0719, -4.3088])\n",
      "Gradient: tensor([-0.3907,  2.2115])\n",
      "Epoch 177, Loss 17.720953\n",
      "Params: tensor([ 3.0758, -4.3309])\n",
      "Gradient: tensor([-0.3900,  2.2077])\n",
      "Epoch 178, Loss 17.670738\n",
      "Params: tensor([ 3.0797, -4.3529])\n",
      "Gradient: tensor([-0.3893,  2.2040])\n",
      "Epoch 179, Loss 17.620689\n",
      "Params: tensor([ 3.0836, -4.3749])\n",
      "Gradient: tensor([-0.3887,  2.2002])\n",
      "Epoch 180, Loss 17.570814\n",
      "Params: tensor([ 3.0875, -4.3969])\n",
      "Gradient: tensor([-0.3880,  2.1965])\n",
      "Epoch 181, Loss 17.521105\n",
      "Params: tensor([ 3.0914, -4.4188])\n",
      "Gradient: tensor([-0.3873,  2.1927])\n",
      "Epoch 182, Loss 17.471563\n",
      "Params: tensor([ 3.0952, -4.4407])\n",
      "Gradient: tensor([-0.3867,  2.1890])\n",
      "Epoch 183, Loss 17.422192\n",
      "Params: tensor([ 3.0991, -4.4626])\n",
      "Gradient: tensor([-0.3860,  2.1853])\n",
      "Epoch 184, Loss 17.372993\n",
      "Params: tensor([ 3.1030, -4.4844])\n",
      "Gradient: tensor([-0.3854,  2.1816])\n",
      "Epoch 185, Loss 17.323954\n",
      "Params: tensor([ 3.1068, -4.5062])\n",
      "Gradient: tensor([-0.3847,  2.1779])\n",
      "Epoch 186, Loss 17.275085\n",
      "Params: tensor([ 3.1106, -4.5279])\n",
      "Gradient: tensor([-0.3841,  2.1742])\n",
      "Epoch 187, Loss 17.226379\n",
      "Params: tensor([ 3.1145, -4.5496])\n",
      "Gradient: tensor([-0.3834,  2.1705])\n",
      "Epoch 188, Loss 17.177839\n",
      "Params: tensor([ 3.1183, -4.5713])\n",
      "Gradient: tensor([-0.3828,  2.1668])\n",
      "Epoch 189, Loss 17.129467\n",
      "Params: tensor([ 3.1221, -4.5929])\n",
      "Gradient: tensor([-0.3821,  2.1631])\n",
      "Epoch 190, Loss 17.081255\n",
      "Params: tensor([ 3.1259, -4.6145])\n",
      "Gradient: tensor([-0.3815,  2.1594])\n",
      "Epoch 191, Loss 17.033211\n",
      "Params: tensor([ 3.1298, -4.6361])\n",
      "Gradient: tensor([-0.3808,  2.1558])\n",
      "Epoch 192, Loss 16.985329\n",
      "Params: tensor([ 3.1336, -4.6576])\n",
      "Gradient: tensor([-0.3802,  2.1521])\n",
      "Epoch 193, Loss 16.937605\n",
      "Params: tensor([ 3.1374, -4.6791])\n",
      "Gradient: tensor([-0.3795,  2.1485])\n",
      "Epoch 194, Loss 16.890049\n",
      "Params: tensor([ 3.1411, -4.7005])\n",
      "Gradient: tensor([-0.3789,  2.1448])\n",
      "Epoch 195, Loss 16.842651\n",
      "Params: tensor([ 3.1449, -4.7219])\n",
      "Gradient: tensor([-0.3782,  2.1412])\n",
      "Epoch 196, Loss 16.795416\n",
      "Params: tensor([ 3.1487, -4.7433])\n",
      "Gradient: tensor([-0.3776,  2.1375])\n",
      "Epoch 197, Loss 16.748339\n",
      "Params: tensor([ 3.1525, -4.7646])\n",
      "Gradient: tensor([-0.3769,  2.1339])\n",
      "Epoch 198, Loss 16.701422\n",
      "Params: tensor([ 3.1562, -4.7859])\n",
      "Gradient: tensor([-0.3763,  2.1303])\n",
      "Epoch 199, Loss 16.654665\n",
      "Params: tensor([ 3.1600, -4.8072])\n",
      "Gradient: tensor([-0.3757,  2.1267])\n",
      "Epoch 200, Loss 16.608065\n",
      "Params: tensor([ 3.1637, -4.8284])\n",
      "Gradient: tensor([-0.3750,  2.1230])\n",
      "Epoch 201, Loss 16.561626\n",
      "Params: tensor([ 3.1675, -4.8496])\n",
      "Gradient: tensor([-0.3744,  2.1194])\n",
      "Epoch 202, Loss 16.515343\n",
      "Params: tensor([ 3.1712, -4.8708])\n",
      "Gradient: tensor([-0.3738,  2.1158])\n",
      "Epoch 203, Loss 16.469219\n",
      "Params: tensor([ 3.1750, -4.8919])\n",
      "Gradient: tensor([-0.3731,  2.1122])\n",
      "Epoch 204, Loss 16.423250\n",
      "Params: tensor([ 3.1787, -4.9130])\n",
      "Gradient: tensor([-0.3725,  2.1087])\n",
      "Epoch 205, Loss 16.377439\n",
      "Params: tensor([ 3.1824, -4.9341])\n",
      "Gradient: tensor([-0.3719,  2.1051])\n",
      "Epoch 206, Loss 16.331779\n",
      "Params: tensor([ 3.1861, -4.9551])\n",
      "Gradient: tensor([-0.3712,  2.1015])\n",
      "Epoch 207, Loss 16.286278\n",
      "Params: tensor([ 3.1898, -4.9760])\n",
      "Gradient: tensor([-0.3706,  2.0979])\n",
      "Epoch 208, Loss 16.240934\n",
      "Params: tensor([ 3.1935, -4.9970])\n",
      "Gradient: tensor([-0.3700,  2.0944])\n",
      "Epoch 209, Loss 16.195738\n",
      "Params: tensor([ 3.1972, -5.0179])\n",
      "Gradient: tensor([-0.3693,  2.0908])\n",
      "Epoch 210, Loss 16.150698\n",
      "Params: tensor([ 3.2009, -5.0388])\n",
      "Gradient: tensor([-0.3687,  2.0872])\n",
      "Epoch 211, Loss 16.105810\n",
      "Params: tensor([ 3.2046, -5.0596])\n",
      "Gradient: tensor([-0.3681,  2.0837])\n",
      "Epoch 212, Loss 16.061071\n",
      "Params: tensor([ 3.2082, -5.0804])\n",
      "Gradient: tensor([-0.3675,  2.0802])\n",
      "Epoch 213, Loss 16.016491\n",
      "Params: tensor([ 3.2119, -5.1012])\n",
      "Gradient: tensor([-0.3668,  2.0766])\n",
      "Epoch 214, Loss 15.972060\n",
      "Params: tensor([ 3.2156, -5.1219])\n",
      "Gradient: tensor([-0.3662,  2.0731])\n",
      "Epoch 215, Loss 15.927780\n",
      "Params: tensor([ 3.2192, -5.1426])\n",
      "Gradient: tensor([-0.3656,  2.0696])\n",
      "Epoch 216, Loss 15.883651\n",
      "Params: tensor([ 3.2229, -5.1633])\n",
      "Gradient: tensor([-0.3650,  2.0661])\n",
      "Epoch 217, Loss 15.839667\n",
      "Params: tensor([ 3.2265, -5.1839])\n",
      "Gradient: tensor([-0.3644,  2.0626])\n",
      "Epoch 218, Loss 15.795835\n",
      "Params: tensor([ 3.2302, -5.2045])\n",
      "Gradient: tensor([-0.3637,  2.0591])\n",
      "Epoch 219, Loss 15.752155\n",
      "Params: tensor([ 3.2338, -5.2250])\n",
      "Gradient: tensor([-0.3631,  2.0556])\n",
      "Epoch 220, Loss 15.708621\n",
      "Params: tensor([ 3.2374, -5.2456])\n",
      "Gradient: tensor([-0.3625,  2.0521])\n",
      "Epoch 221, Loss 15.665234\n",
      "Params: tensor([ 3.2410, -5.2660])\n",
      "Gradient: tensor([-0.3619,  2.0486])\n",
      "Epoch 222, Loss 15.621993\n",
      "Params: tensor([ 3.2447, -5.2865])\n",
      "Gradient: tensor([-0.3613,  2.0451])\n",
      "Epoch 223, Loss 15.578899\n",
      "Params: tensor([ 3.2483, -5.3069])\n",
      "Gradient: tensor([-0.3607,  2.0416])\n",
      "Epoch 224, Loss 15.535953\n",
      "Params: tensor([ 3.2519, -5.3273])\n",
      "Gradient: tensor([-0.3600,  2.0382])\n",
      "Epoch 225, Loss 15.493153\n",
      "Params: tensor([ 3.2555, -5.3476])\n",
      "Gradient: tensor([-0.3594,  2.0347])\n",
      "Epoch 226, Loss 15.450500\n",
      "Params: tensor([ 3.2590, -5.3680])\n",
      "Gradient: tensor([-0.3588,  2.0312])\n",
      "Epoch 227, Loss 15.407981\n",
      "Params: tensor([ 3.2626, -5.3882])\n",
      "Gradient: tensor([-0.3582,  2.0278])\n",
      "Epoch 228, Loss 15.365623\n",
      "Params: tensor([ 3.2662, -5.4085])\n",
      "Gradient: tensor([-0.3576,  2.0243])\n",
      "Epoch 229, Loss 15.323398\n",
      "Params: tensor([ 3.2698, -5.4287])\n",
      "Gradient: tensor([-0.3570,  2.0209])\n",
      "Epoch 230, Loss 15.281317\n",
      "Params: tensor([ 3.2733, -5.4489])\n",
      "Gradient: tensor([-0.3564,  2.0175])\n",
      "Epoch 231, Loss 15.239381\n",
      "Params: tensor([ 3.2769, -5.4690])\n",
      "Gradient: tensor([-0.3558,  2.0140])\n",
      "Epoch 232, Loss 15.197587\n",
      "Params: tensor([ 3.2804, -5.4891])\n",
      "Gradient: tensor([-0.3552,  2.0106])\n",
      "Epoch 233, Loss 15.155941\n",
      "Params: tensor([ 3.2840, -5.5092])\n",
      "Gradient: tensor([-0.3546,  2.0072])\n",
      "Epoch 234, Loss 15.114427\n",
      "Params: tensor([ 3.2875, -5.5292])\n",
      "Gradient: tensor([-0.3540,  2.0038])\n",
      "Epoch 235, Loss 15.073056\n",
      "Params: tensor([ 3.2911, -5.5492])\n",
      "Gradient: tensor([-0.3534,  2.0004])\n",
      "Epoch 236, Loss 15.031829\n",
      "Params: tensor([ 3.2946, -5.5692])\n",
      "Gradient: tensor([-0.3528,  1.9970])\n",
      "Epoch 237, Loss 14.990739\n",
      "Params: tensor([ 3.2981, -5.5891])\n",
      "Gradient: tensor([-0.3522,  1.9936])\n",
      "Epoch 238, Loss 14.949787\n",
      "Params: tensor([ 3.3016, -5.6090])\n",
      "Gradient: tensor([-0.3516,  1.9902])\n",
      "Epoch 239, Loss 14.908975\n",
      "Params: tensor([ 3.3051, -5.6289])\n",
      "Gradient: tensor([-0.3510,  1.9868])\n",
      "Epoch 240, Loss 14.868305\n",
      "Params: tensor([ 3.3086, -5.6487])\n",
      "Gradient: tensor([-0.3504,  1.9835])\n",
      "Epoch 241, Loss 14.827770\n",
      "Params: tensor([ 3.3121, -5.6685])\n",
      "Gradient: tensor([-0.3498,  1.9801])\n",
      "Epoch 242, Loss 14.787373\n",
      "Params: tensor([ 3.3156, -5.6883])\n",
      "Gradient: tensor([-0.3492,  1.9767])\n",
      "Epoch 243, Loss 14.747115\n",
      "Params: tensor([ 3.3191, -5.7080])\n",
      "Gradient: tensor([-0.3486,  1.9734])\n",
      "Epoch 244, Loss 14.706992\n",
      "Params: tensor([ 3.3226, -5.7277])\n",
      "Gradient: tensor([-0.3480,  1.9700])\n",
      "Epoch 245, Loss 14.667005\n",
      "Params: tensor([ 3.3261, -5.7474])\n",
      "Gradient: tensor([-0.3474,  1.9667])\n",
      "Epoch 246, Loss 14.627152\n",
      "Params: tensor([ 3.3295, -5.7670])\n",
      "Gradient: tensor([-0.3468,  1.9633])\n",
      "Epoch 247, Loss 14.587439\n",
      "Params: tensor([ 3.3330, -5.7866])\n",
      "Gradient: tensor([-0.3462,  1.9600])\n",
      "Epoch 248, Loss 14.547857\n",
      "Params: tensor([ 3.3365, -5.8062])\n",
      "Gradient: tensor([-0.3457,  1.9567])\n",
      "Epoch 249, Loss 14.508414\n",
      "Params: tensor([ 3.3399, -5.8257])\n",
      "Gradient: tensor([-0.3451,  1.9533])\n",
      "Epoch 250, Loss 14.469102\n",
      "Params: tensor([ 3.3434, -5.8452])\n",
      "Gradient: tensor([-0.3445,  1.9500])\n",
      "Epoch 251, Loss 14.429920\n",
      "Params: tensor([ 3.3468, -5.8647])\n",
      "Gradient: tensor([-0.3439,  1.9467])\n",
      "Epoch 252, Loss 14.390875\n",
      "Params: tensor([ 3.3502, -5.8841])\n",
      "Gradient: tensor([-0.3433,  1.9434])\n",
      "Epoch 253, Loss 14.351962\n",
      "Params: tensor([ 3.3537, -5.9035])\n",
      "Gradient: tensor([-0.3427,  1.9401])\n",
      "Epoch 254, Loss 14.313181\n",
      "Params: tensor([ 3.3571, -5.9229])\n",
      "Gradient: tensor([-0.3421,  1.9368])\n",
      "Epoch 255, Loss 14.274529\n",
      "Params: tensor([ 3.3605, -5.9422])\n",
      "Gradient: tensor([-0.3416,  1.9335])\n",
      "Epoch 256, Loss 14.236011\n",
      "Params: tensor([ 3.3639, -5.9615])\n",
      "Gradient: tensor([-0.3410,  1.9302])\n",
      "Epoch 257, Loss 14.197623\n",
      "Params: tensor([ 3.3673, -5.9808])\n",
      "Gradient: tensor([-0.3404,  1.9269])\n",
      "Epoch 258, Loss 14.159367\n",
      "Params: tensor([ 3.3707, -6.0000])\n",
      "Gradient: tensor([-0.3398,  1.9237])\n",
      "Epoch 259, Loss 14.121237\n",
      "Params: tensor([ 3.3741, -6.0192])\n",
      "Gradient: tensor([-0.3392,  1.9204])\n",
      "Epoch 260, Loss 14.083238\n",
      "Params: tensor([ 3.3775, -6.0384])\n",
      "Gradient: tensor([-0.3387,  1.9171])\n",
      "Epoch 261, Loss 14.045368\n",
      "Params: tensor([ 3.3809, -6.0576])\n",
      "Gradient: tensor([-0.3381,  1.9139])\n",
      "Epoch 262, Loss 14.007629\n",
      "Params: tensor([ 3.3842, -6.0767])\n",
      "Gradient: tensor([-0.3375,  1.9106])\n",
      "Epoch 263, Loss 13.970017\n",
      "Params: tensor([ 3.3876, -6.0957])\n",
      "Gradient: tensor([-0.3369,  1.9074])\n",
      "Epoch 264, Loss 13.932531\n",
      "Params: tensor([ 3.3910, -6.1148])\n",
      "Gradient: tensor([-0.3364,  1.9041])\n",
      "Epoch 265, Loss 13.895175\n",
      "Params: tensor([ 3.3943, -6.1338])\n",
      "Gradient: tensor([-0.3358,  1.9009])\n",
      "Epoch 266, Loss 13.857944\n",
      "Params: tensor([ 3.3977, -6.1528])\n",
      "Gradient: tensor([-0.3352,  1.8977])\n",
      "Epoch 267, Loss 13.820841\n",
      "Params: tensor([ 3.4010, -6.1717])\n",
      "Gradient: tensor([-0.3347,  1.8945])\n",
      "Epoch 268, Loss 13.783858\n",
      "Params: tensor([ 3.4044, -6.1906])\n",
      "Gradient: tensor([-0.3341,  1.8912])\n",
      "Epoch 269, Loss 13.747009\n",
      "Params: tensor([ 3.4077, -6.2095])\n",
      "Gradient: tensor([-0.3335,  1.8880])\n",
      "Epoch 270, Loss 13.710280\n",
      "Params: tensor([ 3.4110, -6.2284])\n",
      "Gradient: tensor([-0.3330,  1.8848])\n",
      "Epoch 271, Loss 13.673678\n",
      "Params: tensor([ 3.4144, -6.2472])\n",
      "Gradient: tensor([-0.3324,  1.8816])\n",
      "Epoch 272, Loss 13.637198\n",
      "Params: tensor([ 3.4177, -6.2660])\n",
      "Gradient: tensor([-0.3318,  1.8784])\n",
      "Epoch 273, Loss 13.600845\n",
      "Params: tensor([ 3.4210, -6.2847])\n",
      "Gradient: tensor([-0.3313,  1.8752])\n",
      "Epoch 274, Loss 13.564611\n",
      "Params: tensor([ 3.4243, -6.3034])\n",
      "Gradient: tensor([-0.3307,  1.8721])\n",
      "Epoch 275, Loss 13.528502\n",
      "Params: tensor([ 3.4276, -6.3221])\n",
      "Gradient: tensor([-0.3301,  1.8689])\n",
      "Epoch 276, Loss 13.492517\n",
      "Params: tensor([ 3.4309, -6.3408])\n",
      "Gradient: tensor([-0.3296,  1.8657])\n",
      "Epoch 277, Loss 13.456654\n",
      "Params: tensor([ 3.4342, -6.3594])\n",
      "Gradient: tensor([-0.3290,  1.8625])\n",
      "Epoch 278, Loss 13.420913\n",
      "Params: tensor([ 3.4375, -6.3780])\n",
      "Gradient: tensor([-0.3285,  1.8594])\n",
      "Epoch 279, Loss 13.385290\n",
      "Params: tensor([ 3.4407, -6.3965])\n",
      "Gradient: tensor([-0.3279,  1.8562])\n",
      "Epoch 280, Loss 13.349790\n",
      "Params: tensor([ 3.4440, -6.4151])\n",
      "Gradient: tensor([-0.3273,  1.8530])\n",
      "Epoch 281, Loss 13.314414\n",
      "Params: tensor([ 3.4473, -6.4336])\n",
      "Gradient: tensor([-0.3268,  1.8499])\n",
      "Epoch 282, Loss 13.279153\n",
      "Params: tensor([ 3.4506, -6.4520])\n",
      "Gradient: tensor([-0.3262,  1.8468])\n",
      "Epoch 283, Loss 13.244013\n",
      "Params: tensor([ 3.4538, -6.4705])\n",
      "Gradient: tensor([-0.3257,  1.8436])\n",
      "Epoch 284, Loss 13.208991\n",
      "Params: tensor([ 3.4571, -6.4889])\n",
      "Gradient: tensor([-0.3251,  1.8405])\n",
      "Epoch 285, Loss 13.174090\n",
      "Params: tensor([ 3.4603, -6.5073])\n",
      "Gradient: tensor([-0.3246,  1.8374])\n",
      "Epoch 286, Loss 13.139307\n",
      "Params: tensor([ 3.4635, -6.5256])\n",
      "Gradient: tensor([-0.3240,  1.8342])\n",
      "Epoch 287, Loss 13.104644\n",
      "Params: tensor([ 3.4668, -6.5439])\n",
      "Gradient: tensor([-0.3235,  1.8311])\n",
      "Epoch 288, Loss 13.070099\n",
      "Params: tensor([ 3.4700, -6.5622])\n",
      "Gradient: tensor([-0.3229,  1.8280])\n",
      "Epoch 289, Loss 13.035666\n",
      "Params: tensor([ 3.4732, -6.5804])\n",
      "Gradient: tensor([-0.3224,  1.8249])\n",
      "Epoch 290, Loss 13.001354\n",
      "Params: tensor([ 3.4765, -6.5987])\n",
      "Gradient: tensor([-0.3218,  1.8218])\n",
      "Epoch 291, Loss 12.967156\n",
      "Params: tensor([ 3.4797, -6.6168])\n",
      "Gradient: tensor([-0.3213,  1.8187])\n",
      "Epoch 292, Loss 12.933078\n",
      "Params: tensor([ 3.4829, -6.6350])\n",
      "Gradient: tensor([-0.3207,  1.8156])\n",
      "Epoch 293, Loss 12.899110\n",
      "Params: tensor([ 3.4861, -6.6531])\n",
      "Gradient: tensor([-0.3202,  1.8125])\n",
      "Epoch 294, Loss 12.865265\n",
      "Params: tensor([ 3.4893, -6.6712])\n",
      "Gradient: tensor([-0.3197,  1.8095])\n",
      "Epoch 295, Loss 12.831527\n",
      "Params: tensor([ 3.4925, -6.6893])\n",
      "Gradient: tensor([-0.3191,  1.8064])\n",
      "Epoch 296, Loss 12.797909\n",
      "Params: tensor([ 3.4956, -6.7073])\n",
      "Gradient: tensor([-0.3186,  1.8033])\n",
      "Epoch 297, Loss 12.764399\n",
      "Params: tensor([ 3.4988, -6.7253])\n",
      "Gradient: tensor([-0.3180,  1.8003])\n",
      "Epoch 298, Loss 12.731009\n",
      "Params: tensor([ 3.5020, -6.7433])\n",
      "Gradient: tensor([-0.3175,  1.7972])\n",
      "Epoch 299, Loss 12.697729\n",
      "Params: tensor([ 3.5052, -6.7612])\n",
      "Gradient: tensor([-0.3169,  1.7941])\n",
      "Epoch 300, Loss 12.664565\n",
      "Params: tensor([ 3.5083, -6.7791])\n",
      "Gradient: tensor([-0.3164,  1.7911])\n",
      "Epoch 301, Loss 12.631513\n",
      "Params: tensor([ 3.5115, -6.7970])\n",
      "Gradient: tensor([-0.3159,  1.7881])\n",
      "Epoch 302, Loss 12.598572\n",
      "Params: tensor([ 3.5146, -6.8149])\n",
      "Gradient: tensor([-0.3153,  1.7850])\n",
      "Epoch 303, Loss 12.565742\n",
      "Params: tensor([ 3.5178, -6.8327])\n",
      "Gradient: tensor([-0.3148,  1.7820])\n",
      "Epoch 304, Loss 12.533026\n",
      "Params: tensor([ 3.5209, -6.8505])\n",
      "Gradient: tensor([-0.3143,  1.7790])\n",
      "Epoch 305, Loss 12.500416\n",
      "Params: tensor([ 3.5241, -6.8682])\n",
      "Gradient: tensor([-0.3137,  1.7759])\n",
      "Epoch 306, Loss 12.467920\n",
      "Params: tensor([ 3.5272, -6.8860])\n",
      "Gradient: tensor([-0.3132,  1.7729])\n",
      "Epoch 307, Loss 12.435534\n",
      "Params: tensor([ 3.5303, -6.9037])\n",
      "Gradient: tensor([-0.3127,  1.7699])\n",
      "Epoch 308, Loss 12.403259\n",
      "Params: tensor([ 3.5335, -6.9213])\n",
      "Gradient: tensor([-0.3121,  1.7669])\n",
      "Epoch 309, Loss 12.371093\n",
      "Params: tensor([ 3.5366, -6.9390])\n",
      "Gradient: tensor([-0.3116,  1.7639])\n",
      "Epoch 310, Loss 12.339036\n",
      "Params: tensor([ 3.5397, -6.9566])\n",
      "Gradient: tensor([-0.3111,  1.7609])\n",
      "Epoch 311, Loss 12.307087\n",
      "Params: tensor([ 3.5428, -6.9742])\n",
      "Gradient: tensor([-0.3105,  1.7579])\n",
      "Epoch 312, Loss 12.275249\n",
      "Params: tensor([ 3.5459, -6.9917])\n",
      "Gradient: tensor([-0.3100,  1.7549])\n",
      "Epoch 313, Loss 12.243514\n",
      "Params: tensor([ 3.5490, -7.0092])\n",
      "Gradient: tensor([-0.3095,  1.7519])\n",
      "Epoch 314, Loss 12.211892\n",
      "Params: tensor([ 3.5521, -7.0267])\n",
      "Gradient: tensor([-0.3090,  1.7490])\n",
      "Epoch 315, Loss 12.180375\n",
      "Params: tensor([ 3.5552, -7.0442])\n",
      "Gradient: tensor([-0.3084,  1.7460])\n",
      "Epoch 316, Loss 12.148965\n",
      "Params: tensor([ 3.5582, -7.0616])\n",
      "Gradient: tensor([-0.3079,  1.7430])\n",
      "Epoch 317, Loss 12.117664\n",
      "Params: tensor([ 3.5613, -7.0790])\n",
      "Gradient: tensor([-0.3074,  1.7401])\n",
      "Epoch 318, Loss 12.086465\n",
      "Params: tensor([ 3.5644, -7.0964])\n",
      "Gradient: tensor([-0.3069,  1.7371])\n",
      "Epoch 319, Loss 12.055377\n",
      "Params: tensor([ 3.5674, -7.1137])\n",
      "Gradient: tensor([-0.3063,  1.7342])\n",
      "Epoch 320, Loss 12.024389\n",
      "Params: tensor([ 3.5705, -7.1310])\n",
      "Gradient: tensor([-0.3058,  1.7312])\n",
      "Epoch 321, Loss 11.993514\n",
      "Params: tensor([ 3.5736, -7.1483])\n",
      "Gradient: tensor([-0.3053,  1.7283])\n",
      "Epoch 322, Loss 11.962734\n",
      "Params: tensor([ 3.5766, -7.1656])\n",
      "Gradient: tensor([-0.3048,  1.7253])\n",
      "Epoch 323, Loss 11.932062\n",
      "Params: tensor([ 3.5796, -7.1828])\n",
      "Gradient: tensor([-0.3043,  1.7224])\n",
      "Epoch 324, Loss 11.901496\n",
      "Params: tensor([ 3.5827, -7.2000])\n",
      "Gradient: tensor([-0.3038,  1.7195])\n",
      "Epoch 325, Loss 11.871033\n",
      "Params: tensor([ 3.5857, -7.2172])\n",
      "Gradient: tensor([-0.3032,  1.7166])\n",
      "Epoch 326, Loss 11.840673\n",
      "Params: tensor([ 3.5887, -7.2343])\n",
      "Gradient: tensor([-0.3027,  1.7136])\n",
      "Epoch 327, Loss 11.810419\n",
      "Params: tensor([ 3.5918, -7.2514])\n",
      "Gradient: tensor([-0.3022,  1.7107])\n",
      "Epoch 328, Loss 11.780264\n",
      "Params: tensor([ 3.5948, -7.2685])\n",
      "Gradient: tensor([-0.3017,  1.7078])\n",
      "Epoch 329, Loss 11.750211\n",
      "Params: tensor([ 3.5978, -7.2855])\n",
      "Gradient: tensor([-0.3012,  1.7049])\n",
      "Epoch 330, Loss 11.720262\n",
      "Params: tensor([ 3.6008, -7.3026])\n",
      "Gradient: tensor([-0.3007,  1.7020])\n",
      "Epoch 331, Loss 11.690415\n",
      "Params: tensor([ 3.6038, -7.3196])\n",
      "Gradient: tensor([-0.3002,  1.6991])\n",
      "Epoch 332, Loss 11.660668\n",
      "Params: tensor([ 3.6068, -7.3365])\n",
      "Gradient: tensor([-0.2996,  1.6963])\n",
      "Epoch 333, Loss 11.631023\n",
      "Params: tensor([ 3.6098, -7.3534])\n",
      "Gradient: tensor([-0.2991,  1.6934])\n",
      "Epoch 334, Loss 11.601476\n",
      "Params: tensor([ 3.6128, -7.3704])\n",
      "Gradient: tensor([-0.2986,  1.6905])\n",
      "Epoch 335, Loss 11.572034\n",
      "Params: tensor([ 3.6158, -7.3872])\n",
      "Gradient: tensor([-0.2981,  1.6876])\n",
      "Epoch 336, Loss 11.542689\n",
      "Params: tensor([ 3.6187, -7.4041])\n",
      "Gradient: tensor([-0.2976,  1.6848])\n",
      "Epoch 337, Loss 11.513443\n",
      "Params: tensor([ 3.6217, -7.4209])\n",
      "Gradient: tensor([-0.2971,  1.6819])\n",
      "Epoch 338, Loss 11.484297\n",
      "Params: tensor([ 3.6247, -7.4377])\n",
      "Gradient: tensor([-0.2966,  1.6790])\n",
      "Epoch 339, Loss 11.455253\n",
      "Params: tensor([ 3.6276, -7.4544])\n",
      "Gradient: tensor([-0.2961,  1.6762])\n",
      "Epoch 340, Loss 11.426303\n",
      "Params: tensor([ 3.6306, -7.4712])\n",
      "Gradient: tensor([-0.2956,  1.6733])\n",
      "Epoch 341, Loss 11.397453\n",
      "Params: tensor([ 3.6335, -7.4879])\n",
      "Gradient: tensor([-0.2951,  1.6705])\n",
      "Epoch 342, Loss 11.368699\n",
      "Params: tensor([ 3.6365, -7.5046])\n",
      "Gradient: tensor([-0.2946,  1.6677])\n",
      "Epoch 343, Loss 11.340047\n",
      "Params: tensor([ 3.6394, -7.5212])\n",
      "Gradient: tensor([-0.2941,  1.6648])\n",
      "Epoch 344, Loss 11.311490\n",
      "Params: tensor([ 3.6424, -7.5378])\n",
      "Gradient: tensor([-0.2936,  1.6620])\n",
      "Epoch 345, Loss 11.283030\n",
      "Params: tensor([ 3.6453, -7.5544])\n",
      "Gradient: tensor([-0.2931,  1.6592])\n",
      "Epoch 346, Loss 11.254664\n",
      "Params: tensor([ 3.6482, -7.5710])\n",
      "Gradient: tensor([-0.2926,  1.6564])\n",
      "Epoch 347, Loss 11.226398\n",
      "Params: tensor([ 3.6511, -7.5875])\n",
      "Gradient: tensor([-0.2921,  1.6535])\n",
      "Epoch 348, Loss 11.198226\n",
      "Params: tensor([ 3.6541, -7.6040])\n",
      "Gradient: tensor([-0.2916,  1.6507])\n",
      "Epoch 349, Loss 11.170155\n",
      "Params: tensor([ 3.6570, -7.6205])\n",
      "Gradient: tensor([-0.2911,  1.6479])\n",
      "Epoch 350, Loss 11.142173\n",
      "Params: tensor([ 3.6599, -7.6370])\n",
      "Gradient: tensor([-0.2906,  1.6451])\n",
      "Epoch 351, Loss 11.114287\n",
      "Params: tensor([ 3.6628, -7.6534])\n",
      "Gradient: tensor([-0.2901,  1.6423])\n",
      "Epoch 352, Loss 11.086494\n",
      "Params: tensor([ 3.6657, -7.6698])\n",
      "Gradient: tensor([-0.2896,  1.6395])\n",
      "Epoch 353, Loss 11.058800\n",
      "Params: tensor([ 3.6686, -7.6861])\n",
      "Gradient: tensor([-0.2891,  1.6368])\n",
      "Epoch 354, Loss 11.031198\n",
      "Params: tensor([ 3.6714, -7.7025])\n",
      "Gradient: tensor([-0.2886,  1.6340])\n",
      "Epoch 355, Loss 11.003687\n",
      "Params: tensor([ 3.6743, -7.7188])\n",
      "Gradient: tensor([-0.2882,  1.6312])\n",
      "Epoch 356, Loss 10.976272\n",
      "Params: tensor([ 3.6772, -7.7351])\n",
      "Gradient: tensor([-0.2877,  1.6284])\n",
      "Epoch 357, Loss 10.948951\n",
      "Params: tensor([ 3.6801, -7.7513])\n",
      "Gradient: tensor([-0.2872,  1.6257])\n",
      "Epoch 358, Loss 10.921720\n",
      "Params: tensor([ 3.6829, -7.7676])\n",
      "Gradient: tensor([-0.2867,  1.6229])\n",
      "Epoch 359, Loss 10.894586\n",
      "Params: tensor([ 3.6858, -7.7838])\n",
      "Gradient: tensor([-0.2862,  1.6201])\n",
      "Epoch 360, Loss 10.867539\n",
      "Params: tensor([ 3.6887, -7.7999])\n",
      "Gradient: tensor([-0.2857,  1.6174])\n",
      "Epoch 361, Loss 10.840586\n",
      "Params: tensor([ 3.6915, -7.8161])\n",
      "Gradient: tensor([-0.2852,  1.6146])\n",
      "Epoch 362, Loss 10.813725\n",
      "Params: tensor([ 3.6944, -7.8322])\n",
      "Gradient: tensor([-0.2847,  1.6119])\n",
      "Epoch 363, Loss 10.786953\n",
      "Params: tensor([ 3.6972, -7.8483])\n",
      "Gradient: tensor([-0.2843,  1.6092])\n",
      "Epoch 364, Loss 10.760274\n",
      "Params: tensor([ 3.7000, -7.8644])\n",
      "Gradient: tensor([-0.2838,  1.6064])\n",
      "Epoch 365, Loss 10.733685\n",
      "Params: tensor([ 3.7029, -7.8804])\n",
      "Gradient: tensor([-0.2833,  1.6037])\n",
      "Epoch 366, Loss 10.707185\n",
      "Params: tensor([ 3.7057, -7.8964])\n",
      "Gradient: tensor([-0.2828,  1.6010])\n",
      "Epoch 367, Loss 10.680777\n",
      "Params: tensor([ 3.7085, -7.9124])\n",
      "Gradient: tensor([-0.2823,  1.5983])\n",
      "Epoch 368, Loss 10.654456\n",
      "Params: tensor([ 3.7113, -7.9283])\n",
      "Gradient: tensor([-0.2819,  1.5955])\n",
      "Epoch 369, Loss 10.628228\n",
      "Params: tensor([ 3.7142, -7.9443])\n",
      "Gradient: tensor([-0.2814,  1.5928])\n",
      "Epoch 370, Loss 10.602090\n",
      "Params: tensor([ 3.7170, -7.9602])\n",
      "Gradient: tensor([-0.2809,  1.5901])\n",
      "Epoch 371, Loss 10.576035\n",
      "Params: tensor([ 3.7198, -7.9761])\n",
      "Gradient: tensor([-0.2804,  1.5874])\n",
      "Epoch 372, Loss 10.550074\n",
      "Params: tensor([ 3.7226, -7.9919])\n",
      "Gradient: tensor([-0.2799,  1.5847])\n",
      "Epoch 373, Loss 10.524198\n",
      "Params: tensor([ 3.7254, -8.0077])\n",
      "Gradient: tensor([-0.2795,  1.5820])\n",
      "Epoch 374, Loss 10.498412\n",
      "Params: tensor([ 3.7282, -8.0235])\n",
      "Gradient: tensor([-0.2790,  1.5794])\n",
      "Epoch 375, Loss 10.472711\n",
      "Params: tensor([ 3.7309, -8.0393])\n",
      "Gradient: tensor([-0.2785,  1.5767])\n",
      "Epoch 376, Loss 10.447095\n",
      "Params: tensor([ 3.7337, -8.0550])\n",
      "Gradient: tensor([-0.2781,  1.5740])\n",
      "Epoch 377, Loss 10.421571\n",
      "Params: tensor([ 3.7365, -8.0707])\n",
      "Gradient: tensor([-0.2776,  1.5713])\n",
      "Epoch 378, Loss 10.396133\n",
      "Params: tensor([ 3.7393, -8.0864])\n",
      "Gradient: tensor([-0.2771,  1.5686])\n",
      "Epoch 379, Loss 10.370782\n",
      "Params: tensor([ 3.7420, -8.1021])\n",
      "Gradient: tensor([-0.2766,  1.5660])\n",
      "Epoch 380, Loss 10.345513\n",
      "Params: tensor([ 3.7448, -8.1177])\n",
      "Gradient: tensor([-0.2762,  1.5633])\n",
      "Epoch 381, Loss 10.320332\n",
      "Params: tensor([ 3.7476, -8.1333])\n",
      "Gradient: tensor([-0.2757,  1.5607])\n",
      "Epoch 382, Loss 10.295238\n",
      "Params: tensor([ 3.7503, -8.1489])\n",
      "Gradient: tensor([-0.2752,  1.5580])\n",
      "Epoch 383, Loss 10.270225\n",
      "Params: tensor([ 3.7531, -8.1645])\n",
      "Gradient: tensor([-0.2748,  1.5554])\n",
      "Epoch 384, Loss 10.245301\n",
      "Params: tensor([ 3.7558, -8.1800])\n",
      "Gradient: tensor([-0.2743,  1.5527])\n",
      "Epoch 385, Loss 10.220460\n",
      "Params: tensor([ 3.7585, -8.1955])\n",
      "Gradient: tensor([-0.2738,  1.5501])\n",
      "Epoch 386, Loss 10.195704\n",
      "Params: tensor([ 3.7613, -8.2110])\n",
      "Gradient: tensor([-0.2734,  1.5475])\n",
      "Epoch 387, Loss 10.171031\n",
      "Params: tensor([ 3.7640, -8.2264])\n",
      "Gradient: tensor([-0.2729,  1.5448])\n",
      "Epoch 388, Loss 10.146441\n",
      "Params: tensor([ 3.7667, -8.2418])\n",
      "Gradient: tensor([-0.2724,  1.5422])\n",
      "Epoch 389, Loss 10.121937\n",
      "Params: tensor([ 3.7694, -8.2572])\n",
      "Gradient: tensor([-0.2720,  1.5396])\n",
      "Epoch 390, Loss 10.097513\n",
      "Params: tensor([ 3.7722, -8.2726])\n",
      "Gradient: tensor([-0.2715,  1.5370])\n",
      "Epoch 391, Loss 10.073175\n",
      "Params: tensor([ 3.7749, -8.2879])\n",
      "Gradient: tensor([-0.2710,  1.5344])\n",
      "Epoch 392, Loss 10.048920\n",
      "Params: tensor([ 3.7776, -8.3033])\n",
      "Gradient: tensor([-0.2706,  1.5317])\n",
      "Epoch 393, Loss 10.024746\n",
      "Params: tensor([ 3.7803, -8.3185])\n",
      "Gradient: tensor([-0.2701,  1.5291])\n",
      "Epoch 394, Loss 10.000655\n",
      "Params: tensor([ 3.7830, -8.3338])\n",
      "Gradient: tensor([-0.2697,  1.5265])\n",
      "Epoch 395, Loss 9.976643\n",
      "Params: tensor([ 3.7857, -8.3491])\n",
      "Gradient: tensor([-0.2692,  1.5240])\n",
      "Epoch 396, Loss 9.952714\n",
      "Params: tensor([ 3.7884, -8.3643])\n",
      "Gradient: tensor([-0.2688,  1.5214])\n",
      "Epoch 397, Loss 9.928867\n",
      "Params: tensor([ 3.7910, -8.3795])\n",
      "Gradient: tensor([-0.2683,  1.5188])\n",
      "Epoch 398, Loss 9.905097\n",
      "Params: tensor([ 3.7937, -8.3946])\n",
      "Gradient: tensor([-0.2678,  1.5162])\n",
      "Epoch 399, Loss 9.881410\n",
      "Params: tensor([ 3.7964, -8.4098])\n",
      "Gradient: tensor([-0.2674,  1.5136])\n",
      "Epoch 400, Loss 9.857809\n",
      "Params: tensor([ 3.7991, -8.4249])\n",
      "Gradient: tensor([-0.2669,  1.5111])\n",
      "Epoch 401, Loss 9.834280\n",
      "Params: tensor([ 3.8017, -8.4399])\n",
      "Gradient: tensor([-0.2665,  1.5085])\n",
      "Epoch 402, Loss 9.810836\n",
      "Params: tensor([ 3.8044, -8.4550])\n",
      "Gradient: tensor([-0.2660,  1.5059])\n",
      "Epoch 403, Loss 9.787471\n",
      "Params: tensor([ 3.8070, -8.4700])\n",
      "Gradient: tensor([-0.2656,  1.5034])\n",
      "Epoch 404, Loss 9.764179\n",
      "Params: tensor([ 3.8097, -8.4850])\n",
      "Gradient: tensor([-0.2651,  1.5008])\n",
      "Epoch 405, Loss 9.740975\n",
      "Params: tensor([ 3.8123, -8.5000])\n",
      "Gradient: tensor([-0.2647,  1.4983])\n",
      "Epoch 406, Loss 9.717846\n",
      "Params: tensor([ 3.8150, -8.5150])\n",
      "Gradient: tensor([-0.2642,  1.4957])\n",
      "Epoch 407, Loss 9.694796\n",
      "Params: tensor([ 3.8176, -8.5299])\n",
      "Gradient: tensor([-0.2638,  1.4932])\n",
      "Epoch 408, Loss 9.671823\n",
      "Params: tensor([ 3.8202, -8.5448])\n",
      "Gradient: tensor([-0.2633,  1.4906])\n",
      "Epoch 409, Loss 9.648930\n",
      "Params: tensor([ 3.8229, -8.5597])\n",
      "Gradient: tensor([-0.2629,  1.4881])\n",
      "Epoch 410, Loss 9.626113\n",
      "Params: tensor([ 3.8255, -8.5746])\n",
      "Gradient: tensor([-0.2624,  1.4856])\n",
      "Epoch 411, Loss 9.603375\n",
      "Params: tensor([ 3.8281, -8.5894])\n",
      "Gradient: tensor([-0.2620,  1.4831])\n",
      "Epoch 412, Loss 9.580713\n",
      "Params: tensor([ 3.8307, -8.6042])\n",
      "Gradient: tensor([-0.2615,  1.4805])\n",
      "Epoch 413, Loss 9.558127\n",
      "Params: tensor([ 3.8333, -8.6190])\n",
      "Gradient: tensor([-0.2611,  1.4780])\n",
      "Epoch 414, Loss 9.535620\n",
      "Params: tensor([ 3.8360, -8.6337])\n",
      "Gradient: tensor([-0.2607,  1.4755])\n",
      "Epoch 415, Loss 9.513189\n",
      "Params: tensor([ 3.8386, -8.6485])\n",
      "Gradient: tensor([-0.2602,  1.4730])\n",
      "Epoch 416, Loss 9.490830\n",
      "Params: tensor([ 3.8412, -8.6632])\n",
      "Gradient: tensor([-0.2598,  1.4705])\n",
      "Epoch 417, Loss 9.468553\n",
      "Params: tensor([ 3.8437, -8.6778])\n",
      "Gradient: tensor([-0.2593,  1.4680])\n",
      "Epoch 418, Loss 9.446348\n",
      "Params: tensor([ 3.8463, -8.6925])\n",
      "Gradient: tensor([-0.2589,  1.4655])\n",
      "Epoch 419, Loss 9.424220\n",
      "Params: tensor([ 3.8489, -8.7071])\n",
      "Gradient: tensor([-0.2584,  1.4630])\n",
      "Epoch 420, Loss 9.402166\n",
      "Params: tensor([ 3.8515, -8.7217])\n",
      "Gradient: tensor([-0.2580,  1.4605])\n",
      "Epoch 421, Loss 9.380188\n",
      "Params: tensor([ 3.8541, -8.7363])\n",
      "Gradient: tensor([-0.2576,  1.4581])\n",
      "Epoch 422, Loss 9.358285\n",
      "Params: tensor([ 3.8566, -8.7509])\n",
      "Gradient: tensor([-0.2571,  1.4556])\n",
      "Epoch 423, Loss 9.336452\n",
      "Params: tensor([ 3.8592, -8.7654])\n",
      "Gradient: tensor([-0.2567,  1.4531])\n",
      "Epoch 424, Loss 9.314697\n",
      "Params: tensor([ 3.8618, -8.7799])\n",
      "Gradient: tensor([-0.2563,  1.4506])\n",
      "Epoch 425, Loss 9.293015\n",
      "Params: tensor([ 3.8643, -8.7944])\n",
      "Gradient: tensor([-0.2558,  1.4482])\n",
      "Epoch 426, Loss 9.271409\n",
      "Params: tensor([ 3.8669, -8.8089])\n",
      "Gradient: tensor([-0.2554,  1.4457])\n",
      "Epoch 427, Loss 9.249873\n",
      "Params: tensor([ 3.8694, -8.8233])\n",
      "Gradient: tensor([-0.2550,  1.4433])\n",
      "Epoch 428, Loss 9.228410\n",
      "Params: tensor([ 3.8720, -8.8377])\n",
      "Gradient: tensor([-0.2545,  1.4408])\n",
      "Epoch 429, Loss 9.207021\n",
      "Params: tensor([ 3.8745, -8.8521])\n",
      "Gradient: tensor([-0.2541,  1.4384])\n",
      "Epoch 430, Loss 9.185708\n",
      "Params: tensor([ 3.8771, -8.8664])\n",
      "Gradient: tensor([-0.2537,  1.4359])\n",
      "Epoch 431, Loss 9.164463\n",
      "Params: tensor([ 3.8796, -8.8808])\n",
      "Gradient: tensor([-0.2532,  1.4335])\n",
      "Epoch 432, Loss 9.143290\n",
      "Params: tensor([ 3.8821, -8.8951])\n",
      "Gradient: tensor([-0.2528,  1.4310])\n",
      "Epoch 433, Loss 9.122191\n",
      "Params: tensor([ 3.8846, -8.9094])\n",
      "Gradient: tensor([-0.2524,  1.4286])\n",
      "Epoch 434, Loss 9.101160\n",
      "Params: tensor([ 3.8872, -8.9236])\n",
      "Gradient: tensor([-0.2519,  1.4262])\n",
      "Epoch 435, Loss 9.080205\n",
      "Params: tensor([ 3.8897, -8.9379])\n",
      "Gradient: tensor([-0.2515,  1.4238])\n",
      "Epoch 436, Loss 9.059319\n",
      "Params: tensor([ 3.8922, -8.9521])\n",
      "Gradient: tensor([-0.2511,  1.4213])\n",
      "Epoch 437, Loss 9.038504\n",
      "Params: tensor([ 3.8947, -8.9663])\n",
      "Gradient: tensor([-0.2507,  1.4189])\n",
      "Epoch 438, Loss 9.017760\n",
      "Params: tensor([ 3.8972, -8.9804])\n",
      "Gradient: tensor([-0.2502,  1.4165])\n",
      "Epoch 439, Loss 8.997088\n",
      "Params: tensor([ 3.8997, -8.9946])\n",
      "Gradient: tensor([-0.2498,  1.4141])\n",
      "Epoch 440, Loss 8.976482\n",
      "Params: tensor([ 3.9022, -9.0087])\n",
      "Gradient: tensor([-0.2494,  1.4117])\n",
      "Epoch 441, Loss 8.955950\n",
      "Params: tensor([ 3.9047, -9.0228])\n",
      "Gradient: tensor([-0.2490,  1.4093])\n",
      "Epoch 442, Loss 8.935482\n",
      "Params: tensor([ 3.9072, -9.0369])\n",
      "Gradient: tensor([-0.2485,  1.4069])\n",
      "Epoch 443, Loss 8.915091\n",
      "Params: tensor([ 3.9096, -9.0509])\n",
      "Gradient: tensor([-0.2481,  1.4045])\n",
      "Epoch 444, Loss 8.894761\n",
      "Params: tensor([ 3.9121, -9.0649])\n",
      "Gradient: tensor([-0.2477,  1.4021])\n",
      "Epoch 445, Loss 8.874507\n",
      "Params: tensor([ 3.9146, -9.0789])\n",
      "Gradient: tensor([-0.2473,  1.3998])\n",
      "Epoch 446, Loss 8.854321\n",
      "Params: tensor([ 3.9171, -9.0929])\n",
      "Gradient: tensor([-0.2469,  1.3974])\n",
      "Epoch 447, Loss 8.834201\n",
      "Params: tensor([ 3.9195, -9.1068])\n",
      "Gradient: tensor([-0.2464,  1.3950])\n",
      "Epoch 448, Loss 8.814149\n",
      "Params: tensor([ 3.9220, -9.1208])\n",
      "Gradient: tensor([-0.2460,  1.3926])\n",
      "Epoch 449, Loss 8.794168\n",
      "Params: tensor([ 3.9244, -9.1347])\n",
      "Gradient: tensor([-0.2456,  1.3903])\n",
      "Epoch 450, Loss 8.774252\n",
      "Params: tensor([ 3.9269, -9.1486])\n",
      "Gradient: tensor([-0.2452,  1.3879])\n",
      "Epoch 451, Loss 8.754406\n",
      "Params: tensor([ 3.9293, -9.1624])\n",
      "Gradient: tensor([-0.2448,  1.3856])\n",
      "Epoch 452, Loss 8.734623\n",
      "Params: tensor([ 3.9318, -9.1762])\n",
      "Gradient: tensor([-0.2443,  1.3832])\n",
      "Epoch 453, Loss 8.714913\n",
      "Params: tensor([ 3.9342, -9.1901])\n",
      "Gradient: tensor([-0.2439,  1.3808])\n",
      "Epoch 454, Loss 8.695268\n",
      "Params: tensor([ 3.9367, -9.2038])\n",
      "Gradient: tensor([-0.2435,  1.3785])\n",
      "Epoch 455, Loss 8.675687\n",
      "Params: tensor([ 3.9391, -9.2176])\n",
      "Gradient: tensor([-0.2431,  1.3762])\n",
      "Epoch 456, Loss 8.656176\n",
      "Params: tensor([ 3.9415, -9.2313])\n",
      "Gradient: tensor([-0.2427,  1.3738])\n",
      "Epoch 457, Loss 8.636727\n",
      "Params: tensor([ 3.9439, -9.2451])\n",
      "Gradient: tensor([-0.2423,  1.3715])\n",
      "Epoch 458, Loss 8.617349\n",
      "Params: tensor([ 3.9464, -9.2587])\n",
      "Gradient: tensor([-0.2419,  1.3692])\n",
      "Epoch 459, Loss 8.598034\n",
      "Params: tensor([ 3.9488, -9.2724])\n",
      "Gradient: tensor([-0.2415,  1.3668])\n",
      "Epoch 460, Loss 8.578784\n",
      "Params: tensor([ 3.9512, -9.2861])\n",
      "Gradient: tensor([-0.2410,  1.3645])\n",
      "Epoch 461, Loss 8.559600\n",
      "Params: tensor([ 3.9536, -9.2997])\n",
      "Gradient: tensor([-0.2406,  1.3622])\n",
      "Epoch 462, Loss 8.540482\n",
      "Params: tensor([ 3.9560, -9.3133])\n",
      "Gradient: tensor([-0.2402,  1.3599])\n",
      "Epoch 463, Loss 8.521427\n",
      "Params: tensor([ 3.9584, -9.3269])\n",
      "Gradient: tensor([-0.2398,  1.3576])\n",
      "Epoch 464, Loss 8.502440\n",
      "Params: tensor([ 3.9608, -9.3404])\n",
      "Gradient: tensor([-0.2394,  1.3553])\n",
      "Epoch 465, Loss 8.483517\n",
      "Params: tensor([ 3.9632, -9.3539])\n",
      "Gradient: tensor([-0.2390,  1.3530])\n",
      "Epoch 466, Loss 8.464653\n",
      "Params: tensor([ 3.9656, -9.3674])\n",
      "Gradient: tensor([-0.2386,  1.3507])\n",
      "Epoch 467, Loss 8.445861\n",
      "Params: tensor([ 3.9679, -9.3809])\n",
      "Gradient: tensor([-0.2382,  1.3484])\n",
      "Epoch 468, Loss 8.427127\n",
      "Params: tensor([ 3.9703, -9.3944])\n",
      "Gradient: tensor([-0.2378,  1.3461])\n",
      "Epoch 469, Loss 8.408454\n",
      "Params: tensor([ 3.9727, -9.4078])\n",
      "Gradient: tensor([-0.2374,  1.3438])\n",
      "Epoch 470, Loss 8.389850\n",
      "Params: tensor([ 3.9751, -9.4212])\n",
      "Gradient: tensor([-0.2370,  1.3415])\n",
      "Epoch 471, Loss 8.371307\n",
      "Params: tensor([ 3.9774, -9.4346])\n",
      "Gradient: tensor([-0.2366,  1.3392])\n",
      "Epoch 472, Loss 8.352826\n",
      "Params: tensor([ 3.9798, -9.4480])\n",
      "Gradient: tensor([-0.2362,  1.3370])\n",
      "Epoch 473, Loss 8.334411\n",
      "Params: tensor([ 3.9822, -9.4613])\n",
      "Gradient: tensor([-0.2358,  1.3347])\n",
      "Epoch 474, Loss 8.316056\n",
      "Params: tensor([ 3.9845, -9.4747])\n",
      "Gradient: tensor([-0.2354,  1.3324])\n",
      "Epoch 475, Loss 8.297766\n",
      "Params: tensor([ 3.9869, -9.4880])\n",
      "Gradient: tensor([-0.2350,  1.3302])\n",
      "Epoch 476, Loss 8.279538\n",
      "Params: tensor([ 3.9892, -9.5013])\n",
      "Gradient: tensor([-0.2346,  1.3279])\n",
      "Epoch 477, Loss 8.261367\n",
      "Params: tensor([ 3.9915, -9.5145])\n",
      "Gradient: tensor([-0.2342,  1.3256])\n",
      "Epoch 478, Loss 8.243265\n",
      "Params: tensor([ 3.9939, -9.5277])\n",
      "Gradient: tensor([-0.2338,  1.3234])\n",
      "Epoch 479, Loss 8.225216\n",
      "Params: tensor([ 3.9962, -9.5410])\n",
      "Gradient: tensor([-0.2334,  1.3211])\n",
      "Epoch 480, Loss 8.207232\n",
      "Params: tensor([ 3.9985, -9.5541])\n",
      "Gradient: tensor([-0.2330,  1.3189])\n",
      "Epoch 481, Loss 8.189313\n",
      "Params: tensor([ 4.0009, -9.5673])\n",
      "Gradient: tensor([-0.2326,  1.3167])\n",
      "Epoch 482, Loss 8.171451\n",
      "Params: tensor([ 4.0032, -9.5805])\n",
      "Gradient: tensor([-0.2322,  1.3144])\n",
      "Epoch 483, Loss 8.153649\n",
      "Params: tensor([ 4.0055, -9.5936])\n",
      "Gradient: tensor([-0.2318,  1.3122])\n",
      "Epoch 484, Loss 8.135908\n",
      "Params: tensor([ 4.0078, -9.6067])\n",
      "Gradient: tensor([-0.2314,  1.3099])\n",
      "Epoch 485, Loss 8.118228\n",
      "Params: tensor([ 4.0101, -9.6198])\n",
      "Gradient: tensor([-0.2310,  1.3077])\n",
      "Epoch 486, Loss 8.100610\n",
      "Params: tensor([ 4.0124, -9.6328])\n",
      "Gradient: tensor([-0.2306,  1.3055])\n",
      "Epoch 487, Loss 8.083049\n",
      "Params: tensor([ 4.0147, -9.6458])\n",
      "Gradient: tensor([-0.2302,  1.3033])\n",
      "Epoch 488, Loss 8.065549\n",
      "Params: tensor([ 4.0170, -9.6589])\n",
      "Gradient: tensor([-0.2299,  1.3011])\n",
      "Epoch 489, Loss 8.048107\n",
      "Params: tensor([ 4.0193, -9.6718])\n",
      "Gradient: tensor([-0.2295,  1.2989])\n",
      "Epoch 490, Loss 8.030722\n",
      "Params: tensor([ 4.0216, -9.6848])\n",
      "Gradient: tensor([-0.2291,  1.2967])\n",
      "Epoch 491, Loss 8.013401\n",
      "Params: tensor([ 4.0239, -9.6978])\n",
      "Gradient: tensor([-0.2287,  1.2945])\n",
      "Epoch 492, Loss 7.996137\n",
      "Params: tensor([ 4.0262, -9.7107])\n",
      "Gradient: tensor([-0.2283,  1.2923])\n",
      "Epoch 493, Loss 7.978931\n",
      "Params: tensor([ 4.0285, -9.7236])\n",
      "Gradient: tensor([-0.2279,  1.2901])\n",
      "Epoch 494, Loss 7.961786\n",
      "Params: tensor([ 4.0308, -9.7365])\n",
      "Gradient: tensor([-0.2275,  1.2879])\n",
      "Epoch 495, Loss 7.944696\n",
      "Params: tensor([ 4.0330, -9.7493])\n",
      "Gradient: tensor([-0.2271,  1.2857])\n",
      "Epoch 496, Loss 7.927665\n",
      "Params: tensor([ 4.0353, -9.7621])\n",
      "Gradient: tensor([-0.2267,  1.2835])\n",
      "Epoch 497, Loss 7.910692\n",
      "Params: tensor([ 4.0376, -9.7750])\n",
      "Gradient: tensor([-0.2263,  1.2813])\n",
      "Epoch 498, Loss 7.893779\n",
      "Params: tensor([ 4.0398, -9.7877])\n",
      "Gradient: tensor([-0.2260,  1.2791])\n",
      "Epoch 499, Loss 7.876918\n",
      "Params: tensor([ 4.0421, -9.8005])\n",
      "Gradient: tensor([-0.2256,  1.2770])\n",
      "Epoch 500, Loss 7.860118\n",
      "Params: tensor([ 4.0443, -9.8133])\n",
      "Gradient: tensor([-0.2252,  1.2748])\n",
      "Epoch 501, Loss 7.843371\n",
      "Params: tensor([ 4.0466, -9.8260])\n",
      "Gradient: tensor([-0.2248,  1.2726])\n",
      "Epoch 502, Loss 7.826686\n",
      "Params: tensor([ 4.0488, -9.8387])\n",
      "Gradient: tensor([-0.2244,  1.2705])\n",
      "Epoch 503, Loss 7.810054\n",
      "Params: tensor([ 4.0511, -9.8514])\n",
      "Gradient: tensor([-0.2240,  1.2683])\n",
      "Epoch 504, Loss 7.793480\n",
      "Params: tensor([ 4.0533, -9.8640])\n",
      "Gradient: tensor([-0.2237,  1.2662])\n",
      "Epoch 505, Loss 7.776965\n",
      "Params: tensor([ 4.0555, -9.8767])\n",
      "Gradient: tensor([-0.2233,  1.2640])\n",
      "Epoch 506, Loss 7.760502\n",
      "Params: tensor([ 4.0578, -9.8893])\n",
      "Gradient: tensor([-0.2229,  1.2619])\n",
      "Epoch 507, Loss 7.744091\n",
      "Params: tensor([ 4.0600, -9.9019])\n",
      "Gradient: tensor([-0.2225,  1.2597])\n",
      "Epoch 508, Loss 7.727746\n",
      "Params: tensor([ 4.0622, -9.9145])\n",
      "Gradient: tensor([-0.2221,  1.2576])\n",
      "Epoch 509, Loss 7.711448\n",
      "Params: tensor([ 4.0644, -9.9270])\n",
      "Gradient: tensor([-0.2218,  1.2554])\n",
      "Epoch 510, Loss 7.695213\n",
      "Params: tensor([ 4.0666, -9.9396])\n",
      "Gradient: tensor([-0.2214,  1.2533])\n",
      "Epoch 511, Loss 7.679026\n",
      "Params: tensor([ 4.0688, -9.9521])\n",
      "Gradient: tensor([-0.2210,  1.2512])\n",
      "Epoch 512, Loss 7.662897\n",
      "Params: tensor([ 4.0710, -9.9646])\n",
      "Gradient: tensor([-0.2207,  1.2490])\n",
      "Epoch 513, Loss 7.646821\n",
      "Params: tensor([ 4.0733, -9.9770])\n",
      "Gradient: tensor([-0.2203,  1.2469])\n",
      "Epoch 514, Loss 7.630805\n",
      "Params: tensor([ 4.0754, -9.9895])\n",
      "Gradient: tensor([-0.2199,  1.2448])\n",
      "Epoch 515, Loss 7.614837\n",
      "Params: tensor([  4.0776, -10.0019])\n",
      "Gradient: tensor([-0.2195,  1.2427])\n",
      "Epoch 516, Loss 7.598928\n",
      "Params: tensor([  4.0798, -10.0143])\n",
      "Gradient: tensor([-0.2191,  1.2406])\n",
      "Epoch 517, Loss 7.583070\n",
      "Params: tensor([  4.0820, -10.0267])\n",
      "Gradient: tensor([-0.2188,  1.2385])\n",
      "Epoch 518, Loss 7.567266\n",
      "Params: tensor([  4.0842, -10.0391])\n",
      "Gradient: tensor([-0.2184,  1.2364])\n",
      "Epoch 519, Loss 7.551517\n",
      "Params: tensor([  4.0864, -10.0514])\n",
      "Gradient: tensor([-0.2180,  1.2343])\n",
      "Epoch 520, Loss 7.535820\n",
      "Params: tensor([  4.0886, -10.0637])\n",
      "Gradient: tensor([-0.2177,  1.2322])\n",
      "Epoch 521, Loss 7.520178\n",
      "Params: tensor([  4.0907, -10.0760])\n",
      "Gradient: tensor([-0.2173,  1.2301])\n",
      "Epoch 522, Loss 7.504589\n",
      "Params: tensor([  4.0929, -10.0883])\n",
      "Gradient: tensor([-0.2169,  1.2280])\n",
      "Epoch 523, Loss 7.489052\n",
      "Params: tensor([  4.0951, -10.1006])\n",
      "Gradient: tensor([-0.2166,  1.2259])\n",
      "Epoch 524, Loss 7.473567\n",
      "Params: tensor([  4.0972, -10.1128])\n",
      "Gradient: tensor([-0.2162,  1.2238])\n",
      "Epoch 525, Loss 7.458134\n",
      "Params: tensor([  4.0994, -10.1250])\n",
      "Gradient: tensor([-0.2158,  1.2217])\n",
      "Epoch 526, Loss 7.442753\n",
      "Params: tensor([  4.1015, -10.1372])\n",
      "Gradient: tensor([-0.2154,  1.2197])\n",
      "Epoch 527, Loss 7.427427\n",
      "Params: tensor([  4.1037, -10.1494])\n",
      "Gradient: tensor([-0.2151,  1.2176])\n",
      "Epoch 528, Loss 7.412154\n",
      "Params: tensor([  4.1058, -10.1616])\n",
      "Gradient: tensor([-0.2147,  1.2155])\n",
      "Epoch 529, Loss 7.396930\n",
      "Params: tensor([  4.1080, -10.1737])\n",
      "Gradient: tensor([-0.2144,  1.2135])\n",
      "Epoch 530, Loss 7.381758\n",
      "Params: tensor([  4.1101, -10.1858])\n",
      "Gradient: tensor([-0.2140,  1.2114])\n",
      "Epoch 531, Loss 7.366637\n",
      "Params: tensor([  4.1123, -10.1979])\n",
      "Gradient: tensor([-0.2136,  1.2093])\n",
      "Epoch 532, Loss 7.351567\n",
      "Params: tensor([  4.1144, -10.2100])\n",
      "Gradient: tensor([-0.2133,  1.2073])\n",
      "Epoch 533, Loss 7.336549\n",
      "Params: tensor([  4.1165, -10.2220])\n",
      "Gradient: tensor([-0.2129,  1.2052])\n",
      "Epoch 534, Loss 7.321585\n",
      "Params: tensor([  4.1187, -10.2340])\n",
      "Gradient: tensor([-0.2125,  1.2032])\n",
      "Epoch 535, Loss 7.306671\n",
      "Params: tensor([  4.1208, -10.2461])\n",
      "Gradient: tensor([-0.2122,  1.2012])\n",
      "Epoch 536, Loss 7.291803\n",
      "Params: tensor([  4.1229, -10.2581])\n",
      "Gradient: tensor([-0.2118,  1.1991])\n",
      "Epoch 537, Loss 7.276988\n",
      "Params: tensor([  4.1250, -10.2700])\n",
      "Gradient: tensor([-0.2115,  1.1971])\n",
      "Epoch 538, Loss 7.262227\n",
      "Params: tensor([  4.1271, -10.2820])\n",
      "Gradient: tensor([-0.2111,  1.1950])\n",
      "Epoch 539, Loss 7.247512\n",
      "Params: tensor([  4.1292, -10.2939])\n",
      "Gradient: tensor([-0.2108,  1.1930])\n",
      "Epoch 540, Loss 7.232845\n",
      "Params: tensor([  4.1313, -10.3058])\n",
      "Gradient: tensor([-0.2104,  1.1910])\n",
      "Epoch 541, Loss 7.218231\n",
      "Params: tensor([  4.1334, -10.3177])\n",
      "Gradient: tensor([-0.2100,  1.1890])\n",
      "Epoch 542, Loss 7.203666\n",
      "Params: tensor([  4.1355, -10.3296])\n",
      "Gradient: tensor([-0.2097,  1.1869])\n",
      "Epoch 543, Loss 7.189151\n",
      "Params: tensor([  4.1376, -10.3414])\n",
      "Gradient: tensor([-0.2093,  1.1849])\n",
      "Epoch 544, Loss 7.174683\n",
      "Params: tensor([  4.1397, -10.3533])\n",
      "Gradient: tensor([-0.2090,  1.1829])\n",
      "Epoch 545, Loss 7.160266\n",
      "Params: tensor([  4.1418, -10.3651])\n",
      "Gradient: tensor([-0.2086,  1.1809])\n",
      "Epoch 546, Loss 7.145897\n",
      "Params: tensor([  4.1439, -10.3769])\n",
      "Gradient: tensor([-0.2083,  1.1789])\n",
      "Epoch 547, Loss 7.131578\n",
      "Params: tensor([  4.1460, -10.3886])\n",
      "Gradient: tensor([-0.2079,  1.1769])\n",
      "Epoch 548, Loss 7.117305\n",
      "Params: tensor([  4.1480, -10.4004])\n",
      "Gradient: tensor([-0.2075,  1.1749])\n",
      "Epoch 549, Loss 7.103083\n",
      "Params: tensor([  4.1501, -10.4121])\n",
      "Gradient: tensor([-0.2072,  1.1729])\n",
      "Epoch 550, Loss 7.088912\n",
      "Params: tensor([  4.1522, -10.4238])\n",
      "Gradient: tensor([-0.2068,  1.1709])\n",
      "Epoch 551, Loss 7.074785\n",
      "Params: tensor([  4.1542, -10.4355])\n",
      "Gradient: tensor([-0.2065,  1.1689])\n",
      "Epoch 552, Loss 7.060707\n",
      "Params: tensor([  4.1563, -10.4472])\n",
      "Gradient: tensor([-0.2062,  1.1669])\n",
      "Epoch 553, Loss 7.046676\n",
      "Params: tensor([  4.1584, -10.4588])\n",
      "Gradient: tensor([-0.2058,  1.1649])\n",
      "Epoch 554, Loss 7.032695\n",
      "Params: tensor([  4.1604, -10.4704])\n",
      "Gradient: tensor([-0.2054,  1.1630])\n",
      "Epoch 555, Loss 7.018756\n",
      "Params: tensor([  4.1625, -10.4821])\n",
      "Gradient: tensor([-0.2051,  1.1610])\n",
      "Epoch 556, Loss 7.004870\n",
      "Params: tensor([  4.1645, -10.4936])\n",
      "Gradient: tensor([-0.2047,  1.1590])\n",
      "Epoch 557, Loss 6.991028\n",
      "Params: tensor([  4.1666, -10.5052])\n",
      "Gradient: tensor([-0.2044,  1.1571])\n",
      "Epoch 558, Loss 6.977232\n",
      "Params: tensor([  4.1686, -10.5168])\n",
      "Gradient: tensor([-0.2041,  1.1551])\n",
      "Epoch 559, Loss 6.963488\n",
      "Params: tensor([  4.1706, -10.5283])\n",
      "Gradient: tensor([-0.2037,  1.1531])\n",
      "Epoch 560, Loss 6.949787\n",
      "Params: tensor([  4.1727, -10.5398])\n",
      "Gradient: tensor([-0.2034,  1.1512])\n",
      "Epoch 561, Loss 6.936135\n",
      "Params: tensor([  4.1747, -10.5513])\n",
      "Gradient: tensor([-0.2030,  1.1492])\n",
      "Epoch 562, Loss 6.922527\n",
      "Params: tensor([  4.1767, -10.5628])\n",
      "Gradient: tensor([-0.2027,  1.1473])\n",
      "Epoch 563, Loss 6.908967\n",
      "Params: tensor([  4.1787, -10.5742])\n",
      "Gradient: tensor([-0.2023,  1.1453])\n",
      "Epoch 564, Loss 6.895452\n",
      "Params: tensor([  4.1808, -10.5857])\n",
      "Gradient: tensor([-0.2020,  1.1434])\n",
      "Epoch 565, Loss 6.881980\n",
      "Params: tensor([  4.1828, -10.5971])\n",
      "Gradient: tensor([-0.2016,  1.1414])\n",
      "Epoch 566, Loss 6.868558\n",
      "Params: tensor([  4.1848, -10.6085])\n",
      "Gradient: tensor([-0.2013,  1.1395])\n",
      "Epoch 567, Loss 6.855180\n",
      "Params: tensor([  4.1868, -10.6198])\n",
      "Gradient: tensor([-0.2010,  1.1375])\n",
      "Epoch 568, Loss 6.841848\n",
      "Params: tensor([  4.1888, -10.6312])\n",
      "Gradient: tensor([-0.2006,  1.1356])\n",
      "Epoch 569, Loss 6.828561\n",
      "Params: tensor([  4.1908, -10.6425])\n",
      "Gradient: tensor([-0.2003,  1.1337])\n",
      "Epoch 570, Loss 6.815318\n",
      "Params: tensor([  4.1928, -10.6539])\n",
      "Gradient: tensor([-0.1999,  1.1318])\n",
      "Epoch 571, Loss 6.802118\n",
      "Params: tensor([  4.1948, -10.6652])\n",
      "Gradient: tensor([-0.1996,  1.1298])\n",
      "Epoch 572, Loss 6.788968\n",
      "Params: tensor([  4.1968, -10.6764])\n",
      "Gradient: tensor([-0.1992,  1.1279])\n",
      "Epoch 573, Loss 6.775864\n",
      "Params: tensor([  4.1988, -10.6877])\n",
      "Gradient: tensor([-0.1989,  1.1260])\n",
      "Epoch 574, Loss 6.762798\n",
      "Params: tensor([  4.2008, -10.6989])\n",
      "Gradient: tensor([-0.1986,  1.1241])\n",
      "Epoch 575, Loss 6.749778\n",
      "Params: tensor([  4.2028, -10.7102])\n",
      "Gradient: tensor([-0.1982,  1.1222])\n",
      "Epoch 576, Loss 6.736804\n",
      "Params: tensor([  4.2047, -10.7214])\n",
      "Gradient: tensor([-0.1979,  1.1203])\n",
      "Epoch 577, Loss 6.723875\n",
      "Params: tensor([  4.2067, -10.7325])\n",
      "Gradient: tensor([-0.1976,  1.1184])\n",
      "Epoch 578, Loss 6.710986\n",
      "Params: tensor([  4.2087, -10.7437])\n",
      "Gradient: tensor([-0.1972,  1.1165])\n",
      "Epoch 579, Loss 6.698143\n",
      "Params: tensor([  4.2107, -10.7549])\n",
      "Gradient: tensor([-0.1969,  1.1146])\n",
      "Epoch 580, Loss 6.685344\n",
      "Params: tensor([  4.2126, -10.7660])\n",
      "Gradient: tensor([-0.1966,  1.1127])\n",
      "Epoch 581, Loss 6.672589\n",
      "Params: tensor([  4.2146, -10.7771])\n",
      "Gradient: tensor([-0.1962,  1.1108])\n",
      "Epoch 582, Loss 6.659874\n",
      "Params: tensor([  4.2165, -10.7882])\n",
      "Gradient: tensor([-0.1959,  1.1089])\n",
      "Epoch 583, Loss 6.647207\n",
      "Params: tensor([  4.2185, -10.7992])\n",
      "Gradient: tensor([-0.1956,  1.1070])\n",
      "Epoch 584, Loss 6.634578\n",
      "Params: tensor([  4.2204, -10.8103])\n",
      "Gradient: tensor([-0.1952,  1.1051])\n",
      "Epoch 585, Loss 6.621994\n",
      "Params: tensor([  4.2224, -10.8213])\n",
      "Gradient: tensor([-0.1949,  1.1033])\n",
      "Epoch 586, Loss 6.609454\n",
      "Params: tensor([  4.2243, -10.8323])\n",
      "Gradient: tensor([-0.1946,  1.1014])\n",
      "Epoch 587, Loss 6.596954\n",
      "Params: tensor([  4.2263, -10.8433])\n",
      "Gradient: tensor([-0.1942,  1.0995])\n",
      "Epoch 588, Loss 6.584500\n",
      "Params: tensor([  4.2282, -10.8543])\n",
      "Gradient: tensor([-0.1939,  1.0976])\n",
      "Epoch 589, Loss 6.572087\n",
      "Params: tensor([  4.2302, -10.8653])\n",
      "Gradient: tensor([-0.1936,  1.0958])\n",
      "Epoch 590, Loss 6.559712\n",
      "Params: tensor([  4.2321, -10.8762])\n",
      "Gradient: tensor([-0.1932,  1.0939])\n",
      "Epoch 591, Loss 6.547384\n",
      "Params: tensor([  4.2340, -10.8871])\n",
      "Gradient: tensor([-0.1929,  1.0921])\n",
      "Epoch 592, Loss 6.535097\n",
      "Params: tensor([  4.2359, -10.8980])\n",
      "Gradient: tensor([-0.1926,  1.0902])\n",
      "Epoch 593, Loss 6.522851\n",
      "Params: tensor([  4.2379, -10.9089])\n",
      "Gradient: tensor([-0.1923,  1.0884])\n",
      "Epoch 594, Loss 6.510645\n",
      "Params: tensor([  4.2398, -10.9198])\n",
      "Gradient: tensor([-0.1919,  1.0865])\n",
      "Epoch 595, Loss 6.498482\n",
      "Params: tensor([  4.2417, -10.9306])\n",
      "Gradient: tensor([-0.1916,  1.0847])\n",
      "Epoch 596, Loss 6.486362\n",
      "Params: tensor([  4.2436, -10.9415])\n",
      "Gradient: tensor([-0.1913,  1.0828])\n",
      "Epoch 597, Loss 6.474281\n",
      "Params: tensor([  4.2455, -10.9523])\n",
      "Gradient: tensor([-0.1910,  1.0810])\n",
      "Epoch 598, Loss 6.462241\n",
      "Params: tensor([  4.2474, -10.9631])\n",
      "Gradient: tensor([-0.1906,  1.0791])\n",
      "Epoch 599, Loss 6.450243\n",
      "Params: tensor([  4.2493, -10.9738])\n",
      "Gradient: tensor([-0.1903,  1.0773])\n",
      "Epoch 600, Loss 6.438284\n",
      "Params: tensor([  4.2512, -10.9846])\n",
      "Gradient: tensor([-0.1900,  1.0755])\n",
      "Epoch 601, Loss 6.426368\n",
      "Params: tensor([  4.2531, -10.9953])\n",
      "Gradient: tensor([-0.1897,  1.0737])\n",
      "Epoch 602, Loss 6.414490\n",
      "Params: tensor([  4.2550, -11.0060])\n",
      "Gradient: tensor([-0.1893,  1.0718])\n",
      "Epoch 603, Loss 6.402655\n",
      "Params: tensor([  4.2569, -11.0167])\n",
      "Gradient: tensor([-0.1890,  1.0700])\n",
      "Epoch 604, Loss 6.390859\n",
      "Params: tensor([  4.2588, -11.0274])\n",
      "Gradient: tensor([-0.1887,  1.0682])\n",
      "Epoch 605, Loss 6.379103\n",
      "Params: tensor([  4.2607, -11.0381])\n",
      "Gradient: tensor([-0.1884,  1.0664])\n",
      "Epoch 606, Loss 6.367384\n",
      "Params: tensor([  4.2626, -11.0487])\n",
      "Gradient: tensor([-0.1880,  1.0646])\n",
      "Epoch 607, Loss 6.355706\n",
      "Params: tensor([  4.2644, -11.0594])\n",
      "Gradient: tensor([-0.1877,  1.0628])\n",
      "Epoch 608, Loss 6.344071\n",
      "Params: tensor([  4.2663, -11.0700])\n",
      "Gradient: tensor([-0.1874,  1.0609])\n",
      "Epoch 609, Loss 6.332472\n",
      "Params: tensor([  4.2682, -11.0806])\n",
      "Gradient: tensor([-0.1871,  1.0591])\n",
      "Epoch 610, Loss 6.320912\n",
      "Params: tensor([  4.2701, -11.0911])\n",
      "Gradient: tensor([-0.1868,  1.0573])\n",
      "Epoch 611, Loss 6.309395\n",
      "Params: tensor([  4.2719, -11.1017])\n",
      "Gradient: tensor([-0.1865,  1.0555])\n",
      "Epoch 612, Loss 6.297915\n",
      "Params: tensor([  4.2738, -11.1122])\n",
      "Gradient: tensor([-0.1861,  1.0538])\n",
      "Epoch 613, Loss 6.286473\n",
      "Params: tensor([  4.2756, -11.1227])\n",
      "Gradient: tensor([-0.1858,  1.0520])\n",
      "Epoch 614, Loss 6.275073\n",
      "Params: tensor([  4.2775, -11.1333])\n",
      "Gradient: tensor([-0.1855,  1.0502])\n",
      "Epoch 615, Loss 6.263707\n",
      "Params: tensor([  4.2794, -11.1437])\n",
      "Gradient: tensor([-0.1852,  1.0484])\n",
      "Epoch 616, Loss 6.252382\n",
      "Params: tensor([  4.2812, -11.1542])\n",
      "Gradient: tensor([-0.1849,  1.0466])\n",
      "Epoch 617, Loss 6.241098\n",
      "Params: tensor([  4.2830, -11.1646])\n",
      "Gradient: tensor([-0.1846,  1.0448])\n",
      "Epoch 618, Loss 6.229849\n",
      "Params: tensor([  4.2849, -11.1751])\n",
      "Gradient: tensor([-0.1843,  1.0431])\n",
      "Epoch 619, Loss 6.218639\n",
      "Params: tensor([  4.2867, -11.1855])\n",
      "Gradient: tensor([-0.1840,  1.0413])\n",
      "Epoch 620, Loss 6.207470\n",
      "Params: tensor([  4.2886, -11.1959])\n",
      "Gradient: tensor([-0.1836,  1.0395])\n",
      "Epoch 621, Loss 6.196333\n",
      "Params: tensor([  4.2904, -11.2063])\n",
      "Gradient: tensor([-0.1833,  1.0378])\n",
      "Epoch 622, Loss 6.185240\n",
      "Params: tensor([  4.2922, -11.2166])\n",
      "Gradient: tensor([-0.1830,  1.0360])\n",
      "Epoch 623, Loss 6.174181\n",
      "Params: tensor([  4.2941, -11.2270])\n",
      "Gradient: tensor([-0.1827,  1.0342])\n",
      "Epoch 624, Loss 6.163159\n",
      "Params: tensor([  4.2959, -11.2373])\n",
      "Gradient: tensor([-0.1824,  1.0325])\n",
      "Epoch 625, Loss 6.152177\n",
      "Params: tensor([  4.2977, -11.2476])\n",
      "Gradient: tensor([-0.1821,  1.0307])\n",
      "Epoch 626, Loss 6.141228\n",
      "Params: tensor([  4.2995, -11.2579])\n",
      "Gradient: tensor([-0.1818,  1.0290])\n",
      "Epoch 627, Loss 6.130322\n",
      "Params: tensor([  4.3013, -11.2682])\n",
      "Gradient: tensor([-0.1815,  1.0272])\n",
      "Epoch 628, Loss 6.119448\n",
      "Params: tensor([  4.3031, -11.2784])\n",
      "Gradient: tensor([-0.1811,  1.0255])\n",
      "Epoch 629, Loss 6.108613\n",
      "Params: tensor([  4.3050, -11.2887])\n",
      "Gradient: tensor([-0.1808,  1.0237])\n",
      "Epoch 630, Loss 6.097815\n",
      "Params: tensor([  4.3068, -11.2989])\n",
      "Gradient: tensor([-0.1805,  1.0220])\n",
      "Epoch 631, Loss 6.087053\n",
      "Params: tensor([  4.3086, -11.3091])\n",
      "Gradient: tensor([-0.1802,  1.0203])\n",
      "Epoch 632, Loss 6.076329\n",
      "Params: tensor([  4.3104, -11.3193])\n",
      "Gradient: tensor([-0.1799,  1.0185])\n",
      "Epoch 633, Loss 6.065643\n",
      "Params: tensor([  4.3122, -11.3294])\n",
      "Gradient: tensor([-0.1796,  1.0168])\n",
      "Epoch 634, Loss 6.054988\n",
      "Params: tensor([  4.3139, -11.3396])\n",
      "Gradient: tensor([-0.1793,  1.0151])\n",
      "Epoch 635, Loss 6.044371\n",
      "Params: tensor([  4.3157, -11.3497])\n",
      "Gradient: tensor([-0.1790,  1.0133])\n",
      "Epoch 636, Loss 6.033794\n",
      "Params: tensor([  4.3175, -11.3598])\n",
      "Gradient: tensor([-0.1787,  1.0116])\n",
      "Epoch 637, Loss 6.023248\n",
      "Params: tensor([  4.3193, -11.3699])\n",
      "Gradient: tensor([-0.1784,  1.0099])\n",
      "Epoch 638, Loss 6.012738\n",
      "Params: tensor([  4.3211, -11.3800])\n",
      "Gradient: tensor([-0.1781,  1.0082])\n",
      "Epoch 639, Loss 6.002264\n",
      "Params: tensor([  4.3229, -11.3901])\n",
      "Gradient: tensor([-0.1778,  1.0065])\n",
      "Epoch 640, Loss 5.991828\n",
      "Params: tensor([  4.3246, -11.4001])\n",
      "Gradient: tensor([-0.1775,  1.0048])\n",
      "Epoch 641, Loss 5.981425\n",
      "Params: tensor([  4.3264, -11.4102])\n",
      "Gradient: tensor([-0.1772,  1.0031])\n",
      "Epoch 642, Loss 5.971058\n",
      "Params: tensor([  4.3282, -11.4202])\n",
      "Gradient: tensor([-0.1769,  1.0014])\n",
      "Epoch 643, Loss 5.960727\n",
      "Params: tensor([  4.3300, -11.4302])\n",
      "Gradient: tensor([-0.1766,  0.9997])\n",
      "Epoch 644, Loss 5.950432\n",
      "Params: tensor([  4.3317, -11.4401])\n",
      "Gradient: tensor([-0.1763,  0.9980])\n",
      "Epoch 645, Loss 5.940172\n",
      "Params: tensor([  4.3335, -11.4501])\n",
      "Gradient: tensor([-0.1760,  0.9963])\n",
      "Epoch 646, Loss 5.929944\n",
      "Params: tensor([  4.3352, -11.4601])\n",
      "Gradient: tensor([-0.1757,  0.9946])\n",
      "Epoch 647, Loss 5.919753\n",
      "Params: tensor([  4.3370, -11.4700])\n",
      "Gradient: tensor([-0.1754,  0.9929])\n",
      "Epoch 648, Loss 5.909597\n",
      "Params: tensor([  4.3387, -11.4799])\n",
      "Gradient: tensor([-0.1751,  0.9912])\n",
      "Epoch 649, Loss 5.899472\n",
      "Params: tensor([  4.3405, -11.4898])\n",
      "Gradient: tensor([-0.1748,  0.9895])\n",
      "Epoch 650, Loss 5.889383\n",
      "Params: tensor([  4.3422, -11.4997])\n",
      "Gradient: tensor([-0.1745,  0.9878])\n",
      "Epoch 651, Loss 5.879326\n",
      "Params: tensor([  4.3440, -11.5095])\n",
      "Gradient: tensor([-0.1742,  0.9862])\n",
      "Epoch 652, Loss 5.869310\n",
      "Params: tensor([  4.3457, -11.5194])\n",
      "Gradient: tensor([-0.1739,  0.9845])\n",
      "Epoch 653, Loss 5.859323\n",
      "Params: tensor([  4.3474, -11.5292])\n",
      "Gradient: tensor([-0.1736,  0.9828])\n",
      "Epoch 654, Loss 5.849374\n",
      "Params: tensor([  4.3492, -11.5390])\n",
      "Gradient: tensor([-0.1733,  0.9811])\n",
      "Epoch 655, Loss 5.839453\n",
      "Params: tensor([  4.3509, -11.5488])\n",
      "Gradient: tensor([-0.1730,  0.9795])\n",
      "Epoch 656, Loss 5.829569\n",
      "Params: tensor([  4.3526, -11.5586])\n",
      "Gradient: tensor([-0.1727,  0.9778])\n",
      "Epoch 657, Loss 5.819718\n",
      "Params: tensor([  4.3544, -11.5683])\n",
      "Gradient: tensor([-0.1724,  0.9761])\n",
      "Epoch 658, Loss 5.809900\n",
      "Params: tensor([  4.3561, -11.5781])\n",
      "Gradient: tensor([-0.1722,  0.9745])\n",
      "Epoch 659, Loss 5.800117\n",
      "Params: tensor([  4.3578, -11.5878])\n",
      "Gradient: tensor([-0.1719,  0.9728])\n",
      "Epoch 660, Loss 5.790367\n",
      "Params: tensor([  4.3595, -11.5975])\n",
      "Gradient: tensor([-0.1716,  0.9712])\n",
      "Epoch 661, Loss 5.780647\n",
      "Params: tensor([  4.3612, -11.6072])\n",
      "Gradient: tensor([-0.1713,  0.9695])\n",
      "Epoch 662, Loss 5.770962\n",
      "Params: tensor([  4.3629, -11.6169])\n",
      "Gradient: tensor([-0.1710,  0.9679])\n",
      "Epoch 663, Loss 5.761311\n",
      "Params: tensor([  4.3646, -11.6266])\n",
      "Gradient: tensor([-0.1707,  0.9662])\n",
      "Epoch 664, Loss 5.751693\n",
      "Params: tensor([  4.3664, -11.6362])\n",
      "Gradient: tensor([-0.1704,  0.9646])\n",
      "Epoch 665, Loss 5.742105\n",
      "Params: tensor([  4.3681, -11.6458])\n",
      "Gradient: tensor([-0.1701,  0.9630])\n",
      "Epoch 666, Loss 5.732550\n",
      "Params: tensor([  4.3697, -11.6555])\n",
      "Gradient: tensor([-0.1698,  0.9613])\n",
      "Epoch 667, Loss 5.723031\n",
      "Params: tensor([  4.3714, -11.6651])\n",
      "Gradient: tensor([-0.1695,  0.9597])\n",
      "Epoch 668, Loss 5.713540\n",
      "Params: tensor([  4.3731, -11.6746])\n",
      "Gradient: tensor([-0.1692,  0.9581])\n",
      "Epoch 669, Loss 5.704084\n",
      "Params: tensor([  4.3748, -11.6842])\n",
      "Gradient: tensor([-0.1690,  0.9564])\n",
      "Epoch 670, Loss 5.694659\n",
      "Params: tensor([  4.3765, -11.6937])\n",
      "Gradient: tensor([-0.1687,  0.9548])\n",
      "Epoch 671, Loss 5.685265\n",
      "Params: tensor([  4.3782, -11.7033])\n",
      "Gradient: tensor([-0.1684,  0.9532])\n",
      "Epoch 672, Loss 5.675904\n",
      "Params: tensor([  4.3799, -11.7128])\n",
      "Gradient: tensor([-0.1681,  0.9516])\n",
      "Epoch 673, Loss 5.666573\n",
      "Params: tensor([  4.3816, -11.7223])\n",
      "Gradient: tensor([-0.1678,  0.9499])\n",
      "Epoch 674, Loss 5.657277\n",
      "Params: tensor([  4.3832, -11.7318])\n",
      "Gradient: tensor([-0.1675,  0.9483])\n",
      "Epoch 675, Loss 5.648010\n",
      "Params: tensor([  4.3849, -11.7412])\n",
      "Gradient: tensor([-0.1673,  0.9467])\n",
      "Epoch 676, Loss 5.638776\n",
      "Params: tensor([  4.3866, -11.7507])\n",
      "Gradient: tensor([-0.1670,  0.9451])\n",
      "Epoch 677, Loss 5.629575\n",
      "Params: tensor([  4.3882, -11.7601])\n",
      "Gradient: tensor([-0.1667,  0.9435])\n",
      "Epoch 678, Loss 5.620402\n",
      "Params: tensor([  4.3899, -11.7696])\n",
      "Gradient: tensor([-0.1664,  0.9419])\n",
      "Epoch 679, Loss 5.611260\n",
      "Params: tensor([  4.3916, -11.7790])\n",
      "Gradient: tensor([-0.1661,  0.9403])\n",
      "Epoch 680, Loss 5.602148\n",
      "Params: tensor([  4.3932, -11.7883])\n",
      "Gradient: tensor([-0.1658,  0.9387])\n",
      "Epoch 681, Loss 5.593071\n",
      "Params: tensor([  4.3949, -11.7977])\n",
      "Gradient: tensor([-0.1656,  0.9371])\n",
      "Epoch 682, Loss 5.584022\n",
      "Params: tensor([  4.3965, -11.8071])\n",
      "Gradient: tensor([-0.1653,  0.9355])\n",
      "Epoch 683, Loss 5.575005\n",
      "Params: tensor([  4.3982, -11.8164])\n",
      "Gradient: tensor([-0.1650,  0.9339])\n",
      "Epoch 684, Loss 5.566019\n",
      "Params: tensor([  4.3998, -11.8257])\n",
      "Gradient: tensor([-0.1647,  0.9323])\n",
      "Epoch 685, Loss 5.557063\n",
      "Params: tensor([  4.4015, -11.8350])\n",
      "Gradient: tensor([-0.1644,  0.9308])\n",
      "Epoch 686, Loss 5.548136\n",
      "Params: tensor([  4.4031, -11.8443])\n",
      "Gradient: tensor([-0.1641,  0.9292])\n",
      "Epoch 687, Loss 5.539241\n",
      "Params: tensor([  4.4048, -11.8536])\n",
      "Gradient: tensor([-0.1639,  0.9276])\n",
      "Epoch 688, Loss 5.530375\n",
      "Params: tensor([  4.4064, -11.8629])\n",
      "Gradient: tensor([-0.1636,  0.9260])\n",
      "Epoch 689, Loss 5.521540\n",
      "Params: tensor([  4.4080, -11.8721])\n",
      "Gradient: tensor([-0.1633,  0.9245])\n",
      "Epoch 690, Loss 5.512733\n",
      "Params: tensor([  4.4097, -11.8813])\n",
      "Gradient: tensor([-0.1630,  0.9229])\n",
      "Epoch 691, Loss 5.503958\n",
      "Params: tensor([  4.4113, -11.8906])\n",
      "Gradient: tensor([-0.1628,  0.9213])\n",
      "Epoch 692, Loss 5.495212\n",
      "Params: tensor([  4.4129, -11.8998])\n",
      "Gradient: tensor([-0.1625,  0.9197])\n",
      "Epoch 693, Loss 5.486496\n",
      "Params: tensor([  4.4145, -11.9089])\n",
      "Gradient: tensor([-0.1622,  0.9182])\n",
      "Epoch 694, Loss 5.477809\n",
      "Params: tensor([  4.4161, -11.9181])\n",
      "Gradient: tensor([-0.1619,  0.9166])\n",
      "Epoch 695, Loss 5.469152\n",
      "Params: tensor([  4.4178, -11.9272])\n",
      "Gradient: tensor([-0.1617,  0.9151])\n",
      "Epoch 696, Loss 5.460525\n",
      "Params: tensor([  4.4194, -11.9364])\n",
      "Gradient: tensor([-0.1614,  0.9135])\n",
      "Epoch 697, Loss 5.451928\n",
      "Params: tensor([  4.4210, -11.9455])\n",
      "Gradient: tensor([-0.1611,  0.9120])\n",
      "Epoch 698, Loss 5.443358\n",
      "Params: tensor([  4.4226, -11.9546])\n",
      "Gradient: tensor([-0.1608,  0.9104])\n",
      "Epoch 699, Loss 5.434820\n",
      "Params: tensor([  4.4242, -11.9637])\n",
      "Gradient: tensor([-0.1605,  0.9089])\n",
      "Epoch 700, Loss 5.426309\n",
      "Params: tensor([  4.4258, -11.9728])\n",
      "Gradient: tensor([-0.1603,  0.9073])\n",
      "Epoch 701, Loss 5.417827\n",
      "Params: tensor([  4.4274, -11.9818])\n",
      "Gradient: tensor([-0.1600,  0.9058])\n",
      "Epoch 702, Loss 5.409372\n",
      "Params: tensor([  4.4290, -11.9909])\n",
      "Gradient: tensor([-0.1597,  0.9042])\n",
      "Epoch 703, Loss 5.400949\n",
      "Params: tensor([  4.4306, -11.9999])\n",
      "Gradient: tensor([-0.1595,  0.9027])\n",
      "Epoch 704, Loss 5.392551\n",
      "Params: tensor([  4.4322, -12.0089])\n",
      "Gradient: tensor([-0.1592,  0.9012])\n",
      "Epoch 705, Loss 5.384184\n",
      "Params: tensor([  4.4338, -12.0179])\n",
      "Gradient: tensor([-0.1589,  0.8996])\n",
      "Epoch 706, Loss 5.375846\n",
      "Params: tensor([  4.4354, -12.0269])\n",
      "Gradient: tensor([-0.1586,  0.8981])\n",
      "Epoch 707, Loss 5.367537\n",
      "Params: tensor([  4.4369, -12.0359])\n",
      "Gradient: tensor([-0.1584,  0.8966])\n",
      "Epoch 708, Loss 5.359253\n",
      "Params: tensor([  4.4385, -12.0448])\n",
      "Gradient: tensor([-0.1581,  0.8951])\n",
      "Epoch 709, Loss 5.350998\n",
      "Params: tensor([  4.4401, -12.0537])\n",
      "Gradient: tensor([-0.1578,  0.8935])\n",
      "Epoch 710, Loss 5.342772\n",
      "Params: tensor([  4.4417, -12.0627])\n",
      "Gradient: tensor([-0.1576,  0.8920])\n",
      "Epoch 711, Loss 5.334575\n",
      "Params: tensor([  4.4433, -12.0716])\n",
      "Gradient: tensor([-0.1573,  0.8905])\n",
      "Epoch 712, Loss 5.326403\n",
      "Params: tensor([  4.4448, -12.0805])\n",
      "Gradient: tensor([-0.1570,  0.8890])\n",
      "Epoch 713, Loss 5.318260\n",
      "Params: tensor([  4.4464, -12.0893])\n",
      "Gradient: tensor([-0.1568,  0.8875])\n",
      "Epoch 714, Loss 5.310144\n",
      "Params: tensor([  4.4480, -12.0982])\n",
      "Gradient: tensor([-0.1565,  0.8860])\n",
      "Epoch 715, Loss 5.302056\n",
      "Params: tensor([  4.4495, -12.1070])\n",
      "Gradient: tensor([-0.1562,  0.8845])\n",
      "Epoch 716, Loss 5.293994\n",
      "Params: tensor([  4.4511, -12.1159])\n",
      "Gradient: tensor([-0.1560,  0.8830])\n",
      "Epoch 717, Loss 5.285964\n",
      "Params: tensor([  4.4526, -12.1247])\n",
      "Gradient: tensor([-0.1557,  0.8815])\n",
      "Epoch 718, Loss 5.277957\n",
      "Params: tensor([  4.4542, -12.1335])\n",
      "Gradient: tensor([-0.1555,  0.8800])\n",
      "Epoch 719, Loss 5.269979\n",
      "Params: tensor([  4.4557, -12.1423])\n",
      "Gradient: tensor([-0.1552,  0.8785])\n",
      "Epoch 720, Loss 5.262026\n",
      "Params: tensor([  4.4573, -12.1510])\n",
      "Gradient: tensor([-0.1549,  0.8770])\n",
      "Epoch 721, Loss 5.254103\n",
      "Params: tensor([  4.4588, -12.1598])\n",
      "Gradient: tensor([-0.1547,  0.8755])\n",
      "Epoch 722, Loss 5.246205\n",
      "Params: tensor([  4.4604, -12.1685])\n",
      "Gradient: tensor([-0.1544,  0.8740])\n",
      "Epoch 723, Loss 5.238335\n",
      "Params: tensor([  4.4619, -12.1773])\n",
      "Gradient: tensor([-0.1541,  0.8725])\n",
      "Epoch 724, Loss 5.230491\n",
      "Params: tensor([  4.4635, -12.1860])\n",
      "Gradient: tensor([-0.1539,  0.8710])\n",
      "Epoch 725, Loss 5.222673\n",
      "Params: tensor([  4.4650, -12.1947])\n",
      "Gradient: tensor([-0.1536,  0.8696])\n",
      "Epoch 726, Loss 5.214881\n",
      "Params: tensor([  4.4665, -12.2033])\n",
      "Gradient: tensor([-0.1533,  0.8681])\n",
      "Epoch 727, Loss 5.207120\n",
      "Params: tensor([  4.4681, -12.2120])\n",
      "Gradient: tensor([-0.1531,  0.8666])\n",
      "Epoch 728, Loss 5.199380\n",
      "Params: tensor([  4.4696, -12.2207])\n",
      "Gradient: tensor([-0.1528,  0.8651])\n",
      "Epoch 729, Loss 5.191670\n",
      "Params: tensor([  4.4711, -12.2293])\n",
      "Gradient: tensor([-0.1526,  0.8637])\n",
      "Epoch 730, Loss 5.183984\n",
      "Params: tensor([  4.4726, -12.2379])\n",
      "Gradient: tensor([-0.1523,  0.8622])\n",
      "Epoch 731, Loss 5.176325\n",
      "Params: tensor([  4.4742, -12.2465])\n",
      "Gradient: tensor([-0.1520,  0.8607])\n",
      "Epoch 732, Loss 5.168688\n",
      "Params: tensor([  4.4757, -12.2551])\n",
      "Gradient: tensor([-0.1518,  0.8593])\n",
      "Epoch 733, Loss 5.161084\n",
      "Params: tensor([  4.4772, -12.2637])\n",
      "Gradient: tensor([-0.1515,  0.8578])\n",
      "Epoch 734, Loss 5.153500\n",
      "Params: tensor([  4.4787, -12.2723])\n",
      "Gradient: tensor([-0.1513,  0.8564])\n",
      "Epoch 735, Loss 5.145943\n",
      "Params: tensor([  4.4802, -12.2808])\n",
      "Gradient: tensor([-0.1510,  0.8549])\n",
      "Epoch 736, Loss 5.138413\n",
      "Params: tensor([  4.4817, -12.2893])\n",
      "Gradient: tensor([-0.1508,  0.8535])\n",
      "Epoch 737, Loss 5.130909\n",
      "Params: tensor([  4.4832, -12.2979])\n",
      "Gradient: tensor([-0.1505,  0.8520])\n",
      "Epoch 738, Loss 5.123428\n",
      "Params: tensor([  4.4847, -12.3064])\n",
      "Gradient: tensor([-0.1502,  0.8506])\n",
      "Epoch 739, Loss 5.115978\n",
      "Params: tensor([  4.4862, -12.3149])\n",
      "Gradient: tensor([-0.1500,  0.8491])\n",
      "Epoch 740, Loss 5.108547\n",
      "Params: tensor([  4.4877, -12.3233])\n",
      "Gradient: tensor([-0.1497,  0.8477])\n",
      "Epoch 741, Loss 5.101143\n",
      "Params: tensor([  4.4892, -12.3318])\n",
      "Gradient: tensor([-0.1495,  0.8462])\n",
      "Epoch 742, Loss 5.093765\n",
      "Params: tensor([  4.4907, -12.3402])\n",
      "Gradient: tensor([-0.1492,  0.8448])\n",
      "Epoch 743, Loss 5.086412\n",
      "Params: tensor([  4.4922, -12.3487])\n",
      "Gradient: tensor([-0.1490,  0.8434])\n",
      "Epoch 744, Loss 5.079086\n",
      "Params: tensor([  4.4937, -12.3571])\n",
      "Gradient: tensor([-0.1487,  0.8419])\n",
      "Epoch 745, Loss 5.071782\n",
      "Params: tensor([  4.4952, -12.3655])\n",
      "Gradient: tensor([-0.1485,  0.8405])\n",
      "Epoch 746, Loss 5.064505\n",
      "Params: tensor([  4.4967, -12.3739])\n",
      "Gradient: tensor([-0.1482,  0.8391])\n",
      "Epoch 747, Loss 5.057247\n",
      "Params: tensor([  4.4981, -12.3823])\n",
      "Gradient: tensor([-0.1480,  0.8376])\n",
      "Epoch 748, Loss 5.050022\n",
      "Params: tensor([  4.4996, -12.3906])\n",
      "Gradient: tensor([-0.1477,  0.8362])\n",
      "Epoch 749, Loss 5.042817\n",
      "Params: tensor([  4.5011, -12.3990])\n",
      "Gradient: tensor([-0.1475,  0.8348])\n",
      "Epoch 750, Loss 5.035636\n",
      "Params: tensor([  4.5026, -12.4073])\n",
      "Gradient: tensor([-0.1472,  0.8334])\n",
      "Epoch 751, Loss 5.028476\n",
      "Params: tensor([  4.5040, -12.4156])\n",
      "Gradient: tensor([-0.1470,  0.8320])\n",
      "Epoch 752, Loss 5.021347\n",
      "Params: tensor([  4.5055, -12.4239])\n",
      "Gradient: tensor([-0.1467,  0.8305])\n",
      "Epoch 753, Loss 5.014239\n",
      "Params: tensor([  4.5070, -12.4322])\n",
      "Gradient: tensor([-0.1465,  0.8291])\n",
      "Epoch 754, Loss 5.007157\n",
      "Params: tensor([  4.5084, -12.4405])\n",
      "Gradient: tensor([-0.1462,  0.8277])\n",
      "Epoch 755, Loss 5.000099\n",
      "Params: tensor([  4.5099, -12.4488])\n",
      "Gradient: tensor([-0.1460,  0.8263])\n",
      "Epoch 756, Loss 4.993064\n",
      "Params: tensor([  4.5113, -12.4570])\n",
      "Gradient: tensor([-0.1457,  0.8249])\n",
      "Epoch 757, Loss 4.986051\n",
      "Params: tensor([  4.5128, -12.4653])\n",
      "Gradient: tensor([-0.1455,  0.8235])\n",
      "Epoch 758, Loss 4.979064\n",
      "Params: tensor([  4.5143, -12.4735])\n",
      "Gradient: tensor([-0.1452,  0.8221])\n",
      "Epoch 759, Loss 4.972100\n",
      "Params: tensor([  4.5157, -12.4817])\n",
      "Gradient: tensor([-0.1450,  0.8207])\n",
      "Epoch 760, Loss 4.965159\n",
      "Params: tensor([  4.5172, -12.4899])\n",
      "Gradient: tensor([-0.1447,  0.8193])\n",
      "Epoch 761, Loss 4.958245\n",
      "Params: tensor([  4.5186, -12.4981])\n",
      "Gradient: tensor([-0.1445,  0.8179])\n",
      "Epoch 762, Loss 4.951350\n",
      "Params: tensor([  4.5200, -12.5062])\n",
      "Gradient: tensor([-0.1443,  0.8165])\n",
      "Epoch 763, Loss 4.944479\n",
      "Params: tensor([  4.5215, -12.5144])\n",
      "Gradient: tensor([-0.1440,  0.8152])\n",
      "Epoch 764, Loss 4.937633\n",
      "Params: tensor([  4.5229, -12.5225])\n",
      "Gradient: tensor([-0.1438,  0.8138])\n",
      "Epoch 765, Loss 4.930812\n",
      "Params: tensor([  4.5244, -12.5306])\n",
      "Gradient: tensor([-0.1435,  0.8124])\n",
      "Epoch 766, Loss 4.924010\n",
      "Params: tensor([  4.5258, -12.5387])\n",
      "Gradient: tensor([-0.1433,  0.8110])\n",
      "Epoch 767, Loss 4.917233\n",
      "Params: tensor([  4.5272, -12.5468])\n",
      "Gradient: tensor([-0.1430,  0.8096])\n",
      "Epoch 768, Loss 4.910480\n",
      "Params: tensor([  4.5286, -12.5549])\n",
      "Gradient: tensor([-0.1428,  0.8083])\n",
      "Epoch 769, Loss 4.903749\n",
      "Params: tensor([  4.5301, -12.5630])\n",
      "Gradient: tensor([-0.1426,  0.8069])\n",
      "Epoch 770, Loss 4.897039\n",
      "Params: tensor([  4.5315, -12.5711])\n",
      "Gradient: tensor([-0.1423,  0.8055])\n",
      "Epoch 771, Loss 4.890356\n",
      "Params: tensor([  4.5329, -12.5791])\n",
      "Gradient: tensor([-0.1420,  0.8042])\n",
      "Epoch 772, Loss 4.883691\n",
      "Params: tensor([  4.5343, -12.5871])\n",
      "Gradient: tensor([-0.1418,  0.8028])\n",
      "Epoch 773, Loss 4.877052\n",
      "Params: tensor([  4.5357, -12.5951])\n",
      "Gradient: tensor([-0.1416,  0.8014])\n",
      "Epoch 774, Loss 4.870436\n",
      "Params: tensor([  4.5372, -12.6031])\n",
      "Gradient: tensor([-0.1413,  0.8001])\n",
      "Epoch 775, Loss 4.863839\n",
      "Params: tensor([  4.5386, -12.6111])\n",
      "Gradient: tensor([-0.1411,  0.7987])\n",
      "Epoch 776, Loss 4.857267\n",
      "Params: tensor([  4.5400, -12.6191])\n",
      "Gradient: tensor([-0.1408,  0.7973])\n",
      "Epoch 777, Loss 4.850717\n",
      "Params: tensor([  4.5414, -12.6271])\n",
      "Gradient: tensor([-0.1406,  0.7960])\n",
      "Epoch 778, Loss 4.844189\n",
      "Params: tensor([  4.5428, -12.6350])\n",
      "Gradient: tensor([-0.1404,  0.7946])\n",
      "Epoch 779, Loss 4.837683\n",
      "Params: tensor([  4.5442, -12.6429])\n",
      "Gradient: tensor([-0.1401,  0.7933])\n",
      "Epoch 780, Loss 4.831196\n",
      "Params: tensor([  4.5456, -12.6509])\n",
      "Gradient: tensor([-0.1399,  0.7919])\n",
      "Epoch 781, Loss 4.824737\n",
      "Params: tensor([  4.5470, -12.6588])\n",
      "Gradient: tensor([-0.1397,  0.7906])\n",
      "Epoch 782, Loss 4.818298\n",
      "Params: tensor([  4.5484, -12.6667])\n",
      "Gradient: tensor([-0.1394,  0.7892])\n",
      "Epoch 783, Loss 4.811880\n",
      "Params: tensor([  4.5498, -12.6745])\n",
      "Gradient: tensor([-0.1392,  0.7879])\n",
      "Epoch 784, Loss 4.805481\n",
      "Params: tensor([  4.5512, -12.6824])\n",
      "Gradient: tensor([-0.1389,  0.7866])\n",
      "Epoch 785, Loss 4.799106\n",
      "Params: tensor([  4.5525, -12.6902])\n",
      "Gradient: tensor([-0.1387,  0.7852])\n",
      "Epoch 786, Loss 4.792755\n",
      "Params: tensor([  4.5539, -12.6981])\n",
      "Gradient: tensor([-0.1385,  0.7839])\n",
      "Epoch 787, Loss 4.786422\n",
      "Params: tensor([  4.5553, -12.7059])\n",
      "Gradient: tensor([-0.1383,  0.7826])\n",
      "Epoch 788, Loss 4.780111\n",
      "Params: tensor([  4.5567, -12.7137])\n",
      "Gradient: tensor([-0.1380,  0.7812])\n",
      "Epoch 789, Loss 4.773824\n",
      "Params: tensor([  4.5581, -12.7215])\n",
      "Gradient: tensor([-0.1378,  0.7799])\n",
      "Epoch 790, Loss 4.767559\n",
      "Params: tensor([  4.5594, -12.7293])\n",
      "Gradient: tensor([-0.1375,  0.7786])\n",
      "Epoch 791, Loss 4.761311\n",
      "Params: tensor([  4.5608, -12.7371])\n",
      "Gradient: tensor([-0.1373,  0.7773])\n",
      "Epoch 792, Loss 4.755087\n",
      "Params: tensor([  4.5622, -12.7448])\n",
      "Gradient: tensor([-0.1371,  0.7759])\n",
      "Epoch 793, Loss 4.748885\n",
      "Params: tensor([  4.5636, -12.7526])\n",
      "Gradient: tensor([-0.1368,  0.7746])\n",
      "Epoch 794, Loss 4.742701\n",
      "Params: tensor([  4.5649, -12.7603])\n",
      "Gradient: tensor([-0.1366,  0.7733])\n",
      "Epoch 795, Loss 4.736537\n",
      "Params: tensor([  4.5663, -12.7680])\n",
      "Gradient: tensor([-0.1364,  0.7720])\n",
      "Epoch 796, Loss 4.730397\n",
      "Params: tensor([  4.5677, -12.7758])\n",
      "Gradient: tensor([-0.1361,  0.7707])\n",
      "Epoch 797, Loss 4.724279\n",
      "Params: tensor([  4.5690, -12.7834])\n",
      "Gradient: tensor([-0.1359,  0.7694])\n",
      "Epoch 798, Loss 4.718181\n",
      "Params: tensor([  4.5704, -12.7911])\n",
      "Gradient: tensor([-0.1357,  0.7681])\n",
      "Epoch 799, Loss 4.712101\n",
      "Params: tensor([  4.5717, -12.7988])\n",
      "Gradient: tensor([-0.1354,  0.7668])\n",
      "Epoch 800, Loss 4.706046\n",
      "Params: tensor([  4.5731, -12.8064])\n",
      "Gradient: tensor([-0.1352,  0.7655])\n",
      "Epoch 801, Loss 4.700009\n",
      "Params: tensor([  4.5744, -12.8141])\n",
      "Gradient: tensor([-0.1350,  0.7642])\n",
      "Epoch 802, Loss 4.693989\n",
      "Params: tensor([  4.5758, -12.8217])\n",
      "Gradient: tensor([-0.1347,  0.7629])\n",
      "Epoch 803, Loss 4.687995\n",
      "Params: tensor([  4.5771, -12.8293])\n",
      "Gradient: tensor([-0.1345,  0.7616])\n",
      "Epoch 804, Loss 4.682019\n",
      "Params: tensor([  4.5785, -12.8369])\n",
      "Gradient: tensor([-0.1343,  0.7603])\n",
      "Epoch 805, Loss 4.676063\n",
      "Params: tensor([  4.5798, -12.8445])\n",
      "Gradient: tensor([-0.1341,  0.7590])\n",
      "Epoch 806, Loss 4.670130\n",
      "Params: tensor([  4.5811, -12.8521])\n",
      "Gradient: tensor([-0.1338,  0.7577])\n",
      "Epoch 807, Loss 4.664214\n",
      "Params: tensor([  4.5825, -12.8597])\n",
      "Gradient: tensor([-0.1336,  0.7564])\n",
      "Epoch 808, Loss 4.658319\n",
      "Params: tensor([  4.5838, -12.8672])\n",
      "Gradient: tensor([-0.1334,  0.7551])\n",
      "Epoch 809, Loss 4.652445\n",
      "Params: tensor([  4.5851, -12.8748])\n",
      "Gradient: tensor([-0.1332,  0.7538])\n",
      "Epoch 810, Loss 4.646592\n",
      "Params: tensor([  4.5865, -12.8823])\n",
      "Gradient: tensor([-0.1330,  0.7526])\n",
      "Epoch 811, Loss 4.640754\n",
      "Params: tensor([  4.5878, -12.8898])\n",
      "Gradient: tensor([-0.1327,  0.7513])\n",
      "Epoch 812, Loss 4.634938\n",
      "Params: tensor([  4.5891, -12.8973])\n",
      "Gradient: tensor([-0.1325,  0.7500])\n",
      "Epoch 813, Loss 4.629143\n",
      "Params: tensor([  4.5904, -12.9048])\n",
      "Gradient: tensor([-0.1323,  0.7487])\n",
      "Epoch 814, Loss 4.623367\n",
      "Params: tensor([  4.5918, -12.9123])\n",
      "Gradient: tensor([-0.1320,  0.7475])\n",
      "Epoch 815, Loss 4.617611\n",
      "Params: tensor([  4.5931, -12.9197])\n",
      "Gradient: tensor([-0.1318,  0.7462])\n",
      "Epoch 816, Loss 4.611873\n",
      "Params: tensor([  4.5944, -12.9272])\n",
      "Gradient: tensor([-0.1316,  0.7449])\n",
      "Epoch 817, Loss 4.606156\n",
      "Params: tensor([  4.5957, -12.9346])\n",
      "Gradient: tensor([-0.1314,  0.7437])\n",
      "Epoch 818, Loss 4.600458\n",
      "Params: tensor([  4.5970, -12.9420])\n",
      "Gradient: tensor([-0.1311,  0.7424])\n",
      "Epoch 819, Loss 4.594779\n",
      "Params: tensor([  4.5983, -12.9494])\n",
      "Gradient: tensor([-0.1309,  0.7411])\n",
      "Epoch 820, Loss 4.589119\n",
      "Params: tensor([  4.5996, -12.9568])\n",
      "Gradient: tensor([-0.1307,  0.7399])\n",
      "Epoch 821, Loss 4.583479\n",
      "Params: tensor([  4.6009, -12.9642])\n",
      "Gradient: tensor([-0.1305,  0.7386])\n",
      "Epoch 822, Loss 4.577857\n",
      "Params: tensor([  4.6022, -12.9716])\n",
      "Gradient: tensor([-0.1303,  0.7374])\n",
      "Epoch 823, Loss 4.572256\n",
      "Params: tensor([  4.6035, -12.9790])\n",
      "Gradient: tensor([-0.1300,  0.7361])\n",
      "Epoch 824, Loss 4.566675\n",
      "Params: tensor([  4.6048, -12.9863])\n",
      "Gradient: tensor([-0.1298,  0.7349])\n",
      "Epoch 825, Loss 4.561109\n",
      "Params: tensor([  4.6061, -12.9936])\n",
      "Gradient: tensor([-0.1296,  0.7336])\n",
      "Epoch 826, Loss 4.555566\n",
      "Params: tensor([  4.6074, -13.0010])\n",
      "Gradient: tensor([-0.1294,  0.7324])\n",
      "Epoch 827, Loss 4.550039\n",
      "Params: tensor([  4.6087, -13.0083])\n",
      "Gradient: tensor([-0.1292,  0.7311])\n",
      "Epoch 828, Loss 4.544533\n",
      "Params: tensor([  4.6100, -13.0156])\n",
      "Gradient: tensor([-0.1289,  0.7299])\n",
      "Epoch 829, Loss 4.539044\n",
      "Params: tensor([  4.6113, -13.0229])\n",
      "Gradient: tensor([-0.1287,  0.7286])\n",
      "Epoch 830, Loss 4.533575\n",
      "Params: tensor([  4.6126, -13.0301])\n",
      "Gradient: tensor([-0.1285,  0.7274])\n",
      "Epoch 831, Loss 4.528121\n",
      "Params: tensor([  4.6139, -13.0374])\n",
      "Gradient: tensor([-0.1283,  0.7262])\n",
      "Epoch 832, Loss 4.522690\n",
      "Params: tensor([  4.6152, -13.0446])\n",
      "Gradient: tensor([-0.1280,  0.7249])\n",
      "Epoch 833, Loss 4.517276\n",
      "Params: tensor([  4.6164, -13.0519])\n",
      "Gradient: tensor([-0.1278,  0.7237])\n",
      "Epoch 834, Loss 4.511879\n",
      "Params: tensor([  4.6177, -13.0591])\n",
      "Gradient: tensor([-0.1276,  0.7225])\n",
      "Epoch 835, Loss 4.506504\n",
      "Params: tensor([  4.6190, -13.0663])\n",
      "Gradient: tensor([-0.1274,  0.7212])\n",
      "Epoch 836, Loss 4.501141\n",
      "Params: tensor([  4.6203, -13.0735])\n",
      "Gradient: tensor([-0.1272,  0.7200])\n",
      "Epoch 837, Loss 4.495801\n",
      "Params: tensor([  4.6215, -13.0807])\n",
      "Gradient: tensor([-0.1270,  0.7188])\n",
      "Epoch 838, Loss 4.490474\n",
      "Params: tensor([  4.6228, -13.0879])\n",
      "Gradient: tensor([-0.1268,  0.7176])\n",
      "Epoch 839, Loss 4.485169\n",
      "Params: tensor([  4.6241, -13.0950])\n",
      "Gradient: tensor([-0.1266,  0.7163])\n",
      "Epoch 840, Loss 4.479884\n",
      "Params: tensor([  4.6253, -13.1022])\n",
      "Gradient: tensor([-0.1263,  0.7151])\n",
      "Epoch 841, Loss 4.474614\n",
      "Params: tensor([  4.6266, -13.1093])\n",
      "Gradient: tensor([-0.1261,  0.7139])\n",
      "Epoch 842, Loss 4.469364\n",
      "Params: tensor([  4.6278, -13.1165])\n",
      "Gradient: tensor([-0.1259,  0.7127])\n",
      "Epoch 843, Loss 4.464129\n",
      "Params: tensor([  4.6291, -13.1236])\n",
      "Gradient: tensor([-0.1257,  0.7115])\n",
      "Epoch 844, Loss 4.458914\n",
      "Params: tensor([  4.6304, -13.1307])\n",
      "Gradient: tensor([-0.1255,  0.7103])\n",
      "Epoch 845, Loss 4.453716\n",
      "Params: tensor([  4.6316, -13.1378])\n",
      "Gradient: tensor([-0.1253,  0.7091])\n",
      "Epoch 846, Loss 4.448535\n",
      "Params: tensor([  4.6329, -13.1449])\n",
      "Gradient: tensor([-0.1250,  0.7079])\n",
      "Epoch 847, Loss 4.443373\n",
      "Params: tensor([  4.6341, -13.1519])\n",
      "Gradient: tensor([-0.1249,  0.7067])\n",
      "Epoch 848, Loss 4.438227\n",
      "Params: tensor([  4.6353, -13.1590])\n",
      "Gradient: tensor([-0.1246,  0.7055])\n",
      "Epoch 849, Loss 4.433099\n",
      "Params: tensor([  4.6366, -13.1660])\n",
      "Gradient: tensor([-0.1244,  0.7043])\n",
      "Epoch 850, Loss 4.427989\n",
      "Params: tensor([  4.6378, -13.1730])\n",
      "Gradient: tensor([-0.1242,  0.7031])\n",
      "Epoch 851, Loss 4.422897\n",
      "Params: tensor([  4.6391, -13.1801])\n",
      "Gradient: tensor([-0.1240,  0.7019])\n",
      "Epoch 852, Loss 4.417819\n",
      "Params: tensor([  4.6403, -13.1871])\n",
      "Gradient: tensor([-0.1238,  0.7007])\n",
      "Epoch 853, Loss 4.412763\n",
      "Params: tensor([  4.6415, -13.1941])\n",
      "Gradient: tensor([-0.1236,  0.6995])\n",
      "Epoch 854, Loss 4.407720\n",
      "Params: tensor([  4.6428, -13.2010])\n",
      "Gradient: tensor([-0.1234,  0.6983])\n",
      "Epoch 855, Loss 4.402697\n",
      "Params: tensor([  4.6440, -13.2080])\n",
      "Gradient: tensor([-0.1232,  0.6971])\n",
      "Epoch 856, Loss 4.397688\n",
      "Params: tensor([  4.6452, -13.2150])\n",
      "Gradient: tensor([-0.1229,  0.6959])\n",
      "Epoch 857, Loss 4.392697\n",
      "Params: tensor([  4.6465, -13.2219])\n",
      "Gradient: tensor([-0.1227,  0.6948])\n",
      "Epoch 858, Loss 4.387725\n",
      "Params: tensor([  4.6477, -13.2289])\n",
      "Gradient: tensor([-0.1225,  0.6936])\n",
      "Epoch 859, Loss 4.382770\n",
      "Params: tensor([  4.6489, -13.2358])\n",
      "Gradient: tensor([-0.1223,  0.6924])\n",
      "Epoch 860, Loss 4.377828\n",
      "Params: tensor([  4.6501, -13.2427])\n",
      "Gradient: tensor([-0.1221,  0.6912])\n",
      "Epoch 861, Loss 4.372905\n",
      "Params: tensor([  4.6514, -13.2496])\n",
      "Gradient: tensor([-0.1219,  0.6901])\n",
      "Epoch 862, Loss 4.368000\n",
      "Params: tensor([  4.6526, -13.2565])\n",
      "Gradient: tensor([-0.1217,  0.6889])\n",
      "Epoch 863, Loss 4.363111\n",
      "Params: tensor([  4.6538, -13.2634])\n",
      "Gradient: tensor([-0.1215,  0.6877])\n",
      "Epoch 864, Loss 4.358238\n",
      "Params: tensor([  4.6550, -13.2702])\n",
      "Gradient: tensor([-0.1213,  0.6865])\n",
      "Epoch 865, Loss 4.353383\n",
      "Params: tensor([  4.6562, -13.2771])\n",
      "Gradient: tensor([-0.1211,  0.6854])\n",
      "Epoch 866, Loss 4.348542\n",
      "Params: tensor([  4.6574, -13.2839])\n",
      "Gradient: tensor([-0.1209,  0.6842])\n",
      "Epoch 867, Loss 4.343716\n",
      "Params: tensor([  4.6586, -13.2908])\n",
      "Gradient: tensor([-0.1207,  0.6830])\n",
      "Epoch 868, Loss 4.338911\n",
      "Params: tensor([  4.6598, -13.2976])\n",
      "Gradient: tensor([-0.1205,  0.6819])\n",
      "Epoch 869, Loss 4.334121\n",
      "Params: tensor([  4.6610, -13.3044])\n",
      "Gradient: tensor([-0.1203,  0.6807])\n",
      "Epoch 870, Loss 4.329345\n",
      "Params: tensor([  4.6622, -13.3112])\n",
      "Gradient: tensor([-0.1201,  0.6796])\n",
      "Epoch 871, Loss 4.324588\n",
      "Params: tensor([  4.6634, -13.3180])\n",
      "Gradient: tensor([-0.1198,  0.6784])\n",
      "Epoch 872, Loss 4.319846\n",
      "Params: tensor([  4.6646, -13.3247])\n",
      "Gradient: tensor([-0.1196,  0.6773])\n",
      "Epoch 873, Loss 4.315117\n",
      "Params: tensor([  4.6658, -13.3315])\n",
      "Gradient: tensor([-0.1195,  0.6761])\n",
      "Epoch 874, Loss 4.310409\n",
      "Params: tensor([  4.6670, -13.3382])\n",
      "Gradient: tensor([-0.1192,  0.6750])\n",
      "Epoch 875, Loss 4.305714\n",
      "Params: tensor([  4.6682, -13.3450])\n",
      "Gradient: tensor([-0.1190,  0.6738])\n",
      "Epoch 876, Loss 4.301035\n",
      "Params: tensor([  4.6694, -13.3517])\n",
      "Gradient: tensor([-0.1188,  0.6727])\n",
      "Epoch 877, Loss 4.296376\n",
      "Params: tensor([  4.6706, -13.3584])\n",
      "Gradient: tensor([-0.1186,  0.6715])\n",
      "Epoch 878, Loss 4.291727\n",
      "Params: tensor([  4.6718, -13.3651])\n",
      "Gradient: tensor([-0.1184,  0.6704])\n",
      "Epoch 879, Loss 4.287097\n",
      "Params: tensor([  4.6730, -13.3718])\n",
      "Gradient: tensor([-0.1182,  0.6693])\n",
      "Epoch 880, Loss 4.282482\n",
      "Params: tensor([  4.6741, -13.3785])\n",
      "Gradient: tensor([-0.1180,  0.6681])\n",
      "Epoch 881, Loss 4.277882\n",
      "Params: tensor([  4.6753, -13.3852])\n",
      "Gradient: tensor([-0.1178,  0.6670])\n",
      "Epoch 882, Loss 4.273300\n",
      "Params: tensor([  4.6765, -13.3918])\n",
      "Gradient: tensor([-0.1176,  0.6658])\n",
      "Epoch 883, Loss 4.268732\n",
      "Params: tensor([  4.6777, -13.3985])\n",
      "Gradient: tensor([-0.1174,  0.6647])\n",
      "Epoch 884, Loss 4.264178\n",
      "Params: tensor([  4.6788, -13.4051])\n",
      "Gradient: tensor([-0.1172,  0.6636])\n",
      "Epoch 885, Loss 4.259643\n",
      "Params: tensor([  4.6800, -13.4117])\n",
      "Gradient: tensor([-0.1170,  0.6625])\n",
      "Epoch 886, Loss 4.255120\n",
      "Params: tensor([  4.6812, -13.4184])\n",
      "Gradient: tensor([-0.1168,  0.6613])\n",
      "Epoch 887, Loss 4.250613\n",
      "Params: tensor([  4.6823, -13.4250])\n",
      "Gradient: tensor([-0.1166,  0.6602])\n",
      "Epoch 888, Loss 4.246124\n",
      "Params: tensor([  4.6835, -13.4316])\n",
      "Gradient: tensor([-0.1164,  0.6591])\n",
      "Epoch 889, Loss 4.241648\n",
      "Params: tensor([  4.6847, -13.4381])\n",
      "Gradient: tensor([-0.1162,  0.6580])\n",
      "Epoch 890, Loss 4.237185\n",
      "Params: tensor([  4.6858, -13.4447])\n",
      "Gradient: tensor([-0.1160,  0.6569])\n",
      "Epoch 891, Loss 4.232740\n",
      "Params: tensor([  4.6870, -13.4513])\n",
      "Gradient: tensor([-0.1158,  0.6557])\n",
      "Epoch 892, Loss 4.228308\n",
      "Params: tensor([  4.6881, -13.4578])\n",
      "Gradient: tensor([-0.1157,  0.6546])\n",
      "Epoch 893, Loss 4.223895\n",
      "Params: tensor([  4.6893, -13.4643])\n",
      "Gradient: tensor([-0.1154,  0.6535])\n",
      "Epoch 894, Loss 4.219494\n",
      "Params: tensor([  4.6904, -13.4709])\n",
      "Gradient: tensor([-0.1153,  0.6524])\n",
      "Epoch 895, Loss 4.215109\n",
      "Params: tensor([  4.6916, -13.4774])\n",
      "Gradient: tensor([-0.1151,  0.6513])\n",
      "Epoch 896, Loss 4.210737\n",
      "Params: tensor([  4.6927, -13.4839])\n",
      "Gradient: tensor([-0.1148,  0.6502])\n",
      "Epoch 897, Loss 4.206383\n",
      "Params: tensor([  4.6939, -13.4904])\n",
      "Gradient: tensor([-0.1147,  0.6491])\n",
      "Epoch 898, Loss 4.202042\n",
      "Params: tensor([  4.6950, -13.4968])\n",
      "Gradient: tensor([-0.1145,  0.6480])\n",
      "Epoch 899, Loss 4.197716\n",
      "Params: tensor([  4.6962, -13.5033])\n",
      "Gradient: tensor([-0.1143,  0.6469])\n",
      "Epoch 900, Loss 4.193405\n",
      "Params: tensor([  4.6973, -13.5098])\n",
      "Gradient: tensor([-0.1141,  0.6458])\n",
      "Epoch 901, Loss 4.189108\n",
      "Params: tensor([  4.6985, -13.5162])\n",
      "Gradient: tensor([-0.1139,  0.6447])\n",
      "Epoch 902, Loss 4.184825\n",
      "Params: tensor([  4.6996, -13.5227])\n",
      "Gradient: tensor([-0.1137,  0.6436])\n",
      "Epoch 903, Loss 4.180559\n",
      "Params: tensor([  4.7007, -13.5291])\n",
      "Gradient: tensor([-0.1135,  0.6425])\n",
      "Epoch 904, Loss 4.176305\n",
      "Params: tensor([  4.7019, -13.5355])\n",
      "Gradient: tensor([-0.1133,  0.6414])\n",
      "Epoch 905, Loss 4.172065\n",
      "Params: tensor([  4.7030, -13.5419])\n",
      "Gradient: tensor([-0.1131,  0.6403])\n",
      "Epoch 906, Loss 4.167842\n",
      "Params: tensor([  4.7041, -13.5483])\n",
      "Gradient: tensor([-0.1129,  0.6392])\n",
      "Epoch 907, Loss 4.163630\n",
      "Params: tensor([  4.7053, -13.5547])\n",
      "Gradient: tensor([-0.1127,  0.6381])\n",
      "Epoch 908, Loss 4.159436\n",
      "Params: tensor([  4.7064, -13.5610])\n",
      "Gradient: tensor([-0.1125,  0.6371])\n",
      "Epoch 909, Loss 4.155253\n",
      "Params: tensor([  4.7075, -13.5674])\n",
      "Gradient: tensor([-0.1124,  0.6360])\n",
      "Epoch 910, Loss 4.151086\n",
      "Params: tensor([  4.7086, -13.5738])\n",
      "Gradient: tensor([-0.1122,  0.6349])\n",
      "Epoch 911, Loss 4.146933\n",
      "Params: tensor([  4.7097, -13.5801])\n",
      "Gradient: tensor([-0.1120,  0.6338])\n",
      "Epoch 912, Loss 4.142794\n",
      "Params: tensor([  4.7109, -13.5864])\n",
      "Gradient: tensor([-0.1118,  0.6327])\n",
      "Epoch 913, Loss 4.138669\n",
      "Params: tensor([  4.7120, -13.5927])\n",
      "Gradient: tensor([-0.1116,  0.6317])\n",
      "Epoch 914, Loss 4.134559\n",
      "Params: tensor([  4.7131, -13.5990])\n",
      "Gradient: tensor([-0.1114,  0.6306])\n",
      "Epoch 915, Loss 4.130464\n",
      "Params: tensor([  4.7142, -13.6053])\n",
      "Gradient: tensor([-0.1112,  0.6295])\n",
      "Epoch 916, Loss 4.126378\n",
      "Params: tensor([  4.7153, -13.6116])\n",
      "Gradient: tensor([-0.1110,  0.6284])\n",
      "Epoch 917, Loss 4.122310\n",
      "Params: tensor([  4.7164, -13.6179])\n",
      "Gradient: tensor([-0.1108,  0.6274])\n",
      "Epoch 918, Loss 4.118253\n",
      "Params: tensor([  4.7175, -13.6242])\n",
      "Gradient: tensor([-0.1107,  0.6263])\n",
      "Epoch 919, Loss 4.114213\n",
      "Params: tensor([  4.7186, -13.6304])\n",
      "Gradient: tensor([-0.1104,  0.6253])\n",
      "Epoch 920, Loss 4.110184\n",
      "Params: tensor([  4.7197, -13.6367])\n",
      "Gradient: tensor([-0.1103,  0.6242])\n",
      "Epoch 921, Loss 4.106169\n",
      "Params: tensor([  4.7208, -13.6429])\n",
      "Gradient: tensor([-0.1101,  0.6231])\n",
      "Epoch 922, Loss 4.102171\n",
      "Params: tensor([  4.7219, -13.6491])\n",
      "Gradient: tensor([-0.1099,  0.6221])\n",
      "Epoch 923, Loss 4.098181\n",
      "Params: tensor([  4.7230, -13.6553])\n",
      "Gradient: tensor([-0.1097,  0.6210])\n",
      "Epoch 924, Loss 4.094209\n",
      "Params: tensor([  4.7241, -13.6615])\n",
      "Gradient: tensor([-0.1095,  0.6200])\n",
      "Epoch 925, Loss 4.090250\n",
      "Params: tensor([  4.7252, -13.6677])\n",
      "Gradient: tensor([-0.1093,  0.6189])\n",
      "Epoch 926, Loss 4.086300\n",
      "Params: tensor([  4.7263, -13.6739])\n",
      "Gradient: tensor([-0.1091,  0.6179])\n",
      "Epoch 927, Loss 4.082366\n",
      "Params: tensor([  4.7274, -13.6800])\n",
      "Gradient: tensor([-0.1090,  0.6168])\n",
      "Epoch 928, Loss 4.078448\n",
      "Params: tensor([  4.7285, -13.6862])\n",
      "Gradient: tensor([-0.1088,  0.6158])\n",
      "Epoch 929, Loss 4.074540\n",
      "Params: tensor([  4.7296, -13.6924])\n",
      "Gradient: tensor([-0.1086,  0.6147])\n",
      "Epoch 930, Loss 4.070649\n",
      "Params: tensor([  4.7307, -13.6985])\n",
      "Gradient: tensor([-0.1084,  0.6137])\n",
      "Epoch 931, Loss 4.066768\n",
      "Params: tensor([  4.7317, -13.7046])\n",
      "Gradient: tensor([-0.1082,  0.6126])\n",
      "Epoch 932, Loss 4.062900\n",
      "Params: tensor([  4.7328, -13.7107])\n",
      "Gradient: tensor([-0.1080,  0.6116])\n",
      "Epoch 933, Loss 4.059047\n",
      "Params: tensor([  4.7339, -13.7168])\n",
      "Gradient: tensor([-0.1079,  0.6105])\n",
      "Epoch 934, Loss 4.055204\n",
      "Params: tensor([  4.7350, -13.7229])\n",
      "Gradient: tensor([-0.1077,  0.6095])\n",
      "Epoch 935, Loss 4.051379\n",
      "Params: tensor([  4.7360, -13.7290])\n",
      "Gradient: tensor([-0.1075,  0.6085])\n",
      "Epoch 936, Loss 4.047564\n",
      "Params: tensor([  4.7371, -13.7351])\n",
      "Gradient: tensor([-0.1073,  0.6074])\n",
      "Epoch 937, Loss 4.043761\n",
      "Params: tensor([  4.7382, -13.7412])\n",
      "Gradient: tensor([-0.1071,  0.6064])\n",
      "Epoch 938, Loss 4.039972\n",
      "Params: tensor([  4.7393, -13.7472])\n",
      "Gradient: tensor([-0.1069,  0.6054])\n",
      "Epoch 939, Loss 4.036196\n",
      "Params: tensor([  4.7403, -13.7533])\n",
      "Gradient: tensor([-0.1068,  0.6043])\n",
      "Epoch 940, Loss 4.032434\n",
      "Params: tensor([  4.7414, -13.7593])\n",
      "Gradient: tensor([-0.1066,  0.6033])\n",
      "Epoch 941, Loss 4.028686\n",
      "Params: tensor([  4.7425, -13.7653])\n",
      "Gradient: tensor([-0.1064,  0.6023])\n",
      "Epoch 942, Loss 4.024947\n",
      "Params: tensor([  4.7435, -13.7713])\n",
      "Gradient: tensor([-0.1062,  0.6013])\n",
      "Epoch 943, Loss 4.021224\n",
      "Params: tensor([  4.7446, -13.7773])\n",
      "Gradient: tensor([-0.1060,  0.6002])\n",
      "Epoch 944, Loss 4.017508\n",
      "Params: tensor([  4.7456, -13.7833])\n",
      "Gradient: tensor([-0.1058,  0.5992])\n",
      "Epoch 945, Loss 4.013810\n",
      "Params: tensor([  4.7467, -13.7893])\n",
      "Gradient: tensor([-0.1057,  0.5982])\n",
      "Epoch 946, Loss 4.010122\n",
      "Params: tensor([  4.7478, -13.7953])\n",
      "Gradient: tensor([-0.1055,  0.5972])\n",
      "Epoch 947, Loss 4.006450\n",
      "Params: tensor([  4.7488, -13.8012])\n",
      "Gradient: tensor([-0.1053,  0.5962])\n",
      "Epoch 948, Loss 4.002785\n",
      "Params: tensor([  4.7499, -13.8072])\n",
      "Gradient: tensor([-0.1051,  0.5952])\n",
      "Epoch 949, Loss 3.999136\n",
      "Params: tensor([  4.7509, -13.8131])\n",
      "Gradient: tensor([-0.1050,  0.5942])\n",
      "Epoch 950, Loss 3.995498\n",
      "Params: tensor([  4.7520, -13.8191])\n",
      "Gradient: tensor([-0.1048,  0.5932])\n",
      "Epoch 951, Loss 3.991874\n",
      "Params: tensor([  4.7530, -13.8250])\n",
      "Gradient: tensor([-0.1046,  0.5921])\n",
      "Epoch 952, Loss 3.988262\n",
      "Params: tensor([  4.7540, -13.8309])\n",
      "Gradient: tensor([-0.1044,  0.5911])\n",
      "Epoch 953, Loss 3.984659\n",
      "Params: tensor([  4.7551, -13.8368])\n",
      "Gradient: tensor([-0.1043,  0.5901])\n",
      "Epoch 954, Loss 3.981071\n",
      "Params: tensor([  4.7561, -13.8427])\n",
      "Gradient: tensor([-0.1041,  0.5891])\n",
      "Epoch 955, Loss 3.977497\n",
      "Params: tensor([  4.7572, -13.8486])\n",
      "Gradient: tensor([-0.1039,  0.5881])\n",
      "Epoch 956, Loss 3.973931\n",
      "Params: tensor([  4.7582, -13.8544])\n",
      "Gradient: tensor([-0.1037,  0.5871])\n",
      "Epoch 957, Loss 3.970381\n",
      "Params: tensor([  4.7592, -13.8603])\n",
      "Gradient: tensor([-0.1035,  0.5861])\n",
      "Epoch 958, Loss 3.966841\n",
      "Params: tensor([  4.7603, -13.8661])\n",
      "Gradient: tensor([-0.1034,  0.5851])\n",
      "Epoch 959, Loss 3.963313\n",
      "Params: tensor([  4.7613, -13.8720])\n",
      "Gradient: tensor([-0.1032,  0.5841])\n",
      "Epoch 960, Loss 3.959797\n",
      "Params: tensor([  4.7623, -13.8778])\n",
      "Gradient: tensor([-0.1030,  0.5831])\n",
      "Epoch 961, Loss 3.956295\n",
      "Params: tensor([  4.7634, -13.8836])\n",
      "Gradient: tensor([-0.1028,  0.5822])\n",
      "Epoch 962, Loss 3.952801\n",
      "Params: tensor([  4.7644, -13.8895])\n",
      "Gradient: tensor([-0.1026,  0.5812])\n",
      "Epoch 963, Loss 3.949323\n",
      "Params: tensor([  4.7654, -13.8953])\n",
      "Gradient: tensor([-0.1025,  0.5802])\n",
      "Epoch 964, Loss 3.945855\n",
      "Params: tensor([  4.7664, -13.9010])\n",
      "Gradient: tensor([-0.1023,  0.5792])\n",
      "Epoch 965, Loss 3.942398\n",
      "Params: tensor([  4.7675, -13.9068])\n",
      "Gradient: tensor([-0.1021,  0.5782])\n",
      "Epoch 966, Loss 3.938954\n",
      "Params: tensor([  4.7685, -13.9126])\n",
      "Gradient: tensor([-0.1020,  0.5772])\n",
      "Epoch 967, Loss 3.935520\n",
      "Params: tensor([  4.7695, -13.9184])\n",
      "Gradient: tensor([-0.1018,  0.5762])\n",
      "Epoch 968, Loss 3.932096\n",
      "Params: tensor([  4.7705, -13.9241])\n",
      "Gradient: tensor([-0.1016,  0.5753])\n",
      "Epoch 969, Loss 3.928688\n",
      "Params: tensor([  4.7715, -13.9299])\n",
      "Gradient: tensor([-0.1014,  0.5743])\n",
      "Epoch 970, Loss 3.925291\n",
      "Params: tensor([  4.7725, -13.9356])\n",
      "Gradient: tensor([-0.1013,  0.5733])\n",
      "Epoch 971, Loss 3.921906\n",
      "Params: tensor([  4.7736, -13.9413])\n",
      "Gradient: tensor([-0.1011,  0.5723])\n",
      "Epoch 972, Loss 3.918528\n",
      "Params: tensor([  4.7746, -13.9470])\n",
      "Gradient: tensor([-0.1009,  0.5714])\n",
      "Epoch 973, Loss 3.915166\n",
      "Params: tensor([  4.7756, -13.9527])\n",
      "Gradient: tensor([-0.1008,  0.5704])\n",
      "Epoch 974, Loss 3.911815\n",
      "Params: tensor([  4.7766, -13.9584])\n",
      "Gradient: tensor([-0.1006,  0.5694])\n",
      "Epoch 975, Loss 3.908474\n",
      "Params: tensor([  4.7776, -13.9641])\n",
      "Gradient: tensor([-0.1004,  0.5685])\n",
      "Epoch 976, Loss 3.905144\n",
      "Params: tensor([  4.7786, -13.9698])\n",
      "Gradient: tensor([-0.1003,  0.5675])\n",
      "Epoch 977, Loss 3.901824\n",
      "Params: tensor([  4.7796, -13.9755])\n",
      "Gradient: tensor([-0.1001,  0.5665])\n",
      "Epoch 978, Loss 3.898517\n",
      "Params: tensor([  4.7806, -13.9811])\n",
      "Gradient: tensor([-0.0999,  0.5656])\n",
      "Epoch 979, Loss 3.895222\n",
      "Params: tensor([  4.7816, -13.9868])\n",
      "Gradient: tensor([-0.0997,  0.5646])\n",
      "Epoch 980, Loss 3.891935\n",
      "Params: tensor([  4.7826, -13.9924])\n",
      "Gradient: tensor([-0.0996,  0.5637])\n",
      "Epoch 981, Loss 3.888664\n",
      "Params: tensor([  4.7836, -13.9980])\n",
      "Gradient: tensor([-0.0994,  0.5627])\n",
      "Epoch 982, Loss 3.885400\n",
      "Params: tensor([  4.7846, -14.0036])\n",
      "Gradient: tensor([-0.0992,  0.5617])\n",
      "Epoch 983, Loss 3.882150\n",
      "Params: tensor([  4.7856, -14.0092])\n",
      "Gradient: tensor([-0.0991,  0.5608])\n",
      "Epoch 984, Loss 3.878910\n",
      "Params: tensor([  4.7865, -14.0148])\n",
      "Gradient: tensor([-0.0989,  0.5598])\n",
      "Epoch 985, Loss 3.875681\n",
      "Params: tensor([  4.7875, -14.0204])\n",
      "Gradient: tensor([-0.0987,  0.5589])\n",
      "Epoch 986, Loss 3.872463\n",
      "Params: tensor([  4.7885, -14.0260])\n",
      "Gradient: tensor([-0.0986,  0.5579])\n",
      "Epoch 987, Loss 3.869256\n",
      "Params: tensor([  4.7895, -14.0316])\n",
      "Gradient: tensor([-0.0984,  0.5570])\n",
      "Epoch 988, Loss 3.866060\n",
      "Params: tensor([  4.7905, -14.0371])\n",
      "Gradient: tensor([-0.0982,  0.5560])\n",
      "Epoch 989, Loss 3.862873\n",
      "Params: tensor([  4.7915, -14.0427])\n",
      "Gradient: tensor([-0.0981,  0.5551])\n",
      "Epoch 990, Loss 3.859699\n",
      "Params: tensor([  4.7924, -14.0482])\n",
      "Gradient: tensor([-0.0979,  0.5541])\n",
      "Epoch 991, Loss 3.856535\n",
      "Params: tensor([  4.7934, -14.0538])\n",
      "Gradient: tensor([-0.0978,  0.5532])\n",
      "Epoch 992, Loss 3.853381\n",
      "Params: tensor([  4.7944, -14.0593])\n",
      "Gradient: tensor([-0.0976,  0.5523])\n",
      "Epoch 993, Loss 3.850237\n",
      "Params: tensor([  4.7954, -14.0648])\n",
      "Gradient: tensor([-0.0974,  0.5513])\n",
      "Epoch 994, Loss 3.847109\n",
      "Params: tensor([  4.7963, -14.0703])\n",
      "Gradient: tensor([-0.0972,  0.5504])\n",
      "Epoch 995, Loss 3.843984\n",
      "Params: tensor([  4.7973, -14.0758])\n",
      "Gradient: tensor([-0.0971,  0.5495])\n",
      "Epoch 996, Loss 3.840876\n",
      "Params: tensor([  4.7983, -14.0813])\n",
      "Gradient: tensor([-0.0969,  0.5485])\n",
      "Epoch 997, Loss 3.837775\n",
      "Params: tensor([  4.7992, -14.0868])\n",
      "Gradient: tensor([-0.0967,  0.5476])\n",
      "Epoch 998, Loss 3.834686\n",
      "Params: tensor([  4.8002, -14.0922])\n",
      "Gradient: tensor([-0.0966,  0.5467])\n",
      "Epoch 999, Loss 3.831606\n",
      "Params: tensor([  4.8012, -14.0977])\n",
      "Gradient: tensor([-0.0964,  0.5457])\n",
      "Epoch 1000, Loss 3.828538\n",
      "Params: tensor([  4.8021, -14.1031])\n",
      "Gradient: tensor([-0.0962,  0.5448])\n",
      "Epoch 1001, Loss 3.825484\n",
      "Params: tensor([  4.8031, -14.1086])\n",
      "Gradient: tensor([-0.0961,  0.5439])\n",
      "Epoch 1002, Loss 3.822433\n",
      "Params: tensor([  4.8041, -14.1140])\n",
      "Gradient: tensor([-0.0959,  0.5430])\n",
      "Epoch 1003, Loss 3.819398\n",
      "Params: tensor([  4.8050, -14.1194])\n",
      "Gradient: tensor([-0.0957,  0.5420])\n",
      "Epoch 1004, Loss 3.816369\n",
      "Params: tensor([  4.8060, -14.1248])\n",
      "Gradient: tensor([-0.0956,  0.5411])\n",
      "Epoch 1005, Loss 3.813350\n",
      "Params: tensor([  4.8069, -14.1302])\n",
      "Gradient: tensor([-0.0954,  0.5402])\n",
      "Epoch 1006, Loss 3.810344\n",
      "Params: tensor([  4.8079, -14.1356])\n",
      "Gradient: tensor([-0.0953,  0.5393])\n",
      "Epoch 1007, Loss 3.807348\n",
      "Params: tensor([  4.8088, -14.1410])\n",
      "Gradient: tensor([-0.0951,  0.5384])\n",
      "Epoch 1008, Loss 3.804360\n",
      "Params: tensor([  4.8098, -14.1464])\n",
      "Gradient: tensor([-0.0949,  0.5375])\n",
      "Epoch 1009, Loss 3.801384\n",
      "Params: tensor([  4.8107, -14.1518])\n",
      "Gradient: tensor([-0.0948,  0.5365])\n",
      "Epoch 1010, Loss 3.798421\n",
      "Params: tensor([  4.8117, -14.1571])\n",
      "Gradient: tensor([-0.0946,  0.5356])\n",
      "Epoch 1011, Loss 3.795465\n",
      "Params: tensor([  4.8126, -14.1625])\n",
      "Gradient: tensor([-0.0945,  0.5347])\n",
      "Epoch 1012, Loss 3.792518\n",
      "Params: tensor([  4.8136, -14.1678])\n",
      "Gradient: tensor([-0.0943,  0.5338])\n",
      "Epoch 1013, Loss 3.789584\n",
      "Params: tensor([  4.8145, -14.1731])\n",
      "Gradient: tensor([-0.0941,  0.5329])\n",
      "Epoch 1014, Loss 3.786658\n",
      "Params: tensor([  4.8154, -14.1784])\n",
      "Gradient: tensor([-0.0940,  0.5320])\n",
      "Epoch 1015, Loss 3.783740\n",
      "Params: tensor([  4.8164, -14.1838])\n",
      "Gradient: tensor([-0.0938,  0.5311])\n",
      "Epoch 1016, Loss 3.780832\n",
      "Params: tensor([  4.8173, -14.1891])\n",
      "Gradient: tensor([-0.0937,  0.5302])\n",
      "Epoch 1017, Loss 3.777939\n",
      "Params: tensor([  4.8183, -14.1943])\n",
      "Gradient: tensor([-0.0935,  0.5293])\n",
      "Epoch 1018, Loss 3.775053\n",
      "Params: tensor([  4.8192, -14.1996])\n",
      "Gradient: tensor([-0.0933,  0.5284])\n",
      "Epoch 1019, Loss 3.772173\n",
      "Params: tensor([  4.8201, -14.2049])\n",
      "Gradient: tensor([-0.0932,  0.5275])\n",
      "Epoch 1020, Loss 3.769310\n",
      "Params: tensor([  4.8210, -14.2102])\n",
      "Gradient: tensor([-0.0930,  0.5266])\n",
      "Epoch 1021, Loss 3.766451\n",
      "Params: tensor([  4.8220, -14.2154])\n",
      "Gradient: tensor([-0.0929,  0.5257])\n",
      "Epoch 1022, Loss 3.763602\n",
      "Params: tensor([  4.8229, -14.2207])\n",
      "Gradient: tensor([-0.0927,  0.5248])\n",
      "Epoch 1023, Loss 3.760766\n",
      "Params: tensor([  4.8238, -14.2259])\n",
      "Gradient: tensor([-0.0926,  0.5239])\n",
      "Epoch 1024, Loss 3.757936\n",
      "Params: tensor([  4.8248, -14.2311])\n",
      "Gradient: tensor([-0.0924,  0.5230])\n",
      "Epoch 1025, Loss 3.755118\n",
      "Params: tensor([  4.8257, -14.2364])\n",
      "Gradient: tensor([-0.0922,  0.5221])\n",
      "Epoch 1026, Loss 3.752309\n",
      "Params: tensor([  4.8266, -14.2416])\n",
      "Gradient: tensor([-0.0921,  0.5213])\n",
      "Epoch 1027, Loss 3.749511\n",
      "Params: tensor([  4.8275, -14.2468])\n",
      "Gradient: tensor([-0.0919,  0.5204])\n",
      "Epoch 1028, Loss 3.746722\n",
      "Params: tensor([  4.8284, -14.2520])\n",
      "Gradient: tensor([-0.0918,  0.5195])\n",
      "Epoch 1029, Loss 3.743940\n",
      "Params: tensor([  4.8293, -14.2572])\n",
      "Gradient: tensor([-0.0916,  0.5186])\n",
      "Epoch 1030, Loss 3.741169\n",
      "Params: tensor([  4.8303, -14.2623])\n",
      "Gradient: tensor([-0.0915,  0.5177])\n",
      "Epoch 1031, Loss 3.738407\n",
      "Params: tensor([  4.8312, -14.2675])\n",
      "Gradient: tensor([-0.0913,  0.5168])\n",
      "Epoch 1032, Loss 3.735656\n",
      "Params: tensor([  4.8321, -14.2727])\n",
      "Gradient: tensor([-0.0912,  0.5160])\n",
      "Epoch 1033, Loss 3.732914\n",
      "Params: tensor([  4.8330, -14.2778])\n",
      "Gradient: tensor([-0.0910,  0.5151])\n",
      "Epoch 1034, Loss 3.730181\n",
      "Params: tensor([  4.8339, -14.2830])\n",
      "Gradient: tensor([-0.0908,  0.5142])\n",
      "Epoch 1035, Loss 3.727456\n",
      "Params: tensor([  4.8348, -14.2881])\n",
      "Gradient: tensor([-0.0907,  0.5133])\n",
      "Epoch 1036, Loss 3.724741\n",
      "Params: tensor([  4.8357, -14.2932])\n",
      "Gradient: tensor([-0.0905,  0.5125])\n",
      "Epoch 1037, Loss 3.722034\n",
      "Params: tensor([  4.8366, -14.2983])\n",
      "Gradient: tensor([-0.0904,  0.5116])\n",
      "Epoch 1038, Loss 3.719337\n",
      "Params: tensor([  4.8375, -14.3034])\n",
      "Gradient: tensor([-0.0902,  0.5107])\n",
      "Epoch 1039, Loss 3.716651\n",
      "Params: tensor([  4.8384, -14.3085])\n",
      "Gradient: tensor([-0.0901,  0.5099])\n",
      "Epoch 1040, Loss 3.713972\n",
      "Params: tensor([  4.8393, -14.3136])\n",
      "Gradient: tensor([-0.0899,  0.5090])\n",
      "Epoch 1041, Loss 3.711302\n",
      "Params: tensor([  4.8402, -14.3187])\n",
      "Gradient: tensor([-0.0898,  0.5081])\n",
      "Epoch 1042, Loss 3.708643\n",
      "Params: tensor([  4.8411, -14.3238])\n",
      "Gradient: tensor([-0.0896,  0.5073])\n",
      "Epoch 1043, Loss 3.705990\n",
      "Params: tensor([  4.8420, -14.3288])\n",
      "Gradient: tensor([-0.0895,  0.5064])\n",
      "Epoch 1044, Loss 3.703351\n",
      "Params: tensor([  4.8429, -14.3339])\n",
      "Gradient: tensor([-0.0893,  0.5055])\n",
      "Epoch 1045, Loss 3.700716\n",
      "Params: tensor([  4.8438, -14.3390])\n",
      "Gradient: tensor([-0.0892,  0.5047])\n",
      "Epoch 1046, Loss 3.698091\n",
      "Params: tensor([  4.8447, -14.3440])\n",
      "Gradient: tensor([-0.0890,  0.5038])\n",
      "Epoch 1047, Loss 3.695476\n",
      "Params: tensor([  4.8456, -14.3490])\n",
      "Gradient: tensor([-0.0888,  0.5030])\n",
      "Epoch 1048, Loss 3.692869\n",
      "Params: tensor([  4.8465, -14.3540])\n",
      "Gradient: tensor([-0.0887,  0.5021])\n",
      "Epoch 1049, Loss 3.690273\n",
      "Params: tensor([  4.8473, -14.3591])\n",
      "Gradient: tensor([-0.0886,  0.5013])\n",
      "Epoch 1050, Loss 3.687683\n",
      "Params: tensor([  4.8482, -14.3641])\n",
      "Gradient: tensor([-0.0884,  0.5004])\n",
      "Epoch 1051, Loss 3.685104\n",
      "Params: tensor([  4.8491, -14.3691])\n",
      "Gradient: tensor([-0.0882,  0.4996])\n",
      "Epoch 1052, Loss 3.682532\n",
      "Params: tensor([  4.8500, -14.3740])\n",
      "Gradient: tensor([-0.0881,  0.4987])\n",
      "Epoch 1053, Loss 3.679969\n",
      "Params: tensor([  4.8509, -14.3790])\n",
      "Gradient: tensor([-0.0879,  0.4979])\n",
      "Epoch 1054, Loss 3.677417\n",
      "Params: tensor([  4.8518, -14.3840])\n",
      "Gradient: tensor([-0.0878,  0.4970])\n",
      "Epoch 1055, Loss 3.674870\n",
      "Params: tensor([  4.8526, -14.3889])\n",
      "Gradient: tensor([-0.0877,  0.4962])\n",
      "Epoch 1056, Loss 3.672334\n",
      "Params: tensor([  4.8535, -14.3939])\n",
      "Gradient: tensor([-0.0875,  0.4953])\n",
      "Epoch 1057, Loss 3.669805\n",
      "Params: tensor([  4.8544, -14.3988])\n",
      "Gradient: tensor([-0.0873,  0.4945])\n",
      "Epoch 1058, Loss 3.667287\n",
      "Params: tensor([  4.8552, -14.4038])\n",
      "Gradient: tensor([-0.0872,  0.4936])\n",
      "Epoch 1059, Loss 3.664775\n",
      "Params: tensor([  4.8561, -14.4087])\n",
      "Gradient: tensor([-0.0870,  0.4928])\n",
      "Epoch 1060, Loss 3.662274\n",
      "Params: tensor([  4.8570, -14.4136])\n",
      "Gradient: tensor([-0.0869,  0.4920])\n",
      "Epoch 1061, Loss 3.659778\n",
      "Params: tensor([  4.8579, -14.4185])\n",
      "Gradient: tensor([-0.0868,  0.4911])\n",
      "Epoch 1062, Loss 3.657295\n",
      "Params: tensor([  4.8587, -14.4234])\n",
      "Gradient: tensor([-0.0866,  0.4903])\n",
      "Epoch 1063, Loss 3.654816\n",
      "Params: tensor([  4.8596, -14.4283])\n",
      "Gradient: tensor([-0.0865,  0.4895])\n",
      "Epoch 1064, Loss 3.652349\n",
      "Params: tensor([  4.8604, -14.4332])\n",
      "Gradient: tensor([-0.0863,  0.4886])\n",
      "Epoch 1065, Loss 3.649889\n",
      "Params: tensor([  4.8613, -14.4381])\n",
      "Gradient: tensor([-0.0862,  0.4878])\n",
      "Epoch 1066, Loss 3.647437\n",
      "Params: tensor([  4.8622, -14.4430])\n",
      "Gradient: tensor([-0.0860,  0.4870])\n",
      "Epoch 1067, Loss 3.644992\n",
      "Params: tensor([  4.8630, -14.4478])\n",
      "Gradient: tensor([-0.0859,  0.4862])\n",
      "Epoch 1068, Loss 3.642559\n",
      "Params: tensor([  4.8639, -14.4527])\n",
      "Gradient: tensor([-0.0857,  0.4853])\n",
      "Epoch 1069, Loss 3.640131\n",
      "Params: tensor([  4.8647, -14.4575])\n",
      "Gradient: tensor([-0.0856,  0.4845])\n",
      "Epoch 1070, Loss 3.637712\n",
      "Params: tensor([  4.8656, -14.4624])\n",
      "Gradient: tensor([-0.0854,  0.4837])\n",
      "Epoch 1071, Loss 3.635302\n",
      "Params: tensor([  4.8665, -14.4672])\n",
      "Gradient: tensor([-0.0853,  0.4829])\n",
      "Epoch 1072, Loss 3.632902\n",
      "Params: tensor([  4.8673, -14.4720])\n",
      "Gradient: tensor([-0.0851,  0.4820])\n",
      "Epoch 1073, Loss 3.630508\n",
      "Params: tensor([  4.8682, -14.4768])\n",
      "Gradient: tensor([-0.0850,  0.4812])\n",
      "Epoch 1074, Loss 3.628119\n",
      "Params: tensor([  4.8690, -14.4816])\n",
      "Gradient: tensor([-0.0849,  0.4804])\n",
      "Epoch 1075, Loss 3.625741\n",
      "Params: tensor([  4.8698, -14.4864])\n",
      "Gradient: tensor([-0.0847,  0.4796])\n",
      "Epoch 1076, Loss 3.623373\n",
      "Params: tensor([  4.8707, -14.4912])\n",
      "Gradient: tensor([-0.0846,  0.4788])\n",
      "Epoch 1077, Loss 3.621010\n",
      "Params: tensor([  4.8715, -14.4960])\n",
      "Gradient: tensor([-0.0844,  0.4780])\n",
      "Epoch 1078, Loss 3.618659\n",
      "Params: tensor([  4.8724, -14.5008])\n",
      "Gradient: tensor([-0.0843,  0.4771])\n",
      "Epoch 1079, Loss 3.616311\n",
      "Params: tensor([  4.8732, -14.5055])\n",
      "Gradient: tensor([-0.0841,  0.4763])\n",
      "Epoch 1080, Loss 3.613973\n",
      "Params: tensor([  4.8741, -14.5103])\n",
      "Gradient: tensor([-0.0840,  0.4755])\n",
      "Epoch 1081, Loss 3.611643\n",
      "Params: tensor([  4.8749, -14.5150])\n",
      "Gradient: tensor([-0.0839,  0.4747])\n",
      "Epoch 1082, Loss 3.609321\n",
      "Params: tensor([  4.8757, -14.5198])\n",
      "Gradient: tensor([-0.0837,  0.4739])\n",
      "Epoch 1083, Loss 3.607007\n",
      "Params: tensor([  4.8766, -14.5245])\n",
      "Gradient: tensor([-0.0836,  0.4731])\n",
      "Epoch 1084, Loss 3.604701\n",
      "Params: tensor([  4.8774, -14.5292])\n",
      "Gradient: tensor([-0.0834,  0.4723])\n",
      "Epoch 1085, Loss 3.602403\n",
      "Params: tensor([  4.8782, -14.5339])\n",
      "Gradient: tensor([-0.0833,  0.4715])\n",
      "Epoch 1086, Loss 3.600114\n",
      "Params: tensor([  4.8791, -14.5387])\n",
      "Gradient: tensor([-0.0832,  0.4707])\n",
      "Epoch 1087, Loss 3.597830\n",
      "Params: tensor([  4.8799, -14.5434])\n",
      "Gradient: tensor([-0.0830,  0.4699])\n",
      "Epoch 1088, Loss 3.595553\n",
      "Params: tensor([  4.8807, -14.5480])\n",
      "Gradient: tensor([-0.0829,  0.4691])\n",
      "Epoch 1089, Loss 3.593288\n",
      "Params: tensor([  4.8816, -14.5527])\n",
      "Gradient: tensor([-0.0827,  0.4683])\n",
      "Epoch 1090, Loss 3.591029\n",
      "Params: tensor([  4.8824, -14.5574])\n",
      "Gradient: tensor([-0.0826,  0.4675])\n",
      "Epoch 1091, Loss 3.588776\n",
      "Params: tensor([  4.8832, -14.5621])\n",
      "Gradient: tensor([-0.0824,  0.4667])\n",
      "Epoch 1092, Loss 3.586534\n",
      "Params: tensor([  4.8840, -14.5667])\n",
      "Gradient: tensor([-0.0823,  0.4659])\n",
      "Epoch 1093, Loss 3.584294\n",
      "Params: tensor([  4.8849, -14.5714])\n",
      "Gradient: tensor([-0.0822,  0.4651])\n",
      "Epoch 1094, Loss 3.582067\n",
      "Params: tensor([  4.8857, -14.5760])\n",
      "Gradient: tensor([-0.0820,  0.4643])\n",
      "Epoch 1095, Loss 3.579846\n",
      "Params: tensor([  4.8865, -14.5807])\n",
      "Gradient: tensor([-0.0819,  0.4636])\n",
      "Epoch 1096, Loss 3.577631\n",
      "Params: tensor([  4.8873, -14.5853])\n",
      "Gradient: tensor([-0.0818,  0.4628])\n",
      "Epoch 1097, Loss 3.575424\n",
      "Params: tensor([  4.8881, -14.5899])\n",
      "Gradient: tensor([-0.0816,  0.4620])\n",
      "Epoch 1098, Loss 3.573225\n",
      "Params: tensor([  4.8889, -14.5945])\n",
      "Gradient: tensor([-0.0815,  0.4612])\n",
      "Epoch 1099, Loss 3.571034\n",
      "Params: tensor([  4.8898, -14.5991])\n",
      "Gradient: tensor([-0.0813,  0.4604])\n",
      "Epoch 1100, Loss 3.568848\n",
      "Params: tensor([  4.8906, -14.6037])\n",
      "Gradient: tensor([-0.0812,  0.4596])\n",
      "Epoch 1101, Loss 3.566673\n",
      "Params: tensor([  4.8914, -14.6083])\n",
      "Gradient: tensor([-0.0810,  0.4588])\n",
      "Epoch 1102, Loss 3.564506\n",
      "Params: tensor([  4.8922, -14.6129])\n",
      "Gradient: tensor([-0.0809,  0.4581])\n",
      "Epoch 1103, Loss 3.562341\n",
      "Params: tensor([  4.8930, -14.6175])\n",
      "Gradient: tensor([-0.0808,  0.4573])\n",
      "Epoch 1104, Loss 3.560185\n",
      "Params: tensor([  4.8938, -14.6220])\n",
      "Gradient: tensor([-0.0806,  0.4565])\n",
      "Epoch 1105, Loss 3.558040\n",
      "Params: tensor([  4.8946, -14.6266])\n",
      "Gradient: tensor([-0.0805,  0.4557])\n",
      "Epoch 1106, Loss 3.555901\n",
      "Params: tensor([  4.8954, -14.6311])\n",
      "Gradient: tensor([-0.0804,  0.4550])\n",
      "Epoch 1107, Loss 3.553767\n",
      "Params: tensor([  4.8962, -14.6357])\n",
      "Gradient: tensor([-0.0802,  0.4542])\n",
      "Epoch 1108, Loss 3.551641\n",
      "Params: tensor([  4.8970, -14.6402])\n",
      "Gradient: tensor([-0.0801,  0.4534])\n",
      "Epoch 1109, Loss 3.549524\n",
      "Params: tensor([  4.8978, -14.6447])\n",
      "Gradient: tensor([-0.0799,  0.4527])\n",
      "Epoch 1110, Loss 3.547411\n",
      "Params: tensor([  4.8986, -14.6493])\n",
      "Gradient: tensor([-0.0798,  0.4519])\n",
      "Epoch 1111, Loss 3.545308\n",
      "Params: tensor([  4.8994, -14.6538])\n",
      "Gradient: tensor([-0.0797,  0.4511])\n",
      "Epoch 1112, Loss 3.543211\n",
      "Params: tensor([  4.9002, -14.6583])\n",
      "Gradient: tensor([-0.0796,  0.4503])\n",
      "Epoch 1113, Loss 3.541124\n",
      "Params: tensor([  4.9010, -14.6628])\n",
      "Gradient: tensor([-0.0794,  0.4496])\n",
      "Epoch 1114, Loss 3.539041\n",
      "Params: tensor([  4.9018, -14.6673])\n",
      "Gradient: tensor([-0.0793,  0.4488])\n",
      "Epoch 1115, Loss 3.536966\n",
      "Params: tensor([  4.9026, -14.6717])\n",
      "Gradient: tensor([-0.0791,  0.4481])\n",
      "Epoch 1116, Loss 3.534897\n",
      "Params: tensor([  4.9034, -14.6762])\n",
      "Gradient: tensor([-0.0790,  0.4473])\n",
      "Epoch 1117, Loss 3.532835\n",
      "Params: tensor([  4.9042, -14.6807])\n",
      "Gradient: tensor([-0.0789,  0.4465])\n",
      "Epoch 1118, Loss 3.530781\n",
      "Params: tensor([  4.9049, -14.6851])\n",
      "Gradient: tensor([-0.0787,  0.4458])\n",
      "Epoch 1119, Loss 3.528734\n",
      "Params: tensor([  4.9057, -14.6896])\n",
      "Gradient: tensor([-0.0786,  0.4450])\n",
      "Epoch 1120, Loss 3.526694\n",
      "Params: tensor([  4.9065, -14.6940])\n",
      "Gradient: tensor([-0.0785,  0.4443])\n",
      "Epoch 1121, Loss 3.524662\n",
      "Params: tensor([  4.9073, -14.6985])\n",
      "Gradient: tensor([-0.0784,  0.4435])\n",
      "Epoch 1122, Loss 3.522633\n",
      "Params: tensor([  4.9081, -14.7029])\n",
      "Gradient: tensor([-0.0782,  0.4428])\n",
      "Epoch 1123, Loss 3.520614\n",
      "Params: tensor([  4.9089, -14.7073])\n",
      "Gradient: tensor([-0.0781,  0.4420])\n",
      "Epoch 1124, Loss 3.518601\n",
      "Params: tensor([  4.9096, -14.7117])\n",
      "Gradient: tensor([-0.0779,  0.4413])\n",
      "Epoch 1125, Loss 3.516594\n",
      "Params: tensor([  4.9104, -14.7161])\n",
      "Gradient: tensor([-0.0778,  0.4405])\n",
      "Epoch 1126, Loss 3.514594\n",
      "Params: tensor([  4.9112, -14.7205])\n",
      "Gradient: tensor([-0.0777,  0.4398])\n",
      "Epoch 1127, Loss 3.512602\n",
      "Params: tensor([  4.9120, -14.7249])\n",
      "Gradient: tensor([-0.0775,  0.4390])\n",
      "Epoch 1128, Loss 3.510619\n",
      "Params: tensor([  4.9128, -14.7293])\n",
      "Gradient: tensor([-0.0774,  0.4383])\n",
      "Epoch 1129, Loss 3.508637\n",
      "Params: tensor([  4.9135, -14.7337])\n",
      "Gradient: tensor([-0.0773,  0.4375])\n",
      "Epoch 1130, Loss 3.506665\n",
      "Params: tensor([  4.9143, -14.7380])\n",
      "Gradient: tensor([-0.0772,  0.4368])\n",
      "Epoch 1131, Loss 3.504699\n",
      "Params: tensor([  4.9151, -14.7424])\n",
      "Gradient: tensor([-0.0770,  0.4360])\n",
      "Epoch 1132, Loss 3.502741\n",
      "Params: tensor([  4.9158, -14.7467])\n",
      "Gradient: tensor([-0.0769,  0.4353])\n",
      "Epoch 1133, Loss 3.500789\n",
      "Params: tensor([  4.9166, -14.7511])\n",
      "Gradient: tensor([-0.0767,  0.4346])\n",
      "Epoch 1134, Loss 3.498843\n",
      "Params: tensor([  4.9174, -14.7554])\n",
      "Gradient: tensor([-0.0766,  0.4338])\n",
      "Epoch 1135, Loss 3.496905\n",
      "Params: tensor([  4.9181, -14.7598])\n",
      "Gradient: tensor([-0.0765,  0.4331])\n",
      "Epoch 1136, Loss 3.494972\n",
      "Params: tensor([  4.9189, -14.7641])\n",
      "Gradient: tensor([-0.0764,  0.4323])\n",
      "Epoch 1137, Loss 3.493046\n",
      "Params: tensor([  4.9197, -14.7684])\n",
      "Gradient: tensor([-0.0762,  0.4316])\n",
      "Epoch 1138, Loss 3.491126\n",
      "Params: tensor([  4.9204, -14.7727])\n",
      "Gradient: tensor([-0.0761,  0.4309])\n",
      "Epoch 1139, Loss 3.489213\n",
      "Params: tensor([  4.9212, -14.7770])\n",
      "Gradient: tensor([-0.0760,  0.4301])\n",
      "Epoch 1140, Loss 3.487308\n",
      "Params: tensor([  4.9219, -14.7813])\n",
      "Gradient: tensor([-0.0759,  0.4294])\n",
      "Epoch 1141, Loss 3.485409\n",
      "Params: tensor([  4.9227, -14.7856])\n",
      "Gradient: tensor([-0.0757,  0.4287])\n",
      "Epoch 1142, Loss 3.483515\n",
      "Params: tensor([  4.9235, -14.7899])\n",
      "Gradient: tensor([-0.0756,  0.4280])\n",
      "Epoch 1143, Loss 3.481627\n",
      "Params: tensor([  4.9242, -14.7941])\n",
      "Gradient: tensor([-0.0755,  0.4272])\n",
      "Epoch 1144, Loss 3.479746\n",
      "Params: tensor([  4.9250, -14.7984])\n",
      "Gradient: tensor([-0.0753,  0.4265])\n",
      "Epoch 1145, Loss 3.477872\n",
      "Params: tensor([  4.9257, -14.8027])\n",
      "Gradient: tensor([-0.0752,  0.4258])\n",
      "Epoch 1146, Loss 3.476005\n",
      "Params: tensor([  4.9265, -14.8069])\n",
      "Gradient: tensor([-0.0751,  0.4250])\n",
      "Epoch 1147, Loss 3.474143\n",
      "Params: tensor([  4.9272, -14.8112])\n",
      "Gradient: tensor([-0.0750,  0.4243])\n",
      "Epoch 1148, Loss 3.472288\n",
      "Params: tensor([  4.9280, -14.8154])\n",
      "Gradient: tensor([-0.0748,  0.4236])\n",
      "Epoch 1149, Loss 3.470441\n",
      "Params: tensor([  4.9287, -14.8196])\n",
      "Gradient: tensor([-0.0747,  0.4229])\n",
      "Epoch 1150, Loss 3.468597\n",
      "Params: tensor([  4.9295, -14.8238])\n",
      "Gradient: tensor([-0.0746,  0.4222])\n",
      "Epoch 1151, Loss 3.466762\n",
      "Params: tensor([  4.9302, -14.8281])\n",
      "Gradient: tensor([-0.0745,  0.4215])\n",
      "Epoch 1152, Loss 3.464930\n",
      "Params: tensor([  4.9309, -14.8323])\n",
      "Gradient: tensor([-0.0743,  0.4207])\n",
      "Epoch 1153, Loss 3.463105\n",
      "Params: tensor([  4.9317, -14.8365])\n",
      "Gradient: tensor([-0.0742,  0.4200])\n",
      "Epoch 1154, Loss 3.461289\n",
      "Params: tensor([  4.9324, -14.8407])\n",
      "Gradient: tensor([-0.0741,  0.4193])\n",
      "Epoch 1155, Loss 3.459477\n",
      "Params: tensor([  4.9332, -14.8448])\n",
      "Gradient: tensor([-0.0739,  0.4186])\n",
      "Epoch 1156, Loss 3.457671\n",
      "Params: tensor([  4.9339, -14.8490])\n",
      "Gradient: tensor([-0.0738,  0.4179])\n",
      "Epoch 1157, Loss 3.455874\n",
      "Params: tensor([  4.9346, -14.8532])\n",
      "Gradient: tensor([-0.0737,  0.4172])\n",
      "Epoch 1158, Loss 3.454080\n",
      "Params: tensor([  4.9354, -14.8574])\n",
      "Gradient: tensor([-0.0736,  0.4165])\n",
      "Epoch 1159, Loss 3.452293\n",
      "Params: tensor([  4.9361, -14.8615])\n",
      "Gradient: tensor([-0.0734,  0.4158])\n",
      "Epoch 1160, Loss 3.450512\n",
      "Params: tensor([  4.9368, -14.8657])\n",
      "Gradient: tensor([-0.0733,  0.4151])\n",
      "Epoch 1161, Loss 3.448736\n",
      "Params: tensor([  4.9376, -14.8698])\n",
      "Gradient: tensor([-0.0732,  0.4143])\n",
      "Epoch 1162, Loss 3.446968\n",
      "Params: tensor([  4.9383, -14.8739])\n",
      "Gradient: tensor([-0.0731,  0.4136])\n",
      "Epoch 1163, Loss 3.445203\n",
      "Params: tensor([  4.9390, -14.8781])\n",
      "Gradient: tensor([-0.0730,  0.4129])\n",
      "Epoch 1164, Loss 3.443449\n",
      "Params: tensor([  4.9398, -14.8822])\n",
      "Gradient: tensor([-0.0728,  0.4122])\n",
      "Epoch 1165, Loss 3.441696\n",
      "Params: tensor([  4.9405, -14.8863])\n",
      "Gradient: tensor([-0.0727,  0.4115])\n",
      "Epoch 1166, Loss 3.439952\n",
      "Params: tensor([  4.9412, -14.8904])\n",
      "Gradient: tensor([-0.0726,  0.4108])\n",
      "Epoch 1167, Loss 3.438210\n",
      "Params: tensor([  4.9419, -14.8945])\n",
      "Gradient: tensor([-0.0725,  0.4101])\n",
      "Epoch 1168, Loss 3.436479\n",
      "Params: tensor([  4.9427, -14.8986])\n",
      "Gradient: tensor([-0.0723,  0.4094])\n",
      "Epoch 1169, Loss 3.434753\n",
      "Params: tensor([  4.9434, -14.9027])\n",
      "Gradient: tensor([-0.0722,  0.4087])\n",
      "Epoch 1170, Loss 3.433030\n",
      "Params: tensor([  4.9441, -14.9068])\n",
      "Gradient: tensor([-0.0721,  0.4081])\n",
      "Epoch 1171, Loss 3.431314\n",
      "Params: tensor([  4.9448, -14.9109])\n",
      "Gradient: tensor([-0.0720,  0.4074])\n",
      "Epoch 1172, Loss 3.429608\n",
      "Params: tensor([  4.9455, -14.9149])\n",
      "Gradient: tensor([-0.0719,  0.4067])\n",
      "Epoch 1173, Loss 3.427903\n",
      "Params: tensor([  4.9463, -14.9190])\n",
      "Gradient: tensor([-0.0717,  0.4060])\n",
      "Epoch 1174, Loss 3.426204\n",
      "Params: tensor([  4.9470, -14.9230])\n",
      "Gradient: tensor([-0.0716,  0.4053])\n",
      "Epoch 1175, Loss 3.424509\n",
      "Params: tensor([  4.9477, -14.9271])\n",
      "Gradient: tensor([-0.0715,  0.4046])\n",
      "Epoch 1176, Loss 3.422823\n",
      "Params: tensor([  4.9484, -14.9311])\n",
      "Gradient: tensor([-0.0714,  0.4039])\n",
      "Epoch 1177, Loss 3.421144\n",
      "Params: tensor([  4.9491, -14.9352])\n",
      "Gradient: tensor([-0.0712,  0.4032])\n",
      "Epoch 1178, Loss 3.419468\n",
      "Params: tensor([  4.9498, -14.9392])\n",
      "Gradient: tensor([-0.0711,  0.4025])\n",
      "Epoch 1179, Loss 3.417798\n",
      "Params: tensor([  4.9505, -14.9432])\n",
      "Gradient: tensor([-0.0710,  0.4019])\n",
      "Epoch 1180, Loss 3.416134\n",
      "Params: tensor([  4.9512, -14.9472])\n",
      "Gradient: tensor([-0.0709,  0.4012])\n",
      "Epoch 1181, Loss 3.414476\n",
      "Params: tensor([  4.9520, -14.9512])\n",
      "Gradient: tensor([-0.0708,  0.4005])\n",
      "Epoch 1182, Loss 3.412824\n",
      "Params: tensor([  4.9527, -14.9552])\n",
      "Gradient: tensor([-0.0706,  0.3998])\n",
      "Epoch 1183, Loss 3.411176\n",
      "Params: tensor([  4.9534, -14.9592])\n",
      "Gradient: tensor([-0.0705,  0.3991])\n",
      "Epoch 1184, Loss 3.409534\n",
      "Params: tensor([  4.9541, -14.9632])\n",
      "Gradient: tensor([-0.0704,  0.3985])\n",
      "Epoch 1185, Loss 3.407899\n",
      "Params: tensor([  4.9548, -14.9672])\n",
      "Gradient: tensor([-0.0703,  0.3978])\n",
      "Epoch 1186, Loss 3.406271\n",
      "Params: tensor([  4.9555, -14.9711])\n",
      "Gradient: tensor([-0.0701,  0.3971])\n",
      "Epoch 1187, Loss 3.404645\n",
      "Params: tensor([  4.9562, -14.9751])\n",
      "Gradient: tensor([-0.0700,  0.3964])\n",
      "Epoch 1188, Loss 3.403024\n",
      "Params: tensor([  4.9569, -14.9791])\n",
      "Gradient: tensor([-0.0699,  0.3958])\n",
      "Epoch 1189, Loss 3.401413\n",
      "Params: tensor([  4.9576, -14.9830])\n",
      "Gradient: tensor([-0.0698,  0.3951])\n",
      "Epoch 1190, Loss 3.399802\n",
      "Params: tensor([  4.9583, -14.9870])\n",
      "Gradient: tensor([-0.0697,  0.3944])\n",
      "Epoch 1191, Loss 3.398200\n",
      "Params: tensor([  4.9590, -14.9909])\n",
      "Gradient: tensor([-0.0696,  0.3937])\n",
      "Epoch 1192, Loss 3.396602\n",
      "Params: tensor([  4.9597, -14.9948])\n",
      "Gradient: tensor([-0.0694,  0.3931])\n",
      "Epoch 1193, Loss 3.395012\n",
      "Params: tensor([  4.9604, -14.9988])\n",
      "Gradient: tensor([-0.0693,  0.3924])\n",
      "Epoch 1194, Loss 3.393425\n",
      "Params: tensor([  4.9610, -15.0027])\n",
      "Gradient: tensor([-0.0692,  0.3917])\n",
      "Epoch 1195, Loss 3.391845\n",
      "Params: tensor([  4.9617, -15.0066])\n",
      "Gradient: tensor([-0.0691,  0.3911])\n",
      "Epoch 1196, Loss 3.390267\n",
      "Params: tensor([  4.9624, -15.0105])\n",
      "Gradient: tensor([-0.0690,  0.3904])\n",
      "Epoch 1197, Loss 3.388697\n",
      "Params: tensor([  4.9631, -15.0144])\n",
      "Gradient: tensor([-0.0689,  0.3897])\n",
      "Epoch 1198, Loss 3.387132\n",
      "Params: tensor([  4.9638, -15.0183])\n",
      "Gradient: tensor([-0.0687,  0.3891])\n",
      "Epoch 1199, Loss 3.385571\n",
      "Params: tensor([  4.9645, -15.0222])\n",
      "Gradient: tensor([-0.0686,  0.3884])\n",
      "Epoch 1200, Loss 3.384018\n",
      "Params: tensor([  4.9652, -15.0260])\n",
      "Gradient: tensor([-0.0685,  0.3878])\n",
      "Epoch 1201, Loss 3.382467\n",
      "Params: tensor([  4.9659, -15.0299])\n",
      "Gradient: tensor([-0.0684,  0.3871])\n",
      "Epoch 1202, Loss 3.380925\n",
      "Params: tensor([  4.9665, -15.0338])\n",
      "Gradient: tensor([-0.0683,  0.3864])\n",
      "Epoch 1203, Loss 3.379386\n",
      "Params: tensor([  4.9672, -15.0376])\n",
      "Gradient: tensor([-0.0681,  0.3858])\n",
      "Epoch 1204, Loss 3.377851\n",
      "Params: tensor([  4.9679, -15.0415])\n",
      "Gradient: tensor([-0.0680,  0.3851])\n",
      "Epoch 1205, Loss 3.376323\n",
      "Params: tensor([  4.9686, -15.0453])\n",
      "Gradient: tensor([-0.0679,  0.3845])\n",
      "Epoch 1206, Loss 3.374800\n",
      "Params: tensor([  4.9693, -15.0492])\n",
      "Gradient: tensor([-0.0678,  0.3838])\n",
      "Epoch 1207, Loss 3.373284\n",
      "Params: tensor([  4.9699, -15.0530])\n",
      "Gradient: tensor([-0.0677,  0.3832])\n",
      "Epoch 1208, Loss 3.371769\n",
      "Params: tensor([  4.9706, -15.0568])\n",
      "Gradient: tensor([-0.0676,  0.3825])\n",
      "Epoch 1209, Loss 3.370261\n",
      "Params: tensor([  4.9713, -15.0606])\n",
      "Gradient: tensor([-0.0675,  0.3819])\n",
      "Epoch 1210, Loss 3.368760\n",
      "Params: tensor([  4.9720, -15.0645])\n",
      "Gradient: tensor([-0.0673,  0.3812])\n",
      "Epoch 1211, Loss 3.367262\n",
      "Params: tensor([  4.9726, -15.0683])\n",
      "Gradient: tensor([-0.0672,  0.3806])\n",
      "Epoch 1212, Loss 3.365771\n",
      "Params: tensor([  4.9733, -15.0721])\n",
      "Gradient: tensor([-0.0671,  0.3799])\n",
      "Epoch 1213, Loss 3.364282\n",
      "Params: tensor([  4.9740, -15.0758])\n",
      "Gradient: tensor([-0.0670,  0.3793])\n",
      "Epoch 1214, Loss 3.362800\n",
      "Params: tensor([  4.9746, -15.0796])\n",
      "Gradient: tensor([-0.0669,  0.3786])\n",
      "Epoch 1215, Loss 3.361324\n",
      "Params: tensor([  4.9753, -15.0834])\n",
      "Gradient: tensor([-0.0668,  0.3780])\n",
      "Epoch 1216, Loss 3.359851\n",
      "Params: tensor([  4.9760, -15.0872])\n",
      "Gradient: tensor([-0.0667,  0.3774])\n",
      "Epoch 1217, Loss 3.358383\n",
      "Params: tensor([  4.9766, -15.0910])\n",
      "Gradient: tensor([-0.0665,  0.3767])\n",
      "Epoch 1218, Loss 3.356921\n",
      "Params: tensor([  4.9773, -15.0947])\n",
      "Gradient: tensor([-0.0664,  0.3761])\n",
      "Epoch 1219, Loss 3.355464\n",
      "Params: tensor([  4.9780, -15.0985])\n",
      "Gradient: tensor([-0.0663,  0.3754])\n",
      "Epoch 1220, Loss 3.354013\n",
      "Params: tensor([  4.9786, -15.1022])\n",
      "Gradient: tensor([-0.0662,  0.3748])\n",
      "Epoch 1221, Loss 3.352564\n",
      "Params: tensor([  4.9793, -15.1060])\n",
      "Gradient: tensor([-0.0661,  0.3742])\n",
      "Epoch 1222, Loss 3.351122\n",
      "Params: tensor([  4.9799, -15.1097])\n",
      "Gradient: tensor([-0.0660,  0.3735])\n",
      "Epoch 1223, Loss 3.349685\n",
      "Params: tensor([  4.9806, -15.1134])\n",
      "Gradient: tensor([-0.0659,  0.3729])\n",
      "Epoch 1224, Loss 3.348251\n",
      "Params: tensor([  4.9813, -15.1171])\n",
      "Gradient: tensor([-0.0657,  0.3723])\n",
      "Epoch 1225, Loss 3.346825\n",
      "Params: tensor([  4.9819, -15.1209])\n",
      "Gradient: tensor([-0.0656,  0.3716])\n",
      "Epoch 1226, Loss 3.345403\n",
      "Params: tensor([  4.9826, -15.1246])\n",
      "Gradient: tensor([-0.0655,  0.3710])\n",
      "Epoch 1227, Loss 3.343982\n",
      "Params: tensor([  4.9832, -15.1283])\n",
      "Gradient: tensor([-0.0654,  0.3704])\n",
      "Epoch 1228, Loss 3.342571\n",
      "Params: tensor([  4.9839, -15.1320])\n",
      "Gradient: tensor([-0.0653,  0.3697])\n",
      "Epoch 1229, Loss 3.341160\n",
      "Params: tensor([  4.9845, -15.1357])\n",
      "Gradient: tensor([-0.0652,  0.3691])\n",
      "Epoch 1230, Loss 3.339758\n",
      "Params: tensor([  4.9852, -15.1393])\n",
      "Gradient: tensor([-0.0651,  0.3685])\n",
      "Epoch 1231, Loss 3.338359\n",
      "Params: tensor([  4.9858, -15.1430])\n",
      "Gradient: tensor([-0.0650,  0.3679])\n",
      "Epoch 1232, Loss 3.336965\n",
      "Params: tensor([  4.9865, -15.1467])\n",
      "Gradient: tensor([-0.0649,  0.3672])\n",
      "Epoch 1233, Loss 3.335577\n",
      "Params: tensor([  4.9871, -15.1504])\n",
      "Gradient: tensor([-0.0648,  0.3666])\n",
      "Epoch 1234, Loss 3.334191\n",
      "Params: tensor([  4.9878, -15.1540])\n",
      "Gradient: tensor([-0.0646,  0.3660])\n",
      "Epoch 1235, Loss 3.332811\n",
      "Params: tensor([  4.9884, -15.1577])\n",
      "Gradient: tensor([-0.0645,  0.3654])\n",
      "Epoch 1236, Loss 3.331435\n",
      "Params: tensor([  4.9891, -15.1613])\n",
      "Gradient: tensor([-0.0644,  0.3647])\n",
      "Epoch 1237, Loss 3.330065\n",
      "Params: tensor([  4.9897, -15.1650])\n",
      "Gradient: tensor([-0.0643,  0.3641])\n",
      "Epoch 1238, Loss 3.328699\n",
      "Params: tensor([  4.9904, -15.1686])\n",
      "Gradient: tensor([-0.0642,  0.3635])\n",
      "Epoch 1239, Loss 3.327338\n",
      "Params: tensor([  4.9910, -15.1722])\n",
      "Gradient: tensor([-0.0641,  0.3629])\n",
      "Epoch 1240, Loss 3.325980\n",
      "Params: tensor([  4.9916, -15.1759])\n",
      "Gradient: tensor([-0.0640,  0.3623])\n",
      "Epoch 1241, Loss 3.324628\n",
      "Params: tensor([  4.9923, -15.1795])\n",
      "Gradient: tensor([-0.0639,  0.3617])\n",
      "Epoch 1242, Loss 3.323279\n",
      "Params: tensor([  4.9929, -15.1831])\n",
      "Gradient: tensor([-0.0638,  0.3610])\n",
      "Epoch 1243, Loss 3.321934\n",
      "Params: tensor([  4.9936, -15.1867])\n",
      "Gradient: tensor([-0.0637,  0.3604])\n",
      "Epoch 1244, Loss 3.320599\n",
      "Params: tensor([  4.9942, -15.1903])\n",
      "Gradient: tensor([-0.0636,  0.3598])\n",
      "Epoch 1245, Loss 3.319264\n",
      "Params: tensor([  4.9948, -15.1939])\n",
      "Gradient: tensor([-0.0635,  0.3592])\n",
      "Epoch 1246, Loss 3.317935\n",
      "Params: tensor([  4.9955, -15.1975])\n",
      "Gradient: tensor([-0.0633,  0.3586])\n",
      "Epoch 1247, Loss 3.316610\n",
      "Params: tensor([  4.9961, -15.2010])\n",
      "Gradient: tensor([-0.0633,  0.3580])\n",
      "Epoch 1248, Loss 3.315289\n",
      "Params: tensor([  4.9967, -15.2046])\n",
      "Gradient: tensor([-0.0631,  0.3574])\n",
      "Epoch 1249, Loss 3.313973\n",
      "Params: tensor([  4.9973, -15.2082])\n",
      "Gradient: tensor([-0.0630,  0.3568])\n",
      "Epoch 1250, Loss 3.312663\n",
      "Params: tensor([  4.9980, -15.2117])\n",
      "Gradient: tensor([-0.0629,  0.3562])\n",
      "Epoch 1251, Loss 3.311353\n",
      "Params: tensor([  4.9986, -15.2153])\n",
      "Gradient: tensor([-0.0628,  0.3556])\n",
      "Epoch 1252, Loss 3.310052\n",
      "Params: tensor([  4.9992, -15.2189])\n",
      "Gradient: tensor([-0.0627,  0.3550])\n",
      "Epoch 1253, Loss 3.308756\n",
      "Params: tensor([  4.9999, -15.2224])\n",
      "Gradient: tensor([-0.0626,  0.3543])\n",
      "Epoch 1254, Loss 3.307462\n",
      "Params: tensor([  5.0005, -15.2259])\n",
      "Gradient: tensor([-0.0625,  0.3537])\n",
      "Epoch 1255, Loss 3.306170\n",
      "Params: tensor([  5.0011, -15.2295])\n",
      "Gradient: tensor([-0.0624,  0.3531])\n",
      "Epoch 1256, Loss 3.304887\n",
      "Params: tensor([  5.0017, -15.2330])\n",
      "Gradient: tensor([-0.0623,  0.3525])\n",
      "Epoch 1257, Loss 3.303605\n",
      "Params: tensor([  5.0024, -15.2365])\n",
      "Gradient: tensor([-0.0622,  0.3519])\n",
      "Epoch 1258, Loss 3.302329\n",
      "Params: tensor([  5.0030, -15.2400])\n",
      "Gradient: tensor([-0.0621,  0.3514])\n",
      "Epoch 1259, Loss 3.301057\n",
      "Params: tensor([  5.0036, -15.2435])\n",
      "Gradient: tensor([-0.0620,  0.3508])\n",
      "Epoch 1260, Loss 3.299791\n",
      "Params: tensor([  5.0042, -15.2470])\n",
      "Gradient: tensor([-0.0619,  0.3502])\n",
      "Epoch 1261, Loss 3.298527\n",
      "Params: tensor([  5.0048, -15.2505])\n",
      "Gradient: tensor([-0.0618,  0.3496])\n",
      "Epoch 1262, Loss 3.297267\n",
      "Params: tensor([  5.0054, -15.2540])\n",
      "Gradient: tensor([-0.0616,  0.3490])\n",
      "Epoch 1263, Loss 3.296014\n",
      "Params: tensor([  5.0061, -15.2575])\n",
      "Gradient: tensor([-0.0615,  0.3484])\n",
      "Epoch 1264, Loss 3.294762\n",
      "Params: tensor([  5.0067, -15.2610])\n",
      "Gradient: tensor([-0.0614,  0.3478])\n",
      "Epoch 1265, Loss 3.293518\n",
      "Params: tensor([  5.0073, -15.2645])\n",
      "Gradient: tensor([-0.0613,  0.3472])\n",
      "Epoch 1266, Loss 3.292275\n",
      "Params: tensor([  5.0079, -15.2679])\n",
      "Gradient: tensor([-0.0612,  0.3466])\n",
      "Epoch 1267, Loss 3.291036\n",
      "Params: tensor([  5.0085, -15.2714])\n",
      "Gradient: tensor([-0.0611,  0.3460])\n",
      "Epoch 1268, Loss 3.289804\n",
      "Params: tensor([  5.0091, -15.2748])\n",
      "Gradient: tensor([-0.0610,  0.3454])\n",
      "Epoch 1269, Loss 3.288573\n",
      "Params: tensor([  5.0097, -15.2783])\n",
      "Gradient: tensor([-0.0609,  0.3448])\n",
      "Epoch 1270, Loss 3.287347\n",
      "Params: tensor([  5.0103, -15.2817])\n",
      "Gradient: tensor([-0.0608,  0.3443])\n",
      "Epoch 1271, Loss 3.286129\n",
      "Params: tensor([  5.0109, -15.2852])\n",
      "Gradient: tensor([-0.0607,  0.3437])\n",
      "Epoch 1272, Loss 3.284911\n",
      "Params: tensor([  5.0116, -15.2886])\n",
      "Gradient: tensor([-0.0606,  0.3431])\n",
      "Epoch 1273, Loss 3.283698\n",
      "Params: tensor([  5.0122, -15.2920])\n",
      "Gradient: tensor([-0.0605,  0.3425])\n",
      "Epoch 1274, Loss 3.282488\n",
      "Params: tensor([  5.0128, -15.2954])\n",
      "Gradient: tensor([-0.0604,  0.3419])\n",
      "Epoch 1275, Loss 3.281284\n",
      "Params: tensor([  5.0134, -15.2988])\n",
      "Gradient: tensor([-0.0603,  0.3413])\n",
      "Epoch 1276, Loss 3.280085\n",
      "Params: tensor([  5.0140, -15.3023])\n",
      "Gradient: tensor([-0.0602,  0.3408])\n",
      "Epoch 1277, Loss 3.278888\n",
      "Params: tensor([  5.0146, -15.3057])\n",
      "Gradient: tensor([-0.0601,  0.3402])\n",
      "Epoch 1278, Loss 3.277696\n",
      "Params: tensor([  5.0152, -15.3091])\n",
      "Gradient: tensor([-0.0600,  0.3396])\n",
      "Epoch 1279, Loss 3.276506\n",
      "Params: tensor([  5.0158, -15.3124])\n",
      "Gradient: tensor([-0.0599,  0.3390])\n",
      "Epoch 1280, Loss 3.275322\n",
      "Params: tensor([  5.0164, -15.3158])\n",
      "Gradient: tensor([-0.0598,  0.3384])\n",
      "Epoch 1281, Loss 3.274142\n",
      "Params: tensor([  5.0170, -15.3192])\n",
      "Gradient: tensor([-0.0597,  0.3379])\n",
      "Epoch 1282, Loss 3.272967\n",
      "Params: tensor([  5.0176, -15.3226])\n",
      "Gradient: tensor([-0.0596,  0.3373])\n",
      "Epoch 1283, Loss 3.271793\n",
      "Params: tensor([  5.0182, -15.3259])\n",
      "Gradient: tensor([-0.0595,  0.3367])\n",
      "Epoch 1284, Loss 3.270625\n",
      "Params: tensor([  5.0187, -15.3293])\n",
      "Gradient: tensor([-0.0594,  0.3362])\n",
      "Epoch 1285, Loss 3.269461\n",
      "Params: tensor([  5.0193, -15.3327])\n",
      "Gradient: tensor([-0.0593,  0.3356])\n",
      "Epoch 1286, Loss 3.268301\n",
      "Params: tensor([  5.0199, -15.3360])\n",
      "Gradient: tensor([-0.0592,  0.3350])\n",
      "Epoch 1287, Loss 3.267143\n",
      "Params: tensor([  5.0205, -15.3394])\n",
      "Gradient: tensor([-0.0591,  0.3344])\n",
      "Epoch 1288, Loss 3.265992\n",
      "Params: tensor([  5.0211, -15.3427])\n",
      "Gradient: tensor([-0.0590,  0.3339])\n",
      "Epoch 1289, Loss 3.264842\n",
      "Params: tensor([  5.0217, -15.3460])\n",
      "Gradient: tensor([-0.0589,  0.3333])\n",
      "Epoch 1290, Loss 3.263700\n",
      "Params: tensor([  5.0223, -15.3494])\n",
      "Gradient: tensor([-0.0588,  0.3327])\n",
      "Epoch 1291, Loss 3.262556\n",
      "Params: tensor([  5.0229, -15.3527])\n",
      "Gradient: tensor([-0.0587,  0.3322])\n",
      "Epoch 1292, Loss 3.261421\n",
      "Params: tensor([  5.0235, -15.3560])\n",
      "Gradient: tensor([-0.0586,  0.3316])\n",
      "Epoch 1293, Loss 3.260287\n",
      "Params: tensor([  5.0240, -15.3593])\n",
      "Gradient: tensor([-0.0585,  0.3311])\n",
      "Epoch 1294, Loss 3.259161\n",
      "Params: tensor([  5.0246, -15.3626])\n",
      "Gradient: tensor([-0.0584,  0.3305])\n",
      "Epoch 1295, Loss 3.258033\n",
      "Params: tensor([  5.0252, -15.3659])\n",
      "Gradient: tensor([-0.0583,  0.3299])\n",
      "Epoch 1296, Loss 3.256912\n",
      "Params: tensor([  5.0258, -15.3692])\n",
      "Gradient: tensor([-0.0582,  0.3294])\n",
      "Epoch 1297, Loss 3.255795\n",
      "Params: tensor([  5.0264, -15.3725])\n",
      "Gradient: tensor([-0.0581,  0.3288])\n",
      "Epoch 1298, Loss 3.254681\n",
      "Params: tensor([  5.0270, -15.3758])\n",
      "Gradient: tensor([-0.0580,  0.3282])\n",
      "Epoch 1299, Loss 3.253569\n",
      "Params: tensor([  5.0275, -15.3791])\n",
      "Gradient: tensor([-0.0579,  0.3277])\n",
      "Epoch 1300, Loss 3.252462\n",
      "Params: tensor([  5.0281, -15.3823])\n",
      "Gradient: tensor([-0.0578,  0.3271])\n",
      "Epoch 1301, Loss 3.251362\n",
      "Params: tensor([  5.0287, -15.3856])\n",
      "Gradient: tensor([-0.0577,  0.3266])\n",
      "Epoch 1302, Loss 3.250264\n",
      "Params: tensor([  5.0293, -15.3888])\n",
      "Gradient: tensor([-0.0576,  0.3260])\n",
      "Epoch 1303, Loss 3.249168\n",
      "Params: tensor([  5.0298, -15.3921])\n",
      "Gradient: tensor([-0.0575,  0.3255])\n",
      "Epoch 1304, Loss 3.248077\n",
      "Params: tensor([  5.0304, -15.3954])\n",
      "Gradient: tensor([-0.0574,  0.3249])\n",
      "Epoch 1305, Loss 3.246989\n",
      "Params: tensor([  5.0310, -15.3986])\n",
      "Gradient: tensor([-0.0573,  0.3244])\n",
      "Epoch 1306, Loss 3.245904\n",
      "Params: tensor([  5.0316, -15.4018])\n",
      "Gradient: tensor([-0.0572,  0.3238])\n",
      "Epoch 1307, Loss 3.244824\n",
      "Params: tensor([  5.0321, -15.4051])\n",
      "Gradient: tensor([-0.0571,  0.3233])\n",
      "Epoch 1308, Loss 3.243747\n",
      "Params: tensor([  5.0327, -15.4083])\n",
      "Gradient: tensor([-0.0570,  0.3227])\n",
      "Epoch 1309, Loss 3.242674\n",
      "Params: tensor([  5.0333, -15.4115])\n",
      "Gradient: tensor([-0.0569,  0.3222])\n",
      "Epoch 1310, Loss 3.241606\n",
      "Params: tensor([  5.0338, -15.4147])\n",
      "Gradient: tensor([-0.0568,  0.3216])\n",
      "Epoch 1311, Loss 3.240538\n",
      "Params: tensor([  5.0344, -15.4179])\n",
      "Gradient: tensor([-0.0567,  0.3211])\n",
      "Epoch 1312, Loss 3.239475\n",
      "Params: tensor([  5.0350, -15.4211])\n",
      "Gradient: tensor([-0.0566,  0.3205])\n",
      "Epoch 1313, Loss 3.238420\n",
      "Params: tensor([  5.0355, -15.4243])\n",
      "Gradient: tensor([-0.0565,  0.3200])\n",
      "Epoch 1314, Loss 3.237363\n",
      "Params: tensor([  5.0361, -15.4275])\n",
      "Gradient: tensor([-0.0564,  0.3194])\n",
      "Epoch 1315, Loss 3.236314\n",
      "Params: tensor([  5.0367, -15.4307])\n",
      "Gradient: tensor([-0.0563,  0.3189])\n",
      "Epoch 1316, Loss 3.235265\n",
      "Params: tensor([  5.0372, -15.4339])\n",
      "Gradient: tensor([-0.0562,  0.3184])\n",
      "Epoch 1317, Loss 3.234218\n",
      "Params: tensor([  5.0378, -15.4371])\n",
      "Gradient: tensor([-0.0561,  0.3178])\n",
      "Epoch 1318, Loss 3.233179\n",
      "Params: tensor([  5.0383, -15.4403])\n",
      "Gradient: tensor([-0.0561,  0.3173])\n",
      "Epoch 1319, Loss 3.232143\n",
      "Params: tensor([  5.0389, -15.4434])\n",
      "Gradient: tensor([-0.0560,  0.3167])\n",
      "Epoch 1320, Loss 3.231109\n",
      "Params: tensor([  5.0395, -15.4466])\n",
      "Gradient: tensor([-0.0558,  0.3162])\n",
      "Epoch 1321, Loss 3.230078\n",
      "Params: tensor([  5.0400, -15.4498])\n",
      "Gradient: tensor([-0.0558,  0.3157])\n",
      "Epoch 1322, Loss 3.229051\n",
      "Params: tensor([  5.0406, -15.4529])\n",
      "Gradient: tensor([-0.0557,  0.3151])\n",
      "Epoch 1323, Loss 3.228027\n",
      "Params: tensor([  5.0411, -15.4560])\n",
      "Gradient: tensor([-0.0556,  0.3146])\n",
      "Epoch 1324, Loss 3.227010\n",
      "Params: tensor([  5.0417, -15.4592])\n",
      "Gradient: tensor([-0.0555,  0.3141])\n",
      "Epoch 1325, Loss 3.225992\n",
      "Params: tensor([  5.0422, -15.4623])\n",
      "Gradient: tensor([-0.0554,  0.3135])\n",
      "Epoch 1326, Loss 3.224980\n",
      "Params: tensor([  5.0428, -15.4655])\n",
      "Gradient: tensor([-0.0553,  0.3130])\n",
      "Epoch 1327, Loss 3.223971\n",
      "Params: tensor([  5.0433, -15.4686])\n",
      "Gradient: tensor([-0.0552,  0.3125])\n",
      "Epoch 1328, Loss 3.222965\n",
      "Params: tensor([  5.0439, -15.4717])\n",
      "Gradient: tensor([-0.0551,  0.3119])\n",
      "Epoch 1329, Loss 3.221960\n",
      "Params: tensor([  5.0444, -15.4748])\n",
      "Gradient: tensor([-0.0550,  0.3114])\n",
      "Epoch 1330, Loss 3.220962\n",
      "Params: tensor([  5.0450, -15.4779])\n",
      "Gradient: tensor([-0.0549,  0.3109])\n",
      "Epoch 1331, Loss 3.219967\n",
      "Params: tensor([  5.0455, -15.4810])\n",
      "Gradient: tensor([-0.0548,  0.3103])\n",
      "Epoch 1332, Loss 3.218975\n",
      "Params: tensor([  5.0461, -15.4841])\n",
      "Gradient: tensor([-0.0547,  0.3098])\n",
      "Epoch 1333, Loss 3.217986\n",
      "Params: tensor([  5.0466, -15.4872])\n",
      "Gradient: tensor([-0.0546,  0.3093])\n",
      "Epoch 1334, Loss 3.217000\n",
      "Params: tensor([  5.0472, -15.4903])\n",
      "Gradient: tensor([-0.0545,  0.3088])\n",
      "Epoch 1335, Loss 3.216017\n",
      "Params: tensor([  5.0477, -15.4934])\n",
      "Gradient: tensor([-0.0544,  0.3082])\n",
      "Epoch 1336, Loss 3.215038\n",
      "Params: tensor([  5.0483, -15.4965])\n",
      "Gradient: tensor([-0.0543,  0.3077])\n",
      "Epoch 1337, Loss 3.214062\n",
      "Params: tensor([  5.0488, -15.4995])\n",
      "Gradient: tensor([-0.0543,  0.3072])\n",
      "Epoch 1338, Loss 3.213092\n",
      "Params: tensor([  5.0494, -15.5026])\n",
      "Gradient: tensor([-0.0542,  0.3067])\n",
      "Epoch 1339, Loss 3.212122\n",
      "Params: tensor([  5.0499, -15.5057])\n",
      "Gradient: tensor([-0.0541,  0.3061])\n",
      "Epoch 1340, Loss 3.211157\n",
      "Params: tensor([  5.0504, -15.5087])\n",
      "Gradient: tensor([-0.0540,  0.3056])\n",
      "Epoch 1341, Loss 3.210192\n",
      "Params: tensor([  5.0510, -15.5118])\n",
      "Gradient: tensor([-0.0539,  0.3051])\n",
      "Epoch 1342, Loss 3.209235\n",
      "Params: tensor([  5.0515, -15.5148])\n",
      "Gradient: tensor([-0.0538,  0.3046])\n",
      "Epoch 1343, Loss 3.208279\n",
      "Params: tensor([  5.0521, -15.5179])\n",
      "Gradient: tensor([-0.0537,  0.3041])\n",
      "Epoch 1344, Loss 3.207326\n",
      "Params: tensor([  5.0526, -15.5209])\n",
      "Gradient: tensor([-0.0536,  0.3036])\n",
      "Epoch 1345, Loss 3.206377\n",
      "Params: tensor([  5.0531, -15.5239])\n",
      "Gradient: tensor([-0.0535,  0.3030])\n",
      "Epoch 1346, Loss 3.205430\n",
      "Params: tensor([  5.0537, -15.5269])\n",
      "Gradient: tensor([-0.0534,  0.3025])\n",
      "Epoch 1347, Loss 3.204488\n",
      "Params: tensor([  5.0542, -15.5300])\n",
      "Gradient: tensor([-0.0533,  0.3020])\n",
      "Epoch 1348, Loss 3.203547\n",
      "Params: tensor([  5.0547, -15.5330])\n",
      "Gradient: tensor([-0.0532,  0.3015])\n",
      "Epoch 1349, Loss 3.202611\n",
      "Params: tensor([  5.0553, -15.5360])\n",
      "Gradient: tensor([-0.0532,  0.3010])\n",
      "Epoch 1350, Loss 3.201678\n",
      "Params: tensor([  5.0558, -15.5390])\n",
      "Gradient: tensor([-0.0531,  0.3005])\n",
      "Epoch 1351, Loss 3.200747\n",
      "Params: tensor([  5.0563, -15.5420])\n",
      "Gradient: tensor([-0.0530,  0.3000])\n",
      "Epoch 1352, Loss 3.199820\n",
      "Params: tensor([  5.0568, -15.5450])\n",
      "Gradient: tensor([-0.0529,  0.2995])\n",
      "Epoch 1353, Loss 3.198897\n",
      "Params: tensor([  5.0574, -15.5480])\n",
      "Gradient: tensor([-0.0528,  0.2989])\n",
      "Epoch 1354, Loss 3.197976\n",
      "Params: tensor([  5.0579, -15.5510])\n",
      "Gradient: tensor([-0.0527,  0.2984])\n",
      "Epoch 1355, Loss 3.197060\n",
      "Params: tensor([  5.0584, -15.5539])\n",
      "Gradient: tensor([-0.0526,  0.2979])\n",
      "Epoch 1356, Loss 3.196143\n",
      "Params: tensor([  5.0590, -15.5569])\n",
      "Gradient: tensor([-0.0525,  0.2974])\n",
      "Epoch 1357, Loss 3.195231\n",
      "Params: tensor([  5.0595, -15.5599])\n",
      "Gradient: tensor([-0.0524,  0.2969])\n",
      "Epoch 1358, Loss 3.194324\n",
      "Params: tensor([  5.0600, -15.5629])\n",
      "Gradient: tensor([-0.0524,  0.2964])\n",
      "Epoch 1359, Loss 3.193420\n",
      "Params: tensor([  5.0605, -15.5658])\n",
      "Gradient: tensor([-0.0523,  0.2959])\n",
      "Epoch 1360, Loss 3.192517\n",
      "Params: tensor([  5.0610, -15.5688])\n",
      "Gradient: tensor([-0.0522,  0.2954])\n",
      "Epoch 1361, Loss 3.191616\n",
      "Params: tensor([  5.0616, -15.5717])\n",
      "Gradient: tensor([-0.0521,  0.2949])\n",
      "Epoch 1362, Loss 3.190721\n",
      "Params: tensor([  5.0621, -15.5747])\n",
      "Gradient: tensor([-0.0520,  0.2944])\n",
      "Epoch 1363, Loss 3.189829\n",
      "Params: tensor([  5.0626, -15.5776])\n",
      "Gradient: tensor([-0.0519,  0.2939])\n",
      "Epoch 1364, Loss 3.188938\n",
      "Params: tensor([  5.0631, -15.5805])\n",
      "Gradient: tensor([-0.0518,  0.2934])\n",
      "Epoch 1365, Loss 3.188051\n",
      "Params: tensor([  5.0636, -15.5835])\n",
      "Gradient: tensor([-0.0517,  0.2929])\n",
      "Epoch 1366, Loss 3.187166\n",
      "Params: tensor([  5.0642, -15.5864])\n",
      "Gradient: tensor([-0.0516,  0.2924])\n",
      "Epoch 1367, Loss 3.186287\n",
      "Params: tensor([  5.0647, -15.5893])\n",
      "Gradient: tensor([-0.0516,  0.2919])\n",
      "Epoch 1368, Loss 3.185409\n",
      "Params: tensor([  5.0652, -15.5922])\n",
      "Gradient: tensor([-0.0515,  0.2914])\n",
      "Epoch 1369, Loss 3.184535\n",
      "Params: tensor([  5.0657, -15.5951])\n",
      "Gradient: tensor([-0.0514,  0.2909])\n",
      "Epoch 1370, Loss 3.183662\n",
      "Params: tensor([  5.0662, -15.5980])\n",
      "Gradient: tensor([-0.0513,  0.2904])\n",
      "Epoch 1371, Loss 3.182791\n",
      "Params: tensor([  5.0667, -15.6009])\n",
      "Gradient: tensor([-0.0512,  0.2899])\n",
      "Epoch 1372, Loss 3.181925\n",
      "Params: tensor([  5.0672, -15.6038])\n",
      "Gradient: tensor([-0.0511,  0.2894])\n",
      "Epoch 1373, Loss 3.181063\n",
      "Params: tensor([  5.0678, -15.6067])\n",
      "Gradient: tensor([-0.0510,  0.2890])\n",
      "Epoch 1374, Loss 3.180201\n",
      "Params: tensor([  5.0683, -15.6096])\n",
      "Gradient: tensor([-0.0509,  0.2885])\n",
      "Epoch 1375, Loss 3.179347\n",
      "Params: tensor([  5.0688, -15.6125])\n",
      "Gradient: tensor([-0.0509,  0.2880])\n",
      "Epoch 1376, Loss 3.178490\n",
      "Params: tensor([  5.0693, -15.6154])\n",
      "Gradient: tensor([-0.0508,  0.2875])\n",
      "Epoch 1377, Loss 3.177638\n",
      "Params: tensor([  5.0698, -15.6182])\n",
      "Gradient: tensor([-0.0507,  0.2870])\n",
      "Epoch 1378, Loss 3.176789\n",
      "Params: tensor([  5.0703, -15.6211])\n",
      "Gradient: tensor([-0.0506,  0.2865])\n",
      "Epoch 1379, Loss 3.175945\n",
      "Params: tensor([  5.0708, -15.6240])\n",
      "Gradient: tensor([-0.0505,  0.2860])\n",
      "Epoch 1380, Loss 3.175101\n",
      "Params: tensor([  5.0713, -15.6268])\n",
      "Gradient: tensor([-0.0504,  0.2855])\n",
      "Epoch 1381, Loss 3.174262\n",
      "Params: tensor([  5.0718, -15.6297])\n",
      "Gradient: tensor([-0.0504,  0.2850])\n",
      "Epoch 1382, Loss 3.173425\n",
      "Params: tensor([  5.0723, -15.6325])\n",
      "Gradient: tensor([-0.0503,  0.2846])\n",
      "Epoch 1383, Loss 3.172590\n",
      "Params: tensor([  5.0728, -15.6353])\n",
      "Gradient: tensor([-0.0502,  0.2841])\n",
      "Epoch 1384, Loss 3.171759\n",
      "Params: tensor([  5.0733, -15.6382])\n",
      "Gradient: tensor([-0.0501,  0.2836])\n",
      "Epoch 1385, Loss 3.170929\n",
      "Params: tensor([  5.0738, -15.6410])\n",
      "Gradient: tensor([-0.0500,  0.2831])\n",
      "Epoch 1386, Loss 3.170103\n",
      "Params: tensor([  5.0743, -15.6438])\n",
      "Gradient: tensor([-0.0499,  0.2826])\n",
      "Epoch 1387, Loss 3.169280\n",
      "Params: tensor([  5.0748, -15.6467])\n",
      "Gradient: tensor([-0.0498,  0.2822])\n",
      "Epoch 1388, Loss 3.168462\n",
      "Params: tensor([  5.0753, -15.6495])\n",
      "Gradient: tensor([-0.0498,  0.2817])\n",
      "Epoch 1389, Loss 3.167644\n",
      "Params: tensor([  5.0758, -15.6523])\n",
      "Gradient: tensor([-0.0497,  0.2812])\n",
      "Epoch 1390, Loss 3.166827\n",
      "Params: tensor([  5.0763, -15.6551])\n",
      "Gradient: tensor([-0.0496,  0.2807])\n",
      "Epoch 1391, Loss 3.166017\n",
      "Params: tensor([  5.0768, -15.6579])\n",
      "Gradient: tensor([-0.0495,  0.2802])\n",
      "Epoch 1392, Loss 3.165206\n",
      "Params: tensor([  5.0773, -15.6607])\n",
      "Gradient: tensor([-0.0494,  0.2798])\n",
      "Epoch 1393, Loss 3.164401\n",
      "Params: tensor([  5.0778, -15.6635])\n",
      "Gradient: tensor([-0.0493,  0.2793])\n",
      "Epoch 1394, Loss 3.163594\n",
      "Params: tensor([  5.0783, -15.6663])\n",
      "Gradient: tensor([-0.0492,  0.2788])\n",
      "Epoch 1395, Loss 3.162795\n",
      "Params: tensor([  5.0788, -15.6691])\n",
      "Gradient: tensor([-0.0492,  0.2783])\n",
      "Epoch 1396, Loss 3.161996\n",
      "Params: tensor([  5.0793, -15.6718])\n",
      "Gradient: tensor([-0.0491,  0.2779])\n",
      "Epoch 1397, Loss 3.161201\n",
      "Params: tensor([  5.0797, -15.6746])\n",
      "Gradient: tensor([-0.0490,  0.2774])\n",
      "Epoch 1398, Loss 3.160410\n",
      "Params: tensor([  5.0802, -15.6774])\n",
      "Gradient: tensor([-0.0489,  0.2769])\n",
      "Epoch 1399, Loss 3.159618\n",
      "Params: tensor([  5.0807, -15.6802])\n",
      "Gradient: tensor([-0.0488,  0.2765])\n",
      "Epoch 1400, Loss 3.158831\n",
      "Params: tensor([  5.0812, -15.6829])\n",
      "Gradient: tensor([-0.0488,  0.2760])\n",
      "Epoch 1401, Loss 3.158046\n",
      "Params: tensor([  5.0817, -15.6857])\n",
      "Gradient: tensor([-0.0487,  0.2755])\n",
      "Epoch 1402, Loss 3.157263\n",
      "Params: tensor([  5.0822, -15.6884])\n",
      "Gradient: tensor([-0.0486,  0.2751])\n",
      "Epoch 1403, Loss 3.156484\n",
      "Params: tensor([  5.0827, -15.6912])\n",
      "Gradient: tensor([-0.0485,  0.2746])\n",
      "Epoch 1404, Loss 3.155708\n",
      "Params: tensor([  5.0832, -15.6939])\n",
      "Gradient: tensor([-0.0484,  0.2741])\n",
      "Epoch 1405, Loss 3.154933\n",
      "Params: tensor([  5.0836, -15.6966])\n",
      "Gradient: tensor([-0.0483,  0.2736])\n",
      "Epoch 1406, Loss 3.154162\n",
      "Params: tensor([  5.0841, -15.6994])\n",
      "Gradient: tensor([-0.0483,  0.2732])\n",
      "Epoch 1407, Loss 3.153393\n",
      "Params: tensor([  5.0846, -15.7021])\n",
      "Gradient: tensor([-0.0482,  0.2727])\n",
      "Epoch 1408, Loss 3.152627\n",
      "Params: tensor([  5.0851, -15.7048])\n",
      "Gradient: tensor([-0.0481,  0.2723])\n",
      "Epoch 1409, Loss 3.151865\n",
      "Params: tensor([  5.0856, -15.7075])\n",
      "Gradient: tensor([-0.0480,  0.2718])\n",
      "Epoch 1410, Loss 3.151101\n",
      "Params: tensor([  5.0860, -15.7103])\n",
      "Gradient: tensor([-0.0479,  0.2713])\n",
      "Epoch 1411, Loss 3.150343\n",
      "Params: tensor([  5.0865, -15.7130])\n",
      "Gradient: tensor([-0.0479,  0.2709])\n",
      "Epoch 1412, Loss 3.149587\n",
      "Params: tensor([  5.0870, -15.7157])\n",
      "Gradient: tensor([-0.0478,  0.2704])\n",
      "Epoch 1413, Loss 3.148833\n",
      "Params: tensor([  5.0875, -15.7184])\n",
      "Gradient: tensor([-0.0477,  0.2700])\n",
      "Epoch 1414, Loss 3.148082\n",
      "Params: tensor([  5.0879, -15.7211])\n",
      "Gradient: tensor([-0.0476,  0.2695])\n",
      "Epoch 1415, Loss 3.147335\n",
      "Params: tensor([  5.0884, -15.7238])\n",
      "Gradient: tensor([-0.0475,  0.2690])\n",
      "Epoch 1416, Loss 3.146588\n",
      "Params: tensor([  5.0889, -15.7264])\n",
      "Gradient: tensor([-0.0474,  0.2686])\n",
      "Epoch 1417, Loss 3.145845\n",
      "Params: tensor([  5.0894, -15.7291])\n",
      "Gradient: tensor([-0.0474,  0.2681])\n",
      "Epoch 1418, Loss 3.145105\n",
      "Params: tensor([  5.0898, -15.7318])\n",
      "Gradient: tensor([-0.0473,  0.2677])\n",
      "Epoch 1419, Loss 3.144367\n",
      "Params: tensor([  5.0903, -15.7345])\n",
      "Gradient: tensor([-0.0472,  0.2672])\n",
      "Epoch 1420, Loss 3.143630\n",
      "Params: tensor([  5.0908, -15.7371])\n",
      "Gradient: tensor([-0.0471,  0.2668])\n",
      "Epoch 1421, Loss 3.142899\n",
      "Params: tensor([  5.0913, -15.7398])\n",
      "Gradient: tensor([-0.0470,  0.2663])\n",
      "Epoch 1422, Loss 3.142166\n",
      "Params: tensor([  5.0917, -15.7425])\n",
      "Gradient: tensor([-0.0469,  0.2659])\n",
      "Epoch 1423, Loss 3.141439\n",
      "Params: tensor([  5.0922, -15.7451])\n",
      "Gradient: tensor([-0.0469,  0.2654])\n",
      "Epoch 1424, Loss 3.140712\n",
      "Params: tensor([  5.0927, -15.7478])\n",
      "Gradient: tensor([-0.0468,  0.2650])\n",
      "Epoch 1425, Loss 3.139989\n",
      "Params: tensor([  5.0931, -15.7504])\n",
      "Gradient: tensor([-0.0467,  0.2645])\n",
      "Epoch 1426, Loss 3.139271\n",
      "Params: tensor([  5.0936, -15.7530])\n",
      "Gradient: tensor([-0.0466,  0.2641])\n",
      "Epoch 1427, Loss 3.138551\n",
      "Params: tensor([  5.0941, -15.7557])\n",
      "Gradient: tensor([-0.0466,  0.2636])\n",
      "Epoch 1428, Loss 3.137835\n",
      "Params: tensor([  5.0945, -15.7583])\n",
      "Gradient: tensor([-0.0465,  0.2632])\n",
      "Epoch 1429, Loss 3.137121\n",
      "Params: tensor([  5.0950, -15.7609])\n",
      "Gradient: tensor([-0.0464,  0.2627])\n",
      "Epoch 1430, Loss 3.136409\n",
      "Params: tensor([  5.0955, -15.7636])\n",
      "Gradient: tensor([-0.0463,  0.2623])\n",
      "Epoch 1431, Loss 3.135702\n",
      "Params: tensor([  5.0959, -15.7662])\n",
      "Gradient: tensor([-0.0462,  0.2618])\n",
      "Epoch 1432, Loss 3.134995\n",
      "Params: tensor([  5.0964, -15.7688])\n",
      "Gradient: tensor([-0.0461,  0.2614])\n",
      "Epoch 1433, Loss 3.134292\n",
      "Params: tensor([  5.0968, -15.7714])\n",
      "Gradient: tensor([-0.0461,  0.2609])\n",
      "Epoch 1434, Loss 3.133590\n",
      "Params: tensor([  5.0973, -15.7740])\n",
      "Gradient: tensor([-0.0460,  0.2605])\n",
      "Epoch 1435, Loss 3.132889\n",
      "Params: tensor([  5.0978, -15.7766])\n",
      "Gradient: tensor([-0.0459,  0.2600])\n",
      "Epoch 1436, Loss 3.132194\n",
      "Params: tensor([  5.0982, -15.7792])\n",
      "Gradient: tensor([-0.0459,  0.2596])\n",
      "Epoch 1437, Loss 3.131500\n",
      "Params: tensor([  5.0987, -15.7818])\n",
      "Gradient: tensor([-0.0458,  0.2592])\n",
      "Epoch 1438, Loss 3.130810\n",
      "Params: tensor([  5.0991, -15.7844])\n",
      "Gradient: tensor([-0.0457,  0.2587])\n",
      "Epoch 1439, Loss 3.130119\n",
      "Params: tensor([  5.0996, -15.7870])\n",
      "Gradient: tensor([-0.0456,  0.2583])\n",
      "Epoch 1440, Loss 3.129432\n",
      "Params: tensor([  5.1000, -15.7895])\n",
      "Gradient: tensor([-0.0455,  0.2578])\n",
      "Epoch 1441, Loss 3.128746\n",
      "Params: tensor([  5.1005, -15.7921])\n",
      "Gradient: tensor([-0.0455,  0.2574])\n",
      "Epoch 1442, Loss 3.128064\n",
      "Params: tensor([  5.1010, -15.7947])\n",
      "Gradient: tensor([-0.0454,  0.2570])\n",
      "Epoch 1443, Loss 3.127381\n",
      "Params: tensor([  5.1014, -15.7973])\n",
      "Gradient: tensor([-0.0453,  0.2565])\n",
      "Epoch 1444, Loss 3.126705\n",
      "Params: tensor([  5.1019, -15.7998])\n",
      "Gradient: tensor([-0.0453,  0.2561])\n",
      "Epoch 1445, Loss 3.126031\n",
      "Params: tensor([  5.1023, -15.8024])\n",
      "Gradient: tensor([-0.0452,  0.2557])\n",
      "Epoch 1446, Loss 3.125356\n",
      "Params: tensor([  5.1028, -15.8049])\n",
      "Gradient: tensor([-0.0451,  0.2552])\n",
      "Epoch 1447, Loss 3.124683\n",
      "Params: tensor([  5.1032, -15.8075])\n",
      "Gradient: tensor([-0.0450,  0.2548])\n",
      "Epoch 1448, Loss 3.124017\n",
      "Params: tensor([  5.1037, -15.8100])\n",
      "Gradient: tensor([-0.0449,  0.2544])\n",
      "Epoch 1449, Loss 3.123349\n",
      "Params: tensor([  5.1041, -15.8126])\n",
      "Gradient: tensor([-0.0449,  0.2539])\n",
      "Epoch 1450, Loss 3.122686\n",
      "Params: tensor([  5.1046, -15.8151])\n",
      "Gradient: tensor([-0.0448,  0.2535])\n",
      "Epoch 1451, Loss 3.122022\n",
      "Params: tensor([  5.1050, -15.8176])\n",
      "Gradient: tensor([-0.0447,  0.2531])\n",
      "Epoch 1452, Loss 3.121362\n",
      "Params: tensor([  5.1055, -15.8201])\n",
      "Gradient: tensor([-0.0446,  0.2526])\n",
      "Epoch 1453, Loss 3.120706\n",
      "Params: tensor([  5.1059, -15.8227])\n",
      "Gradient: tensor([-0.0445,  0.2522])\n",
      "Epoch 1454, Loss 3.120048\n",
      "Params: tensor([  5.1063, -15.8252])\n",
      "Gradient: tensor([-0.0445,  0.2518])\n",
      "Epoch 1455, Loss 3.119396\n",
      "Params: tensor([  5.1068, -15.8277])\n",
      "Gradient: tensor([-0.0444,  0.2513])\n",
      "Epoch 1456, Loss 3.118746\n",
      "Params: tensor([  5.1072, -15.8302])\n",
      "Gradient: tensor([-0.0443,  0.2509])\n",
      "Epoch 1457, Loss 3.118098\n",
      "Params: tensor([  5.1077, -15.8327])\n",
      "Gradient: tensor([-0.0442,  0.2505])\n",
      "Epoch 1458, Loss 3.117452\n",
      "Params: tensor([  5.1081, -15.8352])\n",
      "Gradient: tensor([-0.0442,  0.2501])\n",
      "Epoch 1459, Loss 3.116805\n",
      "Params: tensor([  5.1086, -15.8377])\n",
      "Gradient: tensor([-0.0441,  0.2496])\n",
      "Epoch 1460, Loss 3.116164\n",
      "Params: tensor([  5.1090, -15.8402])\n",
      "Gradient: tensor([-0.0440,  0.2492])\n",
      "Epoch 1461, Loss 3.115525\n",
      "Params: tensor([  5.1094, -15.8427])\n",
      "Gradient: tensor([-0.0439,  0.2488])\n",
      "Epoch 1462, Loss 3.114886\n",
      "Params: tensor([  5.1099, -15.8452])\n",
      "Gradient: tensor([-0.0439,  0.2484])\n",
      "Epoch 1463, Loss 3.114250\n",
      "Params: tensor([  5.1103, -15.8477])\n",
      "Gradient: tensor([-0.0438,  0.2480])\n",
      "Epoch 1464, Loss 3.113617\n",
      "Params: tensor([  5.1107, -15.8501])\n",
      "Gradient: tensor([-0.0437,  0.2475])\n",
      "Epoch 1465, Loss 3.112985\n",
      "Params: tensor([  5.1112, -15.8526])\n",
      "Gradient: tensor([-0.0437,  0.2471])\n",
      "Epoch 1466, Loss 3.112358\n",
      "Params: tensor([  5.1116, -15.8551])\n",
      "Gradient: tensor([-0.0436,  0.2467])\n",
      "Epoch 1467, Loss 3.111730\n",
      "Params: tensor([  5.1121, -15.8575])\n",
      "Gradient: tensor([-0.0435,  0.2463])\n",
      "Epoch 1468, Loss 3.111103\n",
      "Params: tensor([  5.1125, -15.8600])\n",
      "Gradient: tensor([-0.0434,  0.2459])\n",
      "Epoch 1469, Loss 3.110484\n",
      "Params: tensor([  5.1129, -15.8624])\n",
      "Gradient: tensor([-0.0433,  0.2454])\n",
      "Epoch 1470, Loss 3.109859\n",
      "Params: tensor([  5.1134, -15.8649])\n",
      "Gradient: tensor([-0.0433,  0.2450])\n",
      "Epoch 1471, Loss 3.109242\n",
      "Params: tensor([  5.1138, -15.8673])\n",
      "Gradient: tensor([-0.0432,  0.2446])\n",
      "Epoch 1472, Loss 3.108627\n",
      "Params: tensor([  5.1142, -15.8698])\n",
      "Gradient: tensor([-0.0431,  0.2442])\n",
      "Epoch 1473, Loss 3.108011\n",
      "Params: tensor([  5.1147, -15.8722])\n",
      "Gradient: tensor([-0.0430,  0.2438])\n",
      "Epoch 1474, Loss 3.107401\n",
      "Params: tensor([  5.1151, -15.8747])\n",
      "Gradient: tensor([-0.0430,  0.2434])\n",
      "Epoch 1475, Loss 3.106791\n",
      "Params: tensor([  5.1155, -15.8771])\n",
      "Gradient: tensor([-0.0429,  0.2429])\n",
      "Epoch 1476, Loss 3.106180\n",
      "Params: tensor([  5.1159, -15.8795])\n",
      "Gradient: tensor([-0.0428,  0.2425])\n",
      "Epoch 1477, Loss 3.105575\n",
      "Params: tensor([  5.1164, -15.8819])\n",
      "Gradient: tensor([-0.0428,  0.2421])\n",
      "Epoch 1478, Loss 3.104972\n",
      "Params: tensor([  5.1168, -15.8843])\n",
      "Gradient: tensor([-0.0427,  0.2417])\n",
      "Epoch 1479, Loss 3.104370\n",
      "Params: tensor([  5.1172, -15.8868])\n",
      "Gradient: tensor([-0.0426,  0.2413])\n",
      "Epoch 1480, Loss 3.103770\n",
      "Params: tensor([  5.1176, -15.8892])\n",
      "Gradient: tensor([-0.0425,  0.2409])\n",
      "Epoch 1481, Loss 3.103172\n",
      "Params: tensor([  5.1181, -15.8916])\n",
      "Gradient: tensor([-0.0425,  0.2405])\n",
      "Epoch 1482, Loss 3.102576\n",
      "Params: tensor([  5.1185, -15.8940])\n",
      "Gradient: tensor([-0.0424,  0.2401])\n",
      "Epoch 1483, Loss 3.101982\n",
      "Params: tensor([  5.1189, -15.8964])\n",
      "Gradient: tensor([-0.0423,  0.2397])\n",
      "Epoch 1484, Loss 3.101390\n",
      "Params: tensor([  5.1193, -15.8988])\n",
      "Gradient: tensor([-0.0423,  0.2393])\n",
      "Epoch 1485, Loss 3.100802\n",
      "Params: tensor([  5.1198, -15.9011])\n",
      "Gradient: tensor([-0.0422,  0.2388])\n",
      "Epoch 1486, Loss 3.100213\n",
      "Params: tensor([  5.1202, -15.9035])\n",
      "Gradient: tensor([-0.0421,  0.2384])\n",
      "Epoch 1487, Loss 3.099627\n",
      "Params: tensor([  5.1206, -15.9059])\n",
      "Gradient: tensor([-0.0421,  0.2380])\n",
      "Epoch 1488, Loss 3.099044\n",
      "Params: tensor([  5.1210, -15.9083])\n",
      "Gradient: tensor([-0.0420,  0.2376])\n",
      "Epoch 1489, Loss 3.098463\n",
      "Params: tensor([  5.1214, -15.9107])\n",
      "Gradient: tensor([-0.0419,  0.2372])\n",
      "Epoch 1490, Loss 3.097883\n",
      "Params: tensor([  5.1219, -15.9130])\n",
      "Gradient: tensor([-0.0418,  0.2368])\n",
      "Epoch 1491, Loss 3.097302\n",
      "Params: tensor([  5.1223, -15.9154])\n",
      "Gradient: tensor([-0.0418,  0.2364])\n",
      "Epoch 1492, Loss 3.096727\n",
      "Params: tensor([  5.1227, -15.9178])\n",
      "Gradient: tensor([-0.0417,  0.2360])\n",
      "Epoch 1493, Loss 3.096153\n",
      "Params: tensor([  5.1231, -15.9201])\n",
      "Gradient: tensor([-0.0416,  0.2356])\n",
      "Epoch 1494, Loss 3.095583\n",
      "Params: tensor([  5.1235, -15.9225])\n",
      "Gradient: tensor([-0.0416,  0.2352])\n",
      "Epoch 1495, Loss 3.095011\n",
      "Params: tensor([  5.1239, -15.9248])\n",
      "Gradient: tensor([-0.0415,  0.2348])\n",
      "Epoch 1496, Loss 3.094444\n",
      "Params: tensor([  5.1244, -15.9272])\n",
      "Gradient: tensor([-0.0414,  0.2344])\n",
      "Epoch 1497, Loss 3.093876\n",
      "Params: tensor([  5.1248, -15.9295])\n",
      "Gradient: tensor([-0.0413,  0.2340])\n",
      "Epoch 1498, Loss 3.093314\n",
      "Params: tensor([  5.1252, -15.9318])\n",
      "Gradient: tensor([-0.0413,  0.2336])\n",
      "Epoch 1499, Loss 3.092751\n",
      "Params: tensor([  5.1256, -15.9342])\n",
      "Gradient: tensor([-0.0412,  0.2332])\n",
      "Epoch 1500, Loss 3.092191\n",
      "Params: tensor([  5.1260, -15.9365])\n",
      "Gradient: tensor([-0.0411,  0.2328])\n",
      "Epoch 1501, Loss 3.091631\n",
      "Params: tensor([  5.1264, -15.9388])\n",
      "Gradient: tensor([-0.0411,  0.2324])\n",
      "Epoch 1502, Loss 3.091074\n",
      "Params: tensor([  5.1268, -15.9411])\n",
      "Gradient: tensor([-0.0410,  0.2320])\n",
      "Epoch 1503, Loss 3.090520\n",
      "Params: tensor([  5.1272, -15.9435])\n",
      "Gradient: tensor([-0.0409,  0.2317])\n",
      "Epoch 1504, Loss 3.089969\n",
      "Params: tensor([  5.1276, -15.9458])\n",
      "Gradient: tensor([-0.0409,  0.2313])\n",
      "Epoch 1505, Loss 3.089417\n",
      "Params: tensor([  5.1281, -15.9481])\n",
      "Gradient: tensor([-0.0408,  0.2309])\n",
      "Epoch 1506, Loss 3.088867\n",
      "Params: tensor([  5.1285, -15.9504])\n",
      "Gradient: tensor([-0.0407,  0.2305])\n",
      "Epoch 1507, Loss 3.088320\n",
      "Params: tensor([  5.1289, -15.9527])\n",
      "Gradient: tensor([-0.0406,  0.2301])\n",
      "Epoch 1508, Loss 3.087775\n",
      "Params: tensor([  5.1293, -15.9550])\n",
      "Gradient: tensor([-0.0406,  0.2297])\n",
      "Epoch 1509, Loss 3.087232\n",
      "Params: tensor([  5.1297, -15.9573])\n",
      "Gradient: tensor([-0.0405,  0.2293])\n",
      "Epoch 1510, Loss 3.086690\n",
      "Params: tensor([  5.1301, -15.9596])\n",
      "Gradient: tensor([-0.0404,  0.2289])\n",
      "Epoch 1511, Loss 3.086150\n",
      "Params: tensor([  5.1305, -15.9618])\n",
      "Gradient: tensor([-0.0404,  0.2285])\n",
      "Epoch 1512, Loss 3.085612\n",
      "Params: tensor([  5.1309, -15.9641])\n",
      "Gradient: tensor([-0.0403,  0.2281])\n",
      "Epoch 1513, Loss 3.085075\n",
      "Params: tensor([  5.1313, -15.9664])\n",
      "Gradient: tensor([-0.0402,  0.2277])\n",
      "Epoch 1514, Loss 3.084542\n",
      "Params: tensor([  5.1317, -15.9687])\n",
      "Gradient: tensor([-0.0402,  0.2274])\n",
      "Epoch 1515, Loss 3.084009\n",
      "Params: tensor([  5.1321, -15.9709])\n",
      "Gradient: tensor([-0.0401,  0.2270])\n",
      "Epoch 1516, Loss 3.083478\n",
      "Params: tensor([  5.1325, -15.9732])\n",
      "Gradient: tensor([-0.0400,  0.2266])\n",
      "Epoch 1517, Loss 3.082948\n",
      "Params: tensor([  5.1329, -15.9755])\n",
      "Gradient: tensor([-0.0400,  0.2262])\n",
      "Epoch 1518, Loss 3.082422\n",
      "Params: tensor([  5.1333, -15.9777])\n",
      "Gradient: tensor([-0.0399,  0.2258])\n",
      "Epoch 1519, Loss 3.081897\n",
      "Params: tensor([  5.1337, -15.9800])\n",
      "Gradient: tensor([-0.0398,  0.2254])\n",
      "Epoch 1520, Loss 3.081373\n",
      "Params: tensor([  5.1341, -15.9822])\n",
      "Gradient: tensor([-0.0398,  0.2250])\n",
      "Epoch 1521, Loss 3.080850\n",
      "Params: tensor([  5.1345, -15.9845])\n",
      "Gradient: tensor([-0.0397,  0.2247])\n",
      "Epoch 1522, Loss 3.080331\n",
      "Params: tensor([  5.1349, -15.9867])\n",
      "Gradient: tensor([-0.0396,  0.2243])\n",
      "Epoch 1523, Loss 3.079811\n",
      "Params: tensor([  5.1353, -15.9890])\n",
      "Gradient: tensor([-0.0396,  0.2239])\n",
      "Epoch 1524, Loss 3.079297\n",
      "Params: tensor([  5.1357, -15.9912])\n",
      "Gradient: tensor([-0.0395,  0.2235])\n",
      "Epoch 1525, Loss 3.078781\n",
      "Params: tensor([  5.1361, -15.9934])\n",
      "Gradient: tensor([-0.0394,  0.2231])\n",
      "Epoch 1526, Loss 3.078268\n",
      "Params: tensor([  5.1365, -15.9957])\n",
      "Gradient: tensor([-0.0394,  0.2228])\n",
      "Epoch 1527, Loss 3.077758\n",
      "Params: tensor([  5.1369, -15.9979])\n",
      "Gradient: tensor([-0.0393,  0.2224])\n",
      "Epoch 1528, Loss 3.077247\n",
      "Params: tensor([  5.1372, -16.0001])\n",
      "Gradient: tensor([-0.0392,  0.2220])\n",
      "Epoch 1529, Loss 3.076739\n",
      "Params: tensor([  5.1376, -16.0023])\n",
      "Gradient: tensor([-0.0391,  0.2216])\n",
      "Epoch 1530, Loss 3.076232\n",
      "Params: tensor([  5.1380, -16.0045])\n",
      "Gradient: tensor([-0.0391,  0.2213])\n",
      "Epoch 1531, Loss 3.075729\n",
      "Params: tensor([  5.1384, -16.0067])\n",
      "Gradient: tensor([-0.0390,  0.2209])\n",
      "Epoch 1532, Loss 3.075225\n",
      "Params: tensor([  5.1388, -16.0089])\n",
      "Gradient: tensor([-0.0390,  0.2205])\n",
      "Epoch 1533, Loss 3.074724\n",
      "Params: tensor([  5.1392, -16.0111])\n",
      "Gradient: tensor([-0.0389,  0.2201])\n",
      "Epoch 1534, Loss 3.074227\n",
      "Params: tensor([  5.1396, -16.0133])\n",
      "Gradient: tensor([-0.0388,  0.2198])\n",
      "Epoch 1535, Loss 3.073726\n",
      "Params: tensor([  5.1400, -16.0155])\n",
      "Gradient: tensor([-0.0387,  0.2194])\n",
      "Epoch 1536, Loss 3.073232\n",
      "Params: tensor([  5.1404, -16.0177])\n",
      "Gradient: tensor([-0.0387,  0.2190])\n",
      "Epoch 1537, Loss 3.072739\n",
      "Params: tensor([  5.1407, -16.0199])\n",
      "Gradient: tensor([-0.0386,  0.2186])\n",
      "Epoch 1538, Loss 3.072245\n",
      "Params: tensor([  5.1411, -16.0221])\n",
      "Gradient: tensor([-0.0385,  0.2183])\n",
      "Epoch 1539, Loss 3.071753\n",
      "Params: tensor([  5.1415, -16.0243])\n",
      "Gradient: tensor([-0.0385,  0.2179])\n",
      "Epoch 1540, Loss 3.071265\n",
      "Params: tensor([  5.1419, -16.0264])\n",
      "Gradient: tensor([-0.0384,  0.2175])\n",
      "Epoch 1541, Loss 3.070778\n",
      "Params: tensor([  5.1423, -16.0286])\n",
      "Gradient: tensor([-0.0383,  0.2172])\n",
      "Epoch 1542, Loss 3.070293\n",
      "Params: tensor([  5.1427, -16.0308])\n",
      "Gradient: tensor([-0.0383,  0.2168])\n",
      "Epoch 1543, Loss 3.069808\n",
      "Params: tensor([  5.1430, -16.0330])\n",
      "Gradient: tensor([-0.0382,  0.2164])\n",
      "Epoch 1544, Loss 3.069326\n",
      "Params: tensor([  5.1434, -16.0351])\n",
      "Gradient: tensor([-0.0382,  0.2161])\n",
      "Epoch 1545, Loss 3.068845\n",
      "Params: tensor([  5.1438, -16.0373])\n",
      "Gradient: tensor([-0.0381,  0.2157])\n",
      "Epoch 1546, Loss 3.068365\n",
      "Params: tensor([  5.1442, -16.0394])\n",
      "Gradient: tensor([-0.0380,  0.2153])\n",
      "Epoch 1547, Loss 3.067887\n",
      "Params: tensor([  5.1446, -16.0416])\n",
      "Gradient: tensor([-0.0380,  0.2150])\n",
      "Epoch 1548, Loss 3.067412\n",
      "Params: tensor([  5.1449, -16.0437])\n",
      "Gradient: tensor([-0.0379,  0.2146])\n",
      "Epoch 1549, Loss 3.066937\n",
      "Params: tensor([  5.1453, -16.0459])\n",
      "Gradient: tensor([-0.0378,  0.2142])\n",
      "Epoch 1550, Loss 3.066463\n",
      "Params: tensor([  5.1457, -16.0480])\n",
      "Gradient: tensor([-0.0378,  0.2139])\n",
      "Epoch 1551, Loss 3.065993\n",
      "Params: tensor([  5.1461, -16.0501])\n",
      "Gradient: tensor([-0.0377,  0.2135])\n",
      "Epoch 1552, Loss 3.065524\n",
      "Params: tensor([  5.1465, -16.0523])\n",
      "Gradient: tensor([-0.0376,  0.2131])\n",
      "Epoch 1553, Loss 3.065055\n",
      "Params: tensor([  5.1468, -16.0544])\n",
      "Gradient: tensor([-0.0376,  0.2128])\n",
      "Epoch 1554, Loss 3.064588\n",
      "Params: tensor([  5.1472, -16.0565])\n",
      "Gradient: tensor([-0.0375,  0.2124])\n",
      "Epoch 1555, Loss 3.064123\n",
      "Params: tensor([  5.1476, -16.0586])\n",
      "Gradient: tensor([-0.0375,  0.2120])\n",
      "Epoch 1556, Loss 3.063660\n",
      "Params: tensor([  5.1480, -16.0608])\n",
      "Gradient: tensor([-0.0374,  0.2117])\n",
      "Epoch 1557, Loss 3.063199\n",
      "Params: tensor([  5.1483, -16.0629])\n",
      "Gradient: tensor([-0.0373,  0.2113])\n",
      "Epoch 1558, Loss 3.062738\n",
      "Params: tensor([  5.1487, -16.0650])\n",
      "Gradient: tensor([-0.0373,  0.2110])\n",
      "Epoch 1559, Loss 3.062280\n",
      "Params: tensor([  5.1491, -16.0671])\n",
      "Gradient: tensor([-0.0372,  0.2106])\n",
      "Epoch 1560, Loss 3.061822\n",
      "Params: tensor([  5.1494, -16.0692])\n",
      "Gradient: tensor([-0.0371,  0.2103])\n",
      "Epoch 1561, Loss 3.061368\n",
      "Params: tensor([  5.1498, -16.0713])\n",
      "Gradient: tensor([-0.0371,  0.2099])\n",
      "Epoch 1562, Loss 3.060913\n",
      "Params: tensor([  5.1502, -16.0734])\n",
      "Gradient: tensor([-0.0370,  0.2095])\n",
      "Epoch 1563, Loss 3.060461\n",
      "Params: tensor([  5.1506, -16.0755])\n",
      "Gradient: tensor([-0.0370,  0.2092])\n",
      "Epoch 1564, Loss 3.060011\n",
      "Params: tensor([  5.1509, -16.0776])\n",
      "Gradient: tensor([-0.0369,  0.2088])\n",
      "Epoch 1565, Loss 3.059561\n",
      "Params: tensor([  5.1513, -16.0796])\n",
      "Gradient: tensor([-0.0368,  0.2085])\n",
      "Epoch 1566, Loss 3.059114\n",
      "Params: tensor([  5.1517, -16.0817])\n",
      "Gradient: tensor([-0.0368,  0.2081])\n",
      "Epoch 1567, Loss 3.058668\n",
      "Params: tensor([  5.1520, -16.0838])\n",
      "Gradient: tensor([-0.0367,  0.2078])\n",
      "Epoch 1568, Loss 3.058221\n",
      "Params: tensor([  5.1524, -16.0859])\n",
      "Gradient: tensor([-0.0366,  0.2074])\n",
      "Epoch 1569, Loss 3.057780\n",
      "Params: tensor([  5.1528, -16.0880])\n",
      "Gradient: tensor([-0.0366,  0.2071])\n",
      "Epoch 1570, Loss 3.057338\n",
      "Params: tensor([  5.1531, -16.0900])\n",
      "Gradient: tensor([-0.0365,  0.2067])\n",
      "Epoch 1571, Loss 3.056898\n",
      "Params: tensor([  5.1535, -16.0921])\n",
      "Gradient: tensor([-0.0364,  0.2064])\n",
      "Epoch 1572, Loss 3.056458\n",
      "Params: tensor([  5.1539, -16.0941])\n",
      "Gradient: tensor([-0.0364,  0.2060])\n",
      "Epoch 1573, Loss 3.056020\n",
      "Params: tensor([  5.1542, -16.0962])\n",
      "Gradient: tensor([-0.0363,  0.2057])\n",
      "Epoch 1574, Loss 3.055585\n",
      "Params: tensor([  5.1546, -16.0983])\n",
      "Gradient: tensor([-0.0363,  0.2053])\n",
      "Epoch 1575, Loss 3.055151\n",
      "Params: tensor([  5.1549, -16.1003])\n",
      "Gradient: tensor([-0.0362,  0.2050])\n",
      "Epoch 1576, Loss 3.054717\n",
      "Params: tensor([  5.1553, -16.1023])\n",
      "Gradient: tensor([-0.0361,  0.2046])\n",
      "Epoch 1577, Loss 3.054286\n",
      "Params: tensor([  5.1557, -16.1044])\n",
      "Gradient: tensor([-0.0361,  0.2043])\n",
      "Epoch 1578, Loss 3.053857\n",
      "Params: tensor([  5.1560, -16.1064])\n",
      "Gradient: tensor([-0.0360,  0.2039])\n",
      "Epoch 1579, Loss 3.053428\n",
      "Params: tensor([  5.1564, -16.1085])\n",
      "Gradient: tensor([-0.0360,  0.2036])\n",
      "Epoch 1580, Loss 3.053001\n",
      "Params: tensor([  5.1567, -16.1105])\n",
      "Gradient: tensor([-0.0359,  0.2032])\n",
      "Epoch 1581, Loss 3.052576\n",
      "Params: tensor([  5.1571, -16.1125])\n",
      "Gradient: tensor([-0.0358,  0.2029])\n",
      "Epoch 1582, Loss 3.052152\n",
      "Params: tensor([  5.1575, -16.1146])\n",
      "Gradient: tensor([-0.0358,  0.2025])\n",
      "Epoch 1583, Loss 3.051730\n",
      "Params: tensor([  5.1578, -16.1166])\n",
      "Gradient: tensor([-0.0357,  0.2022])\n",
      "Epoch 1584, Loss 3.051306\n",
      "Params: tensor([  5.1582, -16.1186])\n",
      "Gradient: tensor([-0.0357,  0.2018])\n",
      "Epoch 1585, Loss 3.050888\n",
      "Params: tensor([  5.1585, -16.1206])\n",
      "Gradient: tensor([-0.0356,  0.2015])\n",
      "Epoch 1586, Loss 3.050471\n",
      "Params: tensor([  5.1589, -16.1226])\n",
      "Gradient: tensor([-0.0355,  0.2012])\n",
      "Epoch 1587, Loss 3.050053\n",
      "Params: tensor([  5.1592, -16.1246])\n",
      "Gradient: tensor([-0.0355,  0.2008])\n",
      "Epoch 1588, Loss 3.049639\n",
      "Params: tensor([  5.1596, -16.1266])\n",
      "Gradient: tensor([-0.0354,  0.2005])\n",
      "Epoch 1589, Loss 3.049223\n",
      "Params: tensor([  5.1599, -16.1286])\n",
      "Gradient: tensor([-0.0354,  0.2001])\n",
      "Epoch 1590, Loss 3.048811\n",
      "Params: tensor([  5.1603, -16.1306])\n",
      "Gradient: tensor([-0.0353,  0.1998])\n",
      "Epoch 1591, Loss 3.048398\n",
      "Params: tensor([  5.1607, -16.1326])\n",
      "Gradient: tensor([-0.0353,  0.1995])\n",
      "Epoch 1592, Loss 3.047991\n",
      "Params: tensor([  5.1610, -16.1346])\n",
      "Gradient: tensor([-0.0352,  0.1991])\n",
      "Epoch 1593, Loss 3.047581\n",
      "Params: tensor([  5.1614, -16.1366])\n",
      "Gradient: tensor([-0.0351,  0.1988])\n",
      "Epoch 1594, Loss 3.047173\n",
      "Params: tensor([  5.1617, -16.1386])\n",
      "Gradient: tensor([-0.0351,  0.1984])\n",
      "Epoch 1595, Loss 3.046768\n",
      "Params: tensor([  5.1621, -16.1406])\n",
      "Gradient: tensor([-0.0350,  0.1981])\n",
      "Epoch 1596, Loss 3.046362\n",
      "Params: tensor([  5.1624, -16.1425])\n",
      "Gradient: tensor([-0.0349,  0.1978])\n",
      "Epoch 1597, Loss 3.045960\n",
      "Params: tensor([  5.1628, -16.1445])\n",
      "Gradient: tensor([-0.0349,  0.1974])\n",
      "Epoch 1598, Loss 3.045559\n",
      "Params: tensor([  5.1631, -16.1465])\n",
      "Gradient: tensor([-0.0348,  0.1971])\n",
      "Epoch 1599, Loss 3.045160\n",
      "Params: tensor([  5.1635, -16.1485])\n",
      "Gradient: tensor([-0.0348,  0.1968])\n",
      "Epoch 1600, Loss 3.044759\n",
      "Params: tensor([  5.1638, -16.1504])\n",
      "Gradient: tensor([-0.0347,  0.1964])\n",
      "Epoch 1601, Loss 3.044361\n",
      "Params: tensor([  5.1641, -16.1524])\n",
      "Gradient: tensor([-0.0346,  0.1961])\n",
      "Epoch 1602, Loss 3.043966\n",
      "Params: tensor([  5.1645, -16.1543])\n",
      "Gradient: tensor([-0.0346,  0.1958])\n",
      "Epoch 1603, Loss 3.043571\n",
      "Params: tensor([  5.1648, -16.1563])\n",
      "Gradient: tensor([-0.0345,  0.1954])\n",
      "Epoch 1604, Loss 3.043176\n",
      "Params: tensor([  5.1652, -16.1582])\n",
      "Gradient: tensor([-0.0345,  0.1951])\n",
      "Epoch 1605, Loss 3.042785\n",
      "Params: tensor([  5.1655, -16.1602])\n",
      "Gradient: tensor([-0.0344,  0.1948])\n",
      "Epoch 1606, Loss 3.042395\n",
      "Params: tensor([  5.1659, -16.1621])\n",
      "Gradient: tensor([-0.0343,  0.1944])\n",
      "Epoch 1607, Loss 3.042004\n",
      "Params: tensor([  5.1662, -16.1641])\n",
      "Gradient: tensor([-0.0343,  0.1941])\n",
      "Epoch 1608, Loss 3.041616\n",
      "Params: tensor([  5.1666, -16.1660])\n",
      "Gradient: tensor([-0.0342,  0.1938])\n",
      "Epoch 1609, Loss 3.041230\n",
      "Params: tensor([  5.1669, -16.1680])\n",
      "Gradient: tensor([-0.0342,  0.1934])\n",
      "Epoch 1610, Loss 3.040844\n",
      "Params: tensor([  5.1672, -16.1699])\n",
      "Gradient: tensor([-0.0341,  0.1931])\n",
      "Epoch 1611, Loss 3.040460\n",
      "Params: tensor([  5.1676, -16.1718])\n",
      "Gradient: tensor([-0.0341,  0.1928])\n",
      "Epoch 1612, Loss 3.040077\n",
      "Params: tensor([  5.1679, -16.1737])\n",
      "Gradient: tensor([-0.0340,  0.1925])\n",
      "Epoch 1613, Loss 3.039695\n",
      "Params: tensor([  5.1683, -16.1757])\n",
      "Gradient: tensor([-0.0339,  0.1921])\n",
      "Epoch 1614, Loss 3.039314\n",
      "Params: tensor([  5.1686, -16.1776])\n",
      "Gradient: tensor([-0.0339,  0.1918])\n",
      "Epoch 1615, Loss 3.038934\n",
      "Params: tensor([  5.1689, -16.1795])\n",
      "Gradient: tensor([-0.0338,  0.1915])\n",
      "Epoch 1616, Loss 3.038557\n",
      "Params: tensor([  5.1693, -16.1814])\n",
      "Gradient: tensor([-0.0338,  0.1912])\n",
      "Epoch 1617, Loss 3.038182\n",
      "Params: tensor([  5.1696, -16.1833])\n",
      "Gradient: tensor([-0.0337,  0.1908])\n",
      "Epoch 1618, Loss 3.037806\n",
      "Params: tensor([  5.1699, -16.1852])\n",
      "Gradient: tensor([-0.0337,  0.1905])\n",
      "Epoch 1619, Loss 3.037432\n",
      "Params: tensor([  5.1703, -16.1871])\n",
      "Gradient: tensor([-0.0336,  0.1902])\n",
      "Epoch 1620, Loss 3.037059\n",
      "Params: tensor([  5.1706, -16.1890])\n",
      "Gradient: tensor([-0.0335,  0.1899])\n",
      "Epoch 1621, Loss 3.036689\n",
      "Params: tensor([  5.1710, -16.1909])\n",
      "Gradient: tensor([-0.0335,  0.1895])\n",
      "Epoch 1622, Loss 3.036319\n",
      "Params: tensor([  5.1713, -16.1928])\n",
      "Gradient: tensor([-0.0334,  0.1892])\n",
      "Epoch 1623, Loss 3.035949\n",
      "Params: tensor([  5.1716, -16.1947])\n",
      "Gradient: tensor([-0.0334,  0.1889])\n",
      "Epoch 1624, Loss 3.035583\n",
      "Params: tensor([  5.1720, -16.1966])\n",
      "Gradient: tensor([-0.0333,  0.1886])\n",
      "Epoch 1625, Loss 3.035215\n",
      "Params: tensor([  5.1723, -16.1985])\n",
      "Gradient: tensor([-0.0333,  0.1883])\n",
      "Epoch 1626, Loss 3.034849\n",
      "Params: tensor([  5.1726, -16.2003])\n",
      "Gradient: tensor([-0.0332,  0.1879])\n",
      "Epoch 1627, Loss 3.034485\n",
      "Params: tensor([  5.1729, -16.2022])\n",
      "Gradient: tensor([-0.0331,  0.1876])\n",
      "Epoch 1628, Loss 3.034122\n",
      "Params: tensor([  5.1733, -16.2041])\n",
      "Gradient: tensor([-0.0331,  0.1873])\n",
      "Epoch 1629, Loss 3.033762\n",
      "Params: tensor([  5.1736, -16.2060])\n",
      "Gradient: tensor([-0.0330,  0.1870])\n",
      "Epoch 1630, Loss 3.033402\n",
      "Params: tensor([  5.1739, -16.2078])\n",
      "Gradient: tensor([-0.0330,  0.1867])\n",
      "Epoch 1631, Loss 3.033042\n",
      "Params: tensor([  5.1743, -16.2097])\n",
      "Gradient: tensor([-0.0329,  0.1863])\n",
      "Epoch 1632, Loss 3.032685\n",
      "Params: tensor([  5.1746, -16.2116])\n",
      "Gradient: tensor([-0.0329,  0.1860])\n",
      "Epoch 1633, Loss 3.032328\n",
      "Params: tensor([  5.1749, -16.2134])\n",
      "Gradient: tensor([-0.0328,  0.1857])\n",
      "Epoch 1634, Loss 3.031973\n",
      "Params: tensor([  5.1753, -16.2153])\n",
      "Gradient: tensor([-0.0327,  0.1854])\n",
      "Epoch 1635, Loss 3.031619\n",
      "Params: tensor([  5.1756, -16.2171])\n",
      "Gradient: tensor([-0.0327,  0.1851])\n",
      "Epoch 1636, Loss 3.031265\n",
      "Params: tensor([  5.1759, -16.2190])\n",
      "Gradient: tensor([-0.0326,  0.1848])\n",
      "Epoch 1637, Loss 3.030913\n",
      "Params: tensor([  5.1762, -16.2208])\n",
      "Gradient: tensor([-0.0326,  0.1845])\n",
      "Epoch 1638, Loss 3.030564\n",
      "Params: tensor([  5.1766, -16.2226])\n",
      "Gradient: tensor([-0.0325,  0.1841])\n",
      "Epoch 1639, Loss 3.030215\n",
      "Params: tensor([  5.1769, -16.2245])\n",
      "Gradient: tensor([-0.0325,  0.1838])\n",
      "Epoch 1640, Loss 3.029867\n",
      "Params: tensor([  5.1772, -16.2263])\n",
      "Gradient: tensor([-0.0324,  0.1835])\n",
      "Epoch 1641, Loss 3.029518\n",
      "Params: tensor([  5.1775, -16.2282])\n",
      "Gradient: tensor([-0.0324,  0.1832])\n",
      "Epoch 1642, Loss 3.029173\n",
      "Params: tensor([  5.1779, -16.2300])\n",
      "Gradient: tensor([-0.0323,  0.1829])\n",
      "Epoch 1643, Loss 3.028829\n",
      "Params: tensor([  5.1782, -16.2318])\n",
      "Gradient: tensor([-0.0323,  0.1826])\n",
      "Epoch 1644, Loss 3.028486\n",
      "Params: tensor([  5.1785, -16.2336])\n",
      "Gradient: tensor([-0.0322,  0.1823])\n",
      "Epoch 1645, Loss 3.028142\n",
      "Params: tensor([  5.1788, -16.2355])\n",
      "Gradient: tensor([-0.0321,  0.1820])\n",
      "Epoch 1646, Loss 3.027802\n",
      "Params: tensor([  5.1791, -16.2373])\n",
      "Gradient: tensor([-0.0321,  0.1817])\n",
      "Epoch 1647, Loss 3.027462\n",
      "Params: tensor([  5.1795, -16.2391])\n",
      "Gradient: tensor([-0.0320,  0.1813])\n",
      "Epoch 1648, Loss 3.027122\n",
      "Params: tensor([  5.1798, -16.2409])\n",
      "Gradient: tensor([-0.0320,  0.1810])\n",
      "Epoch 1649, Loss 3.026784\n",
      "Params: tensor([  5.1801, -16.2427])\n",
      "Gradient: tensor([-0.0319,  0.1807])\n",
      "Epoch 1650, Loss 3.026447\n",
      "Params: tensor([  5.1804, -16.2445])\n",
      "Gradient: tensor([-0.0319,  0.1804])\n",
      "Epoch 1651, Loss 3.026111\n",
      "Params: tensor([  5.1807, -16.2463])\n",
      "Gradient: tensor([-0.0318,  0.1801])\n",
      "Epoch 1652, Loss 3.025780\n",
      "Params: tensor([  5.1811, -16.2481])\n",
      "Gradient: tensor([-0.0318,  0.1798])\n",
      "Epoch 1653, Loss 3.025446\n",
      "Params: tensor([  5.1814, -16.2499])\n",
      "Gradient: tensor([-0.0317,  0.1795])\n",
      "Epoch 1654, Loss 3.025114\n",
      "Params: tensor([  5.1817, -16.2517])\n",
      "Gradient: tensor([-0.0317,  0.1792])\n",
      "Epoch 1655, Loss 3.024782\n",
      "Params: tensor([  5.1820, -16.2535])\n",
      "Gradient: tensor([-0.0316,  0.1789])\n",
      "Epoch 1656, Loss 3.024452\n",
      "Params: tensor([  5.1823, -16.2553])\n",
      "Gradient: tensor([-0.0316,  0.1786])\n",
      "Epoch 1657, Loss 3.024125\n",
      "Params: tensor([  5.1826, -16.2570])\n",
      "Gradient: tensor([-0.0315,  0.1783])\n",
      "Epoch 1658, Loss 3.023797\n",
      "Params: tensor([  5.1829, -16.2588])\n",
      "Gradient: tensor([-0.0315,  0.1780])\n",
      "Epoch 1659, Loss 3.023471\n",
      "Params: tensor([  5.1833, -16.2606])\n",
      "Gradient: tensor([-0.0314,  0.1777])\n",
      "Epoch 1660, Loss 3.023145\n",
      "Params: tensor([  5.1836, -16.2624])\n",
      "Gradient: tensor([-0.0313,  0.1774])\n",
      "Epoch 1661, Loss 3.022821\n",
      "Params: tensor([  5.1839, -16.2641])\n",
      "Gradient: tensor([-0.0313,  0.1771])\n",
      "Epoch 1662, Loss 3.022498\n",
      "Params: tensor([  5.1842, -16.2659])\n",
      "Gradient: tensor([-0.0312,  0.1768])\n",
      "Epoch 1663, Loss 3.022177\n",
      "Params: tensor([  5.1845, -16.2677])\n",
      "Gradient: tensor([-0.0312,  0.1765])\n",
      "Epoch 1664, Loss 3.021855\n",
      "Params: tensor([  5.1848, -16.2694])\n",
      "Gradient: tensor([-0.0311,  0.1762])\n",
      "Epoch 1665, Loss 3.021534\n",
      "Params: tensor([  5.1851, -16.2712])\n",
      "Gradient: tensor([-0.0311,  0.1759])\n",
      "Epoch 1666, Loss 3.021216\n",
      "Params: tensor([  5.1854, -16.2730])\n",
      "Gradient: tensor([-0.0310,  0.1756])\n",
      "Epoch 1667, Loss 3.020898\n",
      "Params: tensor([  5.1858, -16.2747])\n",
      "Gradient: tensor([-0.0310,  0.1753])\n",
      "Epoch 1668, Loss 3.020582\n",
      "Params: tensor([  5.1861, -16.2765])\n",
      "Gradient: tensor([-0.0309,  0.1750])\n",
      "Epoch 1669, Loss 3.020266\n",
      "Params: tensor([  5.1864, -16.2782])\n",
      "Gradient: tensor([-0.0309,  0.1747])\n",
      "Epoch 1670, Loss 3.019952\n",
      "Params: tensor([  5.1867, -16.2800])\n",
      "Gradient: tensor([-0.0308,  0.1744])\n",
      "Epoch 1671, Loss 3.019639\n",
      "Params: tensor([  5.1870, -16.2817])\n",
      "Gradient: tensor([-0.0308,  0.1741])\n",
      "Epoch 1672, Loss 3.019325\n",
      "Params: tensor([  5.1873, -16.2834])\n",
      "Gradient: tensor([-0.0307,  0.1738])\n",
      "Epoch 1673, Loss 3.019017\n",
      "Params: tensor([  5.1876, -16.2852])\n",
      "Gradient: tensor([-0.0307,  0.1735])\n",
      "Epoch 1674, Loss 3.018706\n",
      "Params: tensor([  5.1879, -16.2869])\n",
      "Gradient: tensor([-0.0306,  0.1732])\n",
      "Epoch 1675, Loss 3.018395\n",
      "Params: tensor([  5.1882, -16.2886])\n",
      "Gradient: tensor([-0.0305,  0.1729])\n",
      "Epoch 1676, Loss 3.018089\n",
      "Params: tensor([  5.1885, -16.2904])\n",
      "Gradient: tensor([-0.0305,  0.1726])\n",
      "Epoch 1677, Loss 3.017780\n",
      "Params: tensor([  5.1888, -16.2921])\n",
      "Gradient: tensor([-0.0304,  0.1723])\n",
      "Epoch 1678, Loss 3.017475\n",
      "Params: tensor([  5.1891, -16.2938])\n",
      "Gradient: tensor([-0.0304,  0.1720])\n",
      "Epoch 1679, Loss 3.017170\n",
      "Params: tensor([  5.1894, -16.2955])\n",
      "Gradient: tensor([-0.0303,  0.1717])\n",
      "Epoch 1680, Loss 3.016868\n",
      "Params: tensor([  5.1897, -16.2972])\n",
      "Gradient: tensor([-0.0303,  0.1715])\n",
      "Epoch 1681, Loss 3.016564\n",
      "Params: tensor([  5.1900, -16.2989])\n",
      "Gradient: tensor([-0.0302,  0.1712])\n",
      "Epoch 1682, Loss 3.016262\n",
      "Params: tensor([  5.1903, -16.3006])\n",
      "Gradient: tensor([-0.0302,  0.1709])\n",
      "Epoch 1683, Loss 3.015959\n",
      "Params: tensor([  5.1906, -16.3024])\n",
      "Gradient: tensor([-0.0301,  0.1706])\n",
      "Epoch 1684, Loss 3.015661\n",
      "Params: tensor([  5.1909, -16.3041])\n",
      "Gradient: tensor([-0.0301,  0.1703])\n",
      "Epoch 1685, Loss 3.015362\n",
      "Params: tensor([  5.1912, -16.3058])\n",
      "Gradient: tensor([-0.0300,  0.1700])\n",
      "Epoch 1686, Loss 3.015064\n",
      "Params: tensor([  5.1915, -16.3075])\n",
      "Gradient: tensor([-0.0300,  0.1697])\n",
      "Epoch 1687, Loss 3.014768\n",
      "Params: tensor([  5.1918, -16.3091])\n",
      "Gradient: tensor([-0.0299,  0.1694])\n",
      "Epoch 1688, Loss 3.014472\n",
      "Params: tensor([  5.1921, -16.3108])\n",
      "Gradient: tensor([-0.0299,  0.1691])\n",
      "Epoch 1689, Loss 3.014178\n",
      "Params: tensor([  5.1924, -16.3125])\n",
      "Gradient: tensor([-0.0298,  0.1688])\n",
      "Epoch 1690, Loss 3.013884\n",
      "Params: tensor([  5.1927, -16.3142])\n",
      "Gradient: tensor([-0.0298,  0.1686])\n",
      "Epoch 1691, Loss 3.013591\n",
      "Params: tensor([  5.1930, -16.3159])\n",
      "Gradient: tensor([-0.0297,  0.1683])\n",
      "Epoch 1692, Loss 3.013299\n",
      "Params: tensor([  5.1933, -16.3176])\n",
      "Gradient: tensor([-0.0297,  0.1680])\n",
      "Epoch 1693, Loss 3.013008\n",
      "Params: tensor([  5.1936, -16.3193])\n",
      "Gradient: tensor([-0.0296,  0.1677])\n",
      "Epoch 1694, Loss 3.012719\n",
      "Params: tensor([  5.1939, -16.3209])\n",
      "Gradient: tensor([-0.0296,  0.1674])\n",
      "Epoch 1695, Loss 3.012431\n",
      "Params: tensor([  5.1942, -16.3226])\n",
      "Gradient: tensor([-0.0295,  0.1671])\n",
      "Epoch 1696, Loss 3.012141\n",
      "Params: tensor([  5.1945, -16.3243])\n",
      "Gradient: tensor([-0.0295,  0.1668])\n",
      "Epoch 1697, Loss 3.011855\n",
      "Params: tensor([  5.1948, -16.3259])\n",
      "Gradient: tensor([-0.0294,  0.1666])\n",
      "Epoch 1698, Loss 3.011570\n",
      "Params: tensor([  5.1951, -16.3276])\n",
      "Gradient: tensor([-0.0294,  0.1663])\n",
      "Epoch 1699, Loss 3.011284\n",
      "Params: tensor([  5.1954, -16.3293])\n",
      "Gradient: tensor([-0.0293,  0.1660])\n",
      "Epoch 1700, Loss 3.011001\n",
      "Params: tensor([  5.1957, -16.3309])\n",
      "Gradient: tensor([-0.0293,  0.1657])\n",
      "Epoch 1701, Loss 3.010718\n",
      "Params: tensor([  5.1960, -16.3326])\n",
      "Gradient: tensor([-0.0292,  0.1654])\n",
      "Epoch 1702, Loss 3.010436\n",
      "Params: tensor([  5.1963, -16.3342])\n",
      "Gradient: tensor([-0.0292,  0.1652])\n",
      "Epoch 1703, Loss 3.010156\n",
      "Params: tensor([  5.1966, -16.3359])\n",
      "Gradient: tensor([-0.0291,  0.1649])\n",
      "Epoch 1704, Loss 3.009875\n",
      "Params: tensor([  5.1968, -16.3375])\n",
      "Gradient: tensor([-0.0291,  0.1646])\n",
      "Epoch 1705, Loss 3.009595\n",
      "Params: tensor([  5.1971, -16.3392])\n",
      "Gradient: tensor([-0.0290,  0.1643])\n",
      "Epoch 1706, Loss 3.009319\n",
      "Params: tensor([  5.1974, -16.3408])\n",
      "Gradient: tensor([-0.0290,  0.1640])\n",
      "Epoch 1707, Loss 3.009040\n",
      "Params: tensor([  5.1977, -16.3424])\n",
      "Gradient: tensor([-0.0289,  0.1638])\n",
      "Epoch 1708, Loss 3.008763\n",
      "Params: tensor([  5.1980, -16.3441])\n",
      "Gradient: tensor([-0.0289,  0.1635])\n",
      "Epoch 1709, Loss 3.008487\n",
      "Params: tensor([  5.1983, -16.3457])\n",
      "Gradient: tensor([-0.0288,  0.1632])\n",
      "Epoch 1710, Loss 3.008214\n",
      "Params: tensor([  5.1986, -16.3473])\n",
      "Gradient: tensor([-0.0288,  0.1629])\n",
      "Epoch 1711, Loss 3.007940\n",
      "Params: tensor([  5.1989, -16.3490])\n",
      "Gradient: tensor([-0.0287,  0.1626])\n",
      "Epoch 1712, Loss 3.007668\n",
      "Params: tensor([  5.1992, -16.3506])\n",
      "Gradient: tensor([-0.0287,  0.1624])\n",
      "Epoch 1713, Loss 3.007396\n",
      "Params: tensor([  5.1994, -16.3522])\n",
      "Gradient: tensor([-0.0286,  0.1621])\n",
      "Epoch 1714, Loss 3.007126\n",
      "Params: tensor([  5.1997, -16.3538])\n",
      "Gradient: tensor([-0.0286,  0.1618])\n",
      "Epoch 1715, Loss 3.006857\n",
      "Params: tensor([  5.2000, -16.3554])\n",
      "Gradient: tensor([-0.0285,  0.1615])\n",
      "Epoch 1716, Loss 3.006587\n",
      "Params: tensor([  5.2003, -16.3570])\n",
      "Gradient: tensor([-0.0285,  0.1613])\n",
      "Epoch 1717, Loss 3.006318\n",
      "Params: tensor([  5.2006, -16.3587])\n",
      "Gradient: tensor([-0.0284,  0.1610])\n",
      "Epoch 1718, Loss 3.006052\n",
      "Params: tensor([  5.2009, -16.3603])\n",
      "Gradient: tensor([-0.0284,  0.1607])\n",
      "Epoch 1719, Loss 3.005785\n",
      "Params: tensor([  5.2012, -16.3619])\n",
      "Gradient: tensor([-0.0284,  0.1604])\n",
      "Epoch 1720, Loss 3.005520\n",
      "Params: tensor([  5.2014, -16.3635])\n",
      "Gradient: tensor([-0.0283,  0.1602])\n",
      "Epoch 1721, Loss 3.005256\n",
      "Params: tensor([  5.2017, -16.3651])\n",
      "Gradient: tensor([-0.0283,  0.1599])\n",
      "Epoch 1722, Loss 3.004993\n",
      "Params: tensor([  5.2020, -16.3667])\n",
      "Gradient: tensor([-0.0282,  0.1596])\n",
      "Epoch 1723, Loss 3.004729\n",
      "Params: tensor([  5.2023, -16.3683])\n",
      "Gradient: tensor([-0.0281,  0.1594])\n",
      "Epoch 1724, Loss 3.004467\n",
      "Params: tensor([  5.2026, -16.3699])\n",
      "Gradient: tensor([-0.0281,  0.1591])\n",
      "Epoch 1725, Loss 3.004207\n",
      "Params: tensor([  5.2028, -16.3714])\n",
      "Gradient: tensor([-0.0280,  0.1588])\n",
      "Epoch 1726, Loss 3.003947\n",
      "Params: tensor([  5.2031, -16.3730])\n",
      "Gradient: tensor([-0.0280,  0.1586])\n",
      "Epoch 1727, Loss 3.003690\n",
      "Params: tensor([  5.2034, -16.3746])\n",
      "Gradient: tensor([-0.0280,  0.1583])\n",
      "Epoch 1728, Loss 3.003430\n",
      "Params: tensor([  5.2037, -16.3762])\n",
      "Gradient: tensor([-0.0279,  0.1580])\n",
      "Epoch 1729, Loss 3.003173\n",
      "Params: tensor([  5.2040, -16.3778])\n",
      "Gradient: tensor([-0.0279,  0.1577])\n",
      "Epoch 1730, Loss 3.002918\n",
      "Params: tensor([  5.2042, -16.3793])\n",
      "Gradient: tensor([-0.0278,  0.1575])\n",
      "Epoch 1731, Loss 3.002661\n",
      "Params: tensor([  5.2045, -16.3809])\n",
      "Gradient: tensor([-0.0278,  0.1572])\n",
      "Epoch 1732, Loss 3.002406\n",
      "Params: tensor([  5.2048, -16.3825])\n",
      "Gradient: tensor([-0.0277,  0.1569])\n",
      "Epoch 1733, Loss 3.002152\n",
      "Params: tensor([  5.2051, -16.3840])\n",
      "Gradient: tensor([-0.0277,  0.1567])\n",
      "Epoch 1734, Loss 3.001901\n",
      "Params: tensor([  5.2053, -16.3856])\n",
      "Gradient: tensor([-0.0276,  0.1564])\n",
      "Epoch 1735, Loss 3.001649\n",
      "Params: tensor([  5.2056, -16.3872])\n",
      "Gradient: tensor([-0.0276,  0.1561])\n",
      "Epoch 1736, Loss 3.001395\n",
      "Params: tensor([  5.2059, -16.3887])\n",
      "Gradient: tensor([-0.0275,  0.1559])\n",
      "Epoch 1737, Loss 3.001145\n",
      "Params: tensor([  5.2062, -16.3903])\n",
      "Gradient: tensor([-0.0275,  0.1556])\n",
      "Epoch 1738, Loss 3.000897\n",
      "Params: tensor([  5.2064, -16.3918])\n",
      "Gradient: tensor([-0.0274,  0.1553])\n",
      "Epoch 1739, Loss 3.000648\n",
      "Params: tensor([  5.2067, -16.3934])\n",
      "Gradient: tensor([-0.0274,  0.1551])\n",
      "Epoch 1740, Loss 3.000400\n",
      "Params: tensor([  5.2070, -16.3949])\n",
      "Gradient: tensor([-0.0273,  0.1548])\n",
      "Epoch 1741, Loss 3.000154\n",
      "Params: tensor([  5.2073, -16.3965])\n",
      "Gradient: tensor([-0.0273,  0.1546])\n",
      "Epoch 1742, Loss 2.999907\n",
      "Params: tensor([  5.2075, -16.3980])\n",
      "Gradient: tensor([-0.0273,  0.1543])\n",
      "Epoch 1743, Loss 2.999663\n",
      "Params: tensor([  5.2078, -16.3996])\n",
      "Gradient: tensor([-0.0272,  0.1540])\n",
      "Epoch 1744, Loss 2.999417\n",
      "Params: tensor([  5.2081, -16.4011])\n",
      "Gradient: tensor([-0.0272,  0.1538])\n",
      "Epoch 1745, Loss 2.999174\n",
      "Params: tensor([  5.2084, -16.4026])\n",
      "Gradient: tensor([-0.0271,  0.1535])\n",
      "Epoch 1746, Loss 2.998930\n",
      "Params: tensor([  5.2086, -16.4042])\n",
      "Gradient: tensor([-0.0271,  0.1533])\n",
      "Epoch 1747, Loss 2.998688\n",
      "Params: tensor([  5.2089, -16.4057])\n",
      "Gradient: tensor([-0.0270,  0.1530])\n",
      "Epoch 1748, Loss 2.998448\n",
      "Params: tensor([  5.2092, -16.4072])\n",
      "Gradient: tensor([-0.0270,  0.1527])\n",
      "Epoch 1749, Loss 2.998209\n",
      "Params: tensor([  5.2094, -16.4088])\n",
      "Gradient: tensor([-0.0269,  0.1525])\n",
      "Epoch 1750, Loss 2.997968\n",
      "Params: tensor([  5.2097, -16.4103])\n",
      "Gradient: tensor([-0.0269,  0.1522])\n",
      "Epoch 1751, Loss 2.997730\n",
      "Params: tensor([  5.2100, -16.4118])\n",
      "Gradient: tensor([-0.0268,  0.1520])\n",
      "Epoch 1752, Loss 2.997490\n",
      "Params: tensor([  5.2102, -16.4133])\n",
      "Gradient: tensor([-0.0268,  0.1517])\n",
      "Epoch 1753, Loss 2.997254\n",
      "Params: tensor([  5.2105, -16.4148])\n",
      "Gradient: tensor([-0.0267,  0.1514])\n",
      "Epoch 1754, Loss 2.997018\n",
      "Params: tensor([  5.2108, -16.4163])\n",
      "Gradient: tensor([-0.0267,  0.1512])\n",
      "Epoch 1755, Loss 2.996783\n",
      "Params: tensor([  5.2110, -16.4179])\n",
      "Gradient: tensor([-0.0266,  0.1509])\n",
      "Epoch 1756, Loss 2.996548\n",
      "Params: tensor([  5.2113, -16.4194])\n",
      "Gradient: tensor([-0.0266,  0.1507])\n",
      "Epoch 1757, Loss 2.996313\n",
      "Params: tensor([  5.2116, -16.4209])\n",
      "Gradient: tensor([-0.0266,  0.1504])\n",
      "Epoch 1758, Loss 2.996081\n",
      "Params: tensor([  5.2118, -16.4224])\n",
      "Gradient: tensor([-0.0265,  0.1502])\n",
      "Epoch 1759, Loss 2.995847\n",
      "Params: tensor([  5.2121, -16.4239])\n",
      "Gradient: tensor([-0.0265,  0.1499])\n",
      "Epoch 1760, Loss 2.995616\n",
      "Params: tensor([  5.2124, -16.4254])\n",
      "Gradient: tensor([-0.0264,  0.1496])\n",
      "Epoch 1761, Loss 2.995387\n",
      "Params: tensor([  5.2126, -16.4269])\n",
      "Gradient: tensor([-0.0264,  0.1494])\n",
      "Epoch 1762, Loss 2.995156\n",
      "Params: tensor([  5.2129, -16.4283])\n",
      "Gradient: tensor([-0.0263,  0.1491])\n",
      "Epoch 1763, Loss 2.994929\n",
      "Params: tensor([  5.2132, -16.4298])\n",
      "Gradient: tensor([-0.0263,  0.1489])\n",
      "Epoch 1764, Loss 2.994699\n",
      "Params: tensor([  5.2134, -16.4313])\n",
      "Gradient: tensor([-0.0263,  0.1486])\n",
      "Epoch 1765, Loss 2.994471\n",
      "Params: tensor([  5.2137, -16.4328])\n",
      "Gradient: tensor([-0.0262,  0.1484])\n",
      "Epoch 1766, Loss 2.994245\n",
      "Params: tensor([  5.2139, -16.4343])\n",
      "Gradient: tensor([-0.0262,  0.1481])\n",
      "Epoch 1767, Loss 2.994019\n",
      "Params: tensor([  5.2142, -16.4358])\n",
      "Gradient: tensor([-0.0261,  0.1479])\n",
      "Epoch 1768, Loss 2.993794\n",
      "Params: tensor([  5.2145, -16.4372])\n",
      "Gradient: tensor([-0.0261,  0.1476])\n",
      "Epoch 1769, Loss 2.993569\n",
      "Params: tensor([  5.2147, -16.4387])\n",
      "Gradient: tensor([-0.0260,  0.1474])\n",
      "Epoch 1770, Loss 2.993344\n",
      "Params: tensor([  5.2150, -16.4402])\n",
      "Gradient: tensor([-0.0260,  0.1471])\n",
      "Epoch 1771, Loss 2.993121\n",
      "Params: tensor([  5.2152, -16.4417])\n",
      "Gradient: tensor([-0.0260,  0.1469])\n",
      "Epoch 1772, Loss 2.992900\n",
      "Params: tensor([  5.2155, -16.4431])\n",
      "Gradient: tensor([-0.0259,  0.1466])\n",
      "Epoch 1773, Loss 2.992678\n",
      "Params: tensor([  5.2158, -16.4446])\n",
      "Gradient: tensor([-0.0259,  0.1464])\n",
      "Epoch 1774, Loss 2.992457\n",
      "Params: tensor([  5.2160, -16.4460])\n",
      "Gradient: tensor([-0.0258,  0.1461])\n",
      "Epoch 1775, Loss 2.992238\n",
      "Params: tensor([  5.2163, -16.4475])\n",
      "Gradient: tensor([-0.0258,  0.1459])\n",
      "Epoch 1776, Loss 2.992017\n",
      "Params: tensor([  5.2165, -16.4490])\n",
      "Gradient: tensor([-0.0257,  0.1456])\n",
      "Epoch 1777, Loss 2.991798\n",
      "Params: tensor([  5.2168, -16.4504])\n",
      "Gradient: tensor([-0.0257,  0.1454])\n",
      "Epoch 1778, Loss 2.991583\n",
      "Params: tensor([  5.2170, -16.4519])\n",
      "Gradient: tensor([-0.0256,  0.1451])\n",
      "Epoch 1779, Loss 2.991366\n",
      "Params: tensor([  5.2173, -16.4533])\n",
      "Gradient: tensor([-0.0256,  0.1449])\n",
      "Epoch 1780, Loss 2.991146\n",
      "Params: tensor([  5.2176, -16.4548])\n",
      "Gradient: tensor([-0.0256,  0.1446])\n",
      "Epoch 1781, Loss 2.990932\n",
      "Params: tensor([  5.2178, -16.4562])\n",
      "Gradient: tensor([-0.0255,  0.1444])\n",
      "Epoch 1782, Loss 2.990719\n",
      "Params: tensor([  5.2181, -16.4576])\n",
      "Gradient: tensor([-0.0255,  0.1442])\n",
      "Epoch 1783, Loss 2.990503\n",
      "Params: tensor([  5.2183, -16.4591])\n",
      "Gradient: tensor([-0.0254,  0.1439])\n",
      "Epoch 1784, Loss 2.990289\n",
      "Params: tensor([  5.2186, -16.4605])\n",
      "Gradient: tensor([-0.0254,  0.1437])\n",
      "Epoch 1785, Loss 2.990078\n",
      "Params: tensor([  5.2188, -16.4620])\n",
      "Gradient: tensor([-0.0253,  0.1434])\n",
      "Epoch 1786, Loss 2.989866\n",
      "Params: tensor([  5.2191, -16.4634])\n",
      "Gradient: tensor([-0.0253,  0.1432])\n",
      "Epoch 1787, Loss 2.989655\n",
      "Params: tensor([  5.2193, -16.4648])\n",
      "Gradient: tensor([-0.0252,  0.1429])\n",
      "Epoch 1788, Loss 2.989443\n",
      "Params: tensor([  5.2196, -16.4662])\n",
      "Gradient: tensor([-0.0252,  0.1427])\n",
      "Epoch 1789, Loss 2.989233\n",
      "Params: tensor([  5.2198, -16.4677])\n",
      "Gradient: tensor([-0.0252,  0.1424])\n",
      "Epoch 1790, Loss 2.989025\n",
      "Params: tensor([  5.2201, -16.4691])\n",
      "Gradient: tensor([-0.0251,  0.1422])\n",
      "Epoch 1791, Loss 2.988817\n",
      "Params: tensor([  5.2203, -16.4705])\n",
      "Gradient: tensor([-0.0251,  0.1420])\n",
      "Epoch 1792, Loss 2.988609\n",
      "Params: tensor([  5.2206, -16.4719])\n",
      "Gradient: tensor([-0.0250,  0.1417])\n",
      "Epoch 1793, Loss 2.988401\n",
      "Params: tensor([  5.2208, -16.4733])\n",
      "Gradient: tensor([-0.0250,  0.1415])\n",
      "Epoch 1794, Loss 2.988195\n",
      "Params: tensor([  5.2211, -16.4748])\n",
      "Gradient: tensor([-0.0249,  0.1412])\n",
      "Epoch 1795, Loss 2.987989\n",
      "Params: tensor([  5.2213, -16.4762])\n",
      "Gradient: tensor([-0.0249,  0.1410])\n",
      "Epoch 1796, Loss 2.987785\n",
      "Params: tensor([  5.2216, -16.4776])\n",
      "Gradient: tensor([-0.0249,  0.1408])\n",
      "Epoch 1797, Loss 2.987582\n",
      "Params: tensor([  5.2218, -16.4790])\n",
      "Gradient: tensor([-0.0248,  0.1405])\n",
      "Epoch 1798, Loss 2.987377\n",
      "Params: tensor([  5.2221, -16.4804])\n",
      "Gradient: tensor([-0.0248,  0.1403])\n",
      "Epoch 1799, Loss 2.987174\n",
      "Params: tensor([  5.2223, -16.4818])\n",
      "Gradient: tensor([-0.0247,  0.1400])\n",
      "Epoch 1800, Loss 2.986974\n",
      "Params: tensor([  5.2226, -16.4832])\n",
      "Gradient: tensor([-0.0247,  0.1398])\n",
      "Epoch 1801, Loss 2.986771\n",
      "Params: tensor([  5.2228, -16.4846])\n",
      "Gradient: tensor([-0.0246,  0.1396])\n",
      "Epoch 1802, Loss 2.986570\n",
      "Params: tensor([  5.2231, -16.4860])\n",
      "Gradient: tensor([-0.0246,  0.1393])\n",
      "Epoch 1803, Loss 2.986371\n",
      "Params: tensor([  5.2233, -16.4874])\n",
      "Gradient: tensor([-0.0246,  0.1391])\n",
      "Epoch 1804, Loss 2.986171\n",
      "Params: tensor([  5.2236, -16.4888])\n",
      "Gradient: tensor([-0.0245,  0.1389])\n",
      "Epoch 1805, Loss 2.985971\n",
      "Params: tensor([  5.2238, -16.4901])\n",
      "Gradient: tensor([-0.0245,  0.1386])\n",
      "Epoch 1806, Loss 2.985774\n",
      "Params: tensor([  5.2241, -16.4915])\n",
      "Gradient: tensor([-0.0245,  0.1384])\n",
      "Epoch 1807, Loss 2.985578\n",
      "Params: tensor([  5.2243, -16.4929])\n",
      "Gradient: tensor([-0.0244,  0.1382])\n",
      "Epoch 1808, Loss 2.985381\n",
      "Params: tensor([  5.2245, -16.4943])\n",
      "Gradient: tensor([-0.0244,  0.1379])\n",
      "Epoch 1809, Loss 2.985184\n",
      "Params: tensor([  5.2248, -16.4957])\n",
      "Gradient: tensor([-0.0243,  0.1377])\n",
      "Epoch 1810, Loss 2.984989\n",
      "Params: tensor([  5.2250, -16.4970])\n",
      "Gradient: tensor([-0.0243,  0.1374])\n",
      "Epoch 1811, Loss 2.984793\n",
      "Params: tensor([  5.2253, -16.4984])\n",
      "Gradient: tensor([-0.0243,  0.1372])\n",
      "Epoch 1812, Loss 2.984601\n",
      "Params: tensor([  5.2255, -16.4998])\n",
      "Gradient: tensor([-0.0242,  0.1370])\n",
      "Epoch 1813, Loss 2.984407\n",
      "Params: tensor([  5.2258, -16.5011])\n",
      "Gradient: tensor([-0.0242,  0.1368])\n",
      "Epoch 1814, Loss 2.984215\n",
      "Params: tensor([  5.2260, -16.5025])\n",
      "Gradient: tensor([-0.0241,  0.1365])\n",
      "Epoch 1815, Loss 2.984022\n",
      "Params: tensor([  5.2262, -16.5039])\n",
      "Gradient: tensor([-0.0241,  0.1363])\n",
      "Epoch 1816, Loss 2.983831\n",
      "Params: tensor([  5.2265, -16.5052])\n",
      "Gradient: tensor([-0.0240,  0.1361])\n",
      "Epoch 1817, Loss 2.983639\n",
      "Params: tensor([  5.2267, -16.5066])\n",
      "Gradient: tensor([-0.0240,  0.1358])\n",
      "Epoch 1818, Loss 2.983449\n",
      "Params: tensor([  5.2270, -16.5079])\n",
      "Gradient: tensor([-0.0239,  0.1356])\n",
      "Epoch 1819, Loss 2.983260\n",
      "Params: tensor([  5.2272, -16.5093])\n",
      "Gradient: tensor([-0.0239,  0.1354])\n",
      "Epoch 1820, Loss 2.983073\n",
      "Params: tensor([  5.2274, -16.5107])\n",
      "Gradient: tensor([-0.0239,  0.1351])\n",
      "Epoch 1821, Loss 2.982884\n",
      "Params: tensor([  5.2277, -16.5120])\n",
      "Gradient: tensor([-0.0238,  0.1349])\n",
      "Epoch 1822, Loss 2.982697\n",
      "Params: tensor([  5.2279, -16.5133])\n",
      "Gradient: tensor([-0.0238,  0.1347])\n",
      "Epoch 1823, Loss 2.982510\n",
      "Params: tensor([  5.2281, -16.5147])\n",
      "Gradient: tensor([-0.0237,  0.1344])\n",
      "Epoch 1824, Loss 2.982322\n",
      "Params: tensor([  5.2284, -16.5160])\n",
      "Gradient: tensor([-0.0237,  0.1342])\n",
      "Epoch 1825, Loss 2.982137\n",
      "Params: tensor([  5.2286, -16.5174])\n",
      "Gradient: tensor([-0.0237,  0.1340])\n",
      "Epoch 1826, Loss 2.981953\n",
      "Params: tensor([  5.2289, -16.5187])\n",
      "Gradient: tensor([-0.0236,  0.1338])\n",
      "Epoch 1827, Loss 2.981769\n",
      "Params: tensor([  5.2291, -16.5200])\n",
      "Gradient: tensor([-0.0236,  0.1335])\n",
      "Epoch 1828, Loss 2.981586\n",
      "Params: tensor([  5.2293, -16.5214])\n",
      "Gradient: tensor([-0.0236,  0.1333])\n",
      "Epoch 1829, Loss 2.981402\n",
      "Params: tensor([  5.2296, -16.5227])\n",
      "Gradient: tensor([-0.0235,  0.1331])\n",
      "Epoch 1830, Loss 2.981219\n",
      "Params: tensor([  5.2298, -16.5240])\n",
      "Gradient: tensor([-0.0235,  0.1329])\n",
      "Epoch 1831, Loss 2.981037\n",
      "Params: tensor([  5.2300, -16.5254])\n",
      "Gradient: tensor([-0.0235,  0.1326])\n",
      "Epoch 1832, Loss 2.980856\n",
      "Params: tensor([  5.2303, -16.5267])\n",
      "Gradient: tensor([-0.0234,  0.1324])\n",
      "Epoch 1833, Loss 2.980675\n",
      "Params: tensor([  5.2305, -16.5280])\n",
      "Gradient: tensor([-0.0234,  0.1322])\n",
      "Epoch 1834, Loss 2.980495\n",
      "Params: tensor([  5.2307, -16.5293])\n",
      "Gradient: tensor([-0.0233,  0.1320])\n",
      "Epoch 1835, Loss 2.980315\n",
      "Params: tensor([  5.2310, -16.5306])\n",
      "Gradient: tensor([-0.0233,  0.1317])\n",
      "Epoch 1836, Loss 2.980137\n",
      "Params: tensor([  5.2312, -16.5320])\n",
      "Gradient: tensor([-0.0232,  0.1315])\n",
      "Epoch 1837, Loss 2.979958\n",
      "Params: tensor([  5.2314, -16.5333])\n",
      "Gradient: tensor([-0.0232,  0.1313])\n",
      "Epoch 1838, Loss 2.979782\n",
      "Params: tensor([  5.2317, -16.5346])\n",
      "Gradient: tensor([-0.0232,  0.1311])\n",
      "Epoch 1839, Loss 2.979604\n",
      "Params: tensor([  5.2319, -16.5359])\n",
      "Gradient: tensor([-0.0231,  0.1308])\n",
      "Epoch 1840, Loss 2.979429\n",
      "Params: tensor([  5.2321, -16.5372])\n",
      "Gradient: tensor([-0.0231,  0.1306])\n",
      "Epoch 1841, Loss 2.979253\n",
      "Params: tensor([  5.2324, -16.5385])\n",
      "Gradient: tensor([-0.0230,  0.1304])\n",
      "Epoch 1842, Loss 2.979077\n",
      "Params: tensor([  5.2326, -16.5398])\n",
      "Gradient: tensor([-0.0230,  0.1302])\n",
      "Epoch 1843, Loss 2.978902\n",
      "Params: tensor([  5.2328, -16.5411])\n",
      "Gradient: tensor([-0.0229,  0.1300])\n",
      "Epoch 1844, Loss 2.978729\n",
      "Params: tensor([  5.2330, -16.5424])\n",
      "Gradient: tensor([-0.0229,  0.1297])\n",
      "Epoch 1845, Loss 2.978555\n",
      "Params: tensor([  5.2333, -16.5437])\n",
      "Gradient: tensor([-0.0229,  0.1295])\n",
      "Epoch 1846, Loss 2.978382\n",
      "Params: tensor([  5.2335, -16.5450])\n",
      "Gradient: tensor([-0.0228,  0.1293])\n",
      "Epoch 1847, Loss 2.978210\n",
      "Params: tensor([  5.2337, -16.5463])\n",
      "Gradient: tensor([-0.0228,  0.1291])\n",
      "Epoch 1848, Loss 2.978038\n",
      "Params: tensor([  5.2340, -16.5476])\n",
      "Gradient: tensor([-0.0228,  0.1288])\n",
      "Epoch 1849, Loss 2.977867\n",
      "Params: tensor([  5.2342, -16.5489])\n",
      "Gradient: tensor([-0.0227,  0.1286])\n",
      "Epoch 1850, Loss 2.977696\n",
      "Params: tensor([  5.2344, -16.5501])\n",
      "Gradient: tensor([-0.0227,  0.1284])\n",
      "Epoch 1851, Loss 2.977527\n",
      "Params: tensor([  5.2346, -16.5514])\n",
      "Gradient: tensor([-0.0227,  0.1282])\n",
      "Epoch 1852, Loss 2.977357\n",
      "Params: tensor([  5.2349, -16.5527])\n",
      "Gradient: tensor([-0.0226,  0.1280])\n",
      "Epoch 1853, Loss 2.977188\n",
      "Params: tensor([  5.2351, -16.5540])\n",
      "Gradient: tensor([-0.0226,  0.1278])\n",
      "Epoch 1854, Loss 2.977021\n",
      "Params: tensor([  5.2353, -16.5553])\n",
      "Gradient: tensor([-0.0225,  0.1275])\n",
      "Epoch 1855, Loss 2.976853\n",
      "Params: tensor([  5.2355, -16.5565])\n",
      "Gradient: tensor([-0.0225,  0.1273])\n",
      "Epoch 1856, Loss 2.976686\n",
      "Params: tensor([  5.2358, -16.5578])\n",
      "Gradient: tensor([-0.0225,  0.1271])\n",
      "Epoch 1857, Loss 2.976520\n",
      "Params: tensor([  5.2360, -16.5591])\n",
      "Gradient: tensor([-0.0224,  0.1269])\n",
      "Epoch 1858, Loss 2.976354\n",
      "Params: tensor([  5.2362, -16.5603])\n",
      "Gradient: tensor([-0.0224,  0.1267])\n",
      "Epoch 1859, Loss 2.976189\n",
      "Params: tensor([  5.2364, -16.5616])\n",
      "Gradient: tensor([-0.0223,  0.1265])\n",
      "Epoch 1860, Loss 2.976023\n",
      "Params: tensor([  5.2367, -16.5629])\n",
      "Gradient: tensor([-0.0223,  0.1263])\n",
      "Epoch 1861, Loss 2.975860\n",
      "Params: tensor([  5.2369, -16.5641])\n",
      "Gradient: tensor([-0.0223,  0.1260])\n",
      "Epoch 1862, Loss 2.975696\n",
      "Params: tensor([  5.2371, -16.5654])\n",
      "Gradient: tensor([-0.0222,  0.1258])\n",
      "Epoch 1863, Loss 2.975533\n",
      "Params: tensor([  5.2373, -16.5666])\n",
      "Gradient: tensor([-0.0222,  0.1256])\n",
      "Epoch 1864, Loss 2.975369\n",
      "Params: tensor([  5.2375, -16.5679])\n",
      "Gradient: tensor([-0.0222,  0.1254])\n",
      "Epoch 1865, Loss 2.975208\n",
      "Params: tensor([  5.2378, -16.5691])\n",
      "Gradient: tensor([-0.0221,  0.1252])\n",
      "Epoch 1866, Loss 2.975046\n",
      "Params: tensor([  5.2380, -16.5704])\n",
      "Gradient: tensor([-0.0221,  0.1250])\n",
      "Epoch 1867, Loss 2.974886\n",
      "Params: tensor([  5.2382, -16.5716])\n",
      "Gradient: tensor([-0.0220,  0.1248])\n",
      "Epoch 1868, Loss 2.974725\n",
      "Params: tensor([  5.2384, -16.5729])\n",
      "Gradient: tensor([-0.0220,  0.1245])\n",
      "Epoch 1869, Loss 2.974565\n",
      "Params: tensor([  5.2386, -16.5741])\n",
      "Gradient: tensor([-0.0220,  0.1243])\n",
      "Epoch 1870, Loss 2.974406\n",
      "Params: tensor([  5.2389, -16.5754])\n",
      "Gradient: tensor([-0.0219,  0.1241])\n",
      "Epoch 1871, Loss 2.974248\n",
      "Params: tensor([  5.2391, -16.5766])\n",
      "Gradient: tensor([-0.0219,  0.1239])\n",
      "Epoch 1872, Loss 2.974088\n",
      "Params: tensor([  5.2393, -16.5778])\n",
      "Gradient: tensor([-0.0219,  0.1237])\n",
      "Epoch 1873, Loss 2.973930\n",
      "Params: tensor([  5.2395, -16.5791])\n",
      "Gradient: tensor([-0.0218,  0.1235])\n",
      "Epoch 1874, Loss 2.973776\n",
      "Params: tensor([  5.2397, -16.5803])\n",
      "Gradient: tensor([-0.0218,  0.1233])\n",
      "Epoch 1875, Loss 2.973618\n",
      "Params: tensor([  5.2400, -16.5815])\n",
      "Gradient: tensor([-0.0217,  0.1231])\n",
      "Epoch 1876, Loss 2.973463\n",
      "Params: tensor([  5.2402, -16.5828])\n",
      "Gradient: tensor([-0.0217,  0.1229])\n",
      "Epoch 1877, Loss 2.973307\n",
      "Params: tensor([  5.2404, -16.5840])\n",
      "Gradient: tensor([-0.0217,  0.1227])\n",
      "Epoch 1878, Loss 2.973150\n",
      "Params: tensor([  5.2406, -16.5852])\n",
      "Gradient: tensor([-0.0216,  0.1224])\n",
      "Epoch 1879, Loss 2.972996\n",
      "Params: tensor([  5.2408, -16.5864])\n",
      "Gradient: tensor([-0.0216,  0.1222])\n",
      "Epoch 1880, Loss 2.972844\n",
      "Params: tensor([  5.2410, -16.5877])\n",
      "Gradient: tensor([-0.0215,  0.1220])\n",
      "Epoch 1881, Loss 2.972690\n",
      "Params: tensor([  5.2413, -16.5889])\n",
      "Gradient: tensor([-0.0215,  0.1218])\n",
      "Epoch 1882, Loss 2.972536\n",
      "Params: tensor([  5.2415, -16.5901])\n",
      "Gradient: tensor([-0.0215,  0.1216])\n",
      "Epoch 1883, Loss 2.972384\n",
      "Params: tensor([  5.2417, -16.5913])\n",
      "Gradient: tensor([-0.0214,  0.1214])\n",
      "Epoch 1884, Loss 2.972232\n",
      "Params: tensor([  5.2419, -16.5925])\n",
      "Gradient: tensor([-0.0214,  0.1212])\n",
      "Epoch 1885, Loss 2.972081\n",
      "Params: tensor([  5.2421, -16.5937])\n",
      "Gradient: tensor([-0.0214,  0.1210])\n",
      "Epoch 1886, Loss 2.971931\n",
      "Params: tensor([  5.2423, -16.5949])\n",
      "Gradient: tensor([-0.0213,  0.1208])\n",
      "Epoch 1887, Loss 2.971780\n",
      "Params: tensor([  5.2425, -16.5961])\n",
      "Gradient: tensor([-0.0213,  0.1206])\n",
      "Epoch 1888, Loss 2.971630\n",
      "Params: tensor([  5.2427, -16.5974])\n",
      "Gradient: tensor([-0.0213,  0.1204])\n",
      "Epoch 1889, Loss 2.971481\n",
      "Params: tensor([  5.2430, -16.5986])\n",
      "Gradient: tensor([-0.0212,  0.1202])\n",
      "Epoch 1890, Loss 2.971332\n",
      "Params: tensor([  5.2432, -16.5998])\n",
      "Gradient: tensor([-0.0212,  0.1200])\n",
      "Epoch 1891, Loss 2.971185\n",
      "Params: tensor([  5.2434, -16.6010])\n",
      "Gradient: tensor([-0.0212,  0.1198])\n",
      "Epoch 1892, Loss 2.971035\n",
      "Params: tensor([  5.2436, -16.6021])\n",
      "Gradient: tensor([-0.0211,  0.1196])\n",
      "Epoch 1893, Loss 2.970888\n",
      "Params: tensor([  5.2438, -16.6033])\n",
      "Gradient: tensor([-0.0211,  0.1194])\n",
      "Epoch 1894, Loss 2.970741\n",
      "Params: tensor([  5.2440, -16.6045])\n",
      "Gradient: tensor([-0.0211,  0.1192])\n",
      "Epoch 1895, Loss 2.970596\n",
      "Params: tensor([  5.2442, -16.6057])\n",
      "Gradient: tensor([-0.0210,  0.1190])\n",
      "Epoch 1896, Loss 2.970449\n",
      "Params: tensor([  5.2444, -16.6069])\n",
      "Gradient: tensor([-0.0210,  0.1188])\n",
      "Epoch 1897, Loss 2.970304\n",
      "Params: tensor([  5.2446, -16.6081])\n",
      "Gradient: tensor([-0.0209,  0.1186])\n",
      "Epoch 1898, Loss 2.970159\n",
      "Params: tensor([  5.2449, -16.6093])\n",
      "Gradient: tensor([-0.0209,  0.1183])\n",
      "Epoch 1899, Loss 2.970015\n",
      "Params: tensor([  5.2451, -16.6105])\n",
      "Gradient: tensor([-0.0209,  0.1182])\n",
      "Epoch 1900, Loss 2.969871\n",
      "Params: tensor([  5.2453, -16.6116])\n",
      "Gradient: tensor([-0.0208,  0.1180])\n",
      "Epoch 1901, Loss 2.969727\n",
      "Params: tensor([  5.2455, -16.6128])\n",
      "Gradient: tensor([-0.0208,  0.1178])\n",
      "Epoch 1902, Loss 2.969586\n",
      "Params: tensor([  5.2457, -16.6140])\n",
      "Gradient: tensor([-0.0208,  0.1175])\n",
      "Epoch 1903, Loss 2.969443\n",
      "Params: tensor([  5.2459, -16.6152])\n",
      "Gradient: tensor([-0.0207,  0.1173])\n",
      "Epoch 1904, Loss 2.969302\n",
      "Params: tensor([  5.2461, -16.6163])\n",
      "Gradient: tensor([-0.0207,  0.1172])\n",
      "Epoch 1905, Loss 2.969160\n",
      "Params: tensor([  5.2463, -16.6175])\n",
      "Gradient: tensor([-0.0206,  0.1170])\n",
      "Epoch 1906, Loss 2.969017\n",
      "Params: tensor([  5.2465, -16.6187])\n",
      "Gradient: tensor([-0.0206,  0.1168])\n",
      "Epoch 1907, Loss 2.968879\n",
      "Params: tensor([  5.2467, -16.6198])\n",
      "Gradient: tensor([-0.0206,  0.1166])\n",
      "Epoch 1908, Loss 2.968739\n",
      "Params: tensor([  5.2469, -16.6210])\n",
      "Gradient: tensor([-0.0205,  0.1164])\n",
      "Epoch 1909, Loss 2.968599\n",
      "Params: tensor([  5.2471, -16.6222])\n",
      "Gradient: tensor([-0.0205,  0.1162])\n",
      "Epoch 1910, Loss 2.968460\n",
      "Params: tensor([  5.2473, -16.6233])\n",
      "Gradient: tensor([-0.0205,  0.1160])\n",
      "Epoch 1911, Loss 2.968321\n",
      "Params: tensor([  5.2475, -16.6245])\n",
      "Gradient: tensor([-0.0204,  0.1158])\n",
      "Epoch 1912, Loss 2.968183\n",
      "Params: tensor([  5.2477, -16.6256])\n",
      "Gradient: tensor([-0.0204,  0.1156])\n",
      "Epoch 1913, Loss 2.968046\n",
      "Params: tensor([  5.2479, -16.6268])\n",
      "Gradient: tensor([-0.0204,  0.1154])\n",
      "Epoch 1914, Loss 2.967908\n",
      "Params: tensor([  5.2482, -16.6279])\n",
      "Gradient: tensor([-0.0204,  0.1152])\n",
      "Epoch 1915, Loss 2.967772\n",
      "Params: tensor([  5.2484, -16.6291])\n",
      "Gradient: tensor([-0.0203,  0.1150])\n",
      "Epoch 1916, Loss 2.967635\n",
      "Params: tensor([  5.2486, -16.6302])\n",
      "Gradient: tensor([-0.0203,  0.1148])\n",
      "Epoch 1917, Loss 2.967499\n",
      "Params: tensor([  5.2488, -16.6314])\n",
      "Gradient: tensor([-0.0202,  0.1146])\n",
      "Epoch 1918, Loss 2.967365\n",
      "Params: tensor([  5.2490, -16.6325])\n",
      "Gradient: tensor([-0.0202,  0.1144])\n",
      "Epoch 1919, Loss 2.967230\n",
      "Params: tensor([  5.2492, -16.6337])\n",
      "Gradient: tensor([-0.0202,  0.1142])\n",
      "Epoch 1920, Loss 2.967095\n",
      "Params: tensor([  5.2494, -16.6348])\n",
      "Gradient: tensor([-0.0202,  0.1140])\n",
      "Epoch 1921, Loss 2.966961\n",
      "Params: tensor([  5.2496, -16.6360])\n",
      "Gradient: tensor([-0.0201,  0.1138])\n",
      "Epoch 1922, Loss 2.966828\n",
      "Params: tensor([  5.2498, -16.6371])\n",
      "Gradient: tensor([-0.0201,  0.1136])\n",
      "Epoch 1923, Loss 2.966693\n",
      "Params: tensor([  5.2500, -16.6382])\n",
      "Gradient: tensor([-0.0200,  0.1134])\n",
      "Epoch 1924, Loss 2.966561\n",
      "Params: tensor([  5.2502, -16.6394])\n",
      "Gradient: tensor([-0.0200,  0.1132])\n",
      "Epoch 1925, Loss 2.966429\n",
      "Params: tensor([  5.2504, -16.6405])\n",
      "Gradient: tensor([-0.0200,  0.1130])\n",
      "Epoch 1926, Loss 2.966297\n",
      "Params: tensor([  5.2506, -16.6416])\n",
      "Gradient: tensor([-0.0199,  0.1128])\n",
      "Epoch 1927, Loss 2.966168\n",
      "Params: tensor([  5.2508, -16.6427])\n",
      "Gradient: tensor([-0.0199,  0.1127])\n",
      "Epoch 1928, Loss 2.966036\n",
      "Params: tensor([  5.2510, -16.6439])\n",
      "Gradient: tensor([-0.0199,  0.1125])\n",
      "Epoch 1929, Loss 2.965904\n",
      "Params: tensor([  5.2512, -16.6450])\n",
      "Gradient: tensor([-0.0198,  0.1123])\n",
      "Epoch 1930, Loss 2.965777\n",
      "Params: tensor([  5.2514, -16.6461])\n",
      "Gradient: tensor([-0.0198,  0.1121])\n",
      "Epoch 1931, Loss 2.965646\n",
      "Params: tensor([  5.2516, -16.6472])\n",
      "Gradient: tensor([-0.0198,  0.1119])\n",
      "Epoch 1932, Loss 2.965517\n",
      "Params: tensor([  5.2518, -16.6484])\n",
      "Gradient: tensor([-0.0197,  0.1117])\n",
      "Epoch 1933, Loss 2.965388\n",
      "Params: tensor([  5.2520, -16.6495])\n",
      "Gradient: tensor([-0.0197,  0.1115])\n",
      "Epoch 1934, Loss 2.965260\n",
      "Params: tensor([  5.2522, -16.6506])\n",
      "Gradient: tensor([-0.0197,  0.1113])\n",
      "Epoch 1935, Loss 2.965131\n",
      "Params: tensor([  5.2523, -16.6517])\n",
      "Gradient: tensor([-0.0196,  0.1111])\n",
      "Epoch 1936, Loss 2.965005\n",
      "Params: tensor([  5.2525, -16.6528])\n",
      "Gradient: tensor([-0.0196,  0.1109])\n",
      "Epoch 1937, Loss 2.964877\n",
      "Params: tensor([  5.2527, -16.6539])\n",
      "Gradient: tensor([-0.0196,  0.1108])\n",
      "Epoch 1938, Loss 2.964751\n",
      "Params: tensor([  5.2529, -16.6550])\n",
      "Gradient: tensor([-0.0195,  0.1106])\n",
      "Epoch 1939, Loss 2.964625\n",
      "Params: tensor([  5.2531, -16.6561])\n",
      "Gradient: tensor([-0.0195,  0.1104])\n",
      "Epoch 1940, Loss 2.964500\n",
      "Params: tensor([  5.2533, -16.6572])\n",
      "Gradient: tensor([-0.0195,  0.1102])\n",
      "Epoch 1941, Loss 2.964375\n",
      "Params: tensor([  5.2535, -16.6583])\n",
      "Gradient: tensor([-0.0195,  0.1100])\n",
      "Epoch 1942, Loss 2.964250\n",
      "Params: tensor([  5.2537, -16.6594])\n",
      "Gradient: tensor([-0.0194,  0.1098])\n",
      "Epoch 1943, Loss 2.964126\n",
      "Params: tensor([  5.2539, -16.6605])\n",
      "Gradient: tensor([-0.0194,  0.1096])\n",
      "Epoch 1944, Loss 2.964001\n",
      "Params: tensor([  5.2541, -16.6616])\n",
      "Gradient: tensor([-0.0194,  0.1094])\n",
      "Epoch 1945, Loss 2.963879\n",
      "Params: tensor([  5.2543, -16.6627])\n",
      "Gradient: tensor([-0.0193,  0.1093])\n",
      "Epoch 1946, Loss 2.963756\n",
      "Params: tensor([  5.2545, -16.6638])\n",
      "Gradient: tensor([-0.0193,  0.1091])\n",
      "Epoch 1947, Loss 2.963632\n",
      "Params: tensor([  5.2547, -16.6649])\n",
      "Gradient: tensor([-0.0192,  0.1089])\n",
      "Epoch 1948, Loss 2.963511\n",
      "Params: tensor([  5.2549, -16.6660])\n",
      "Gradient: tensor([-0.0192,  0.1087])\n",
      "Epoch 1949, Loss 2.963388\n",
      "Params: tensor([  5.2551, -16.6671])\n",
      "Gradient: tensor([-0.0192,  0.1085])\n",
      "Epoch 1950, Loss 2.963266\n",
      "Params: tensor([  5.2553, -16.6681])\n",
      "Gradient: tensor([-0.0191,  0.1083])\n",
      "Epoch 1951, Loss 2.963149\n",
      "Params: tensor([  5.2554, -16.6692])\n",
      "Gradient: tensor([-0.0191,  0.1081])\n",
      "Epoch 1952, Loss 2.963026\n",
      "Params: tensor([  5.2556, -16.6703])\n",
      "Gradient: tensor([-0.0191,  0.1080])\n",
      "Epoch 1953, Loss 2.962907\n",
      "Params: tensor([  5.2558, -16.6714])\n",
      "Gradient: tensor([-0.0190,  0.1078])\n",
      "Epoch 1954, Loss 2.962788\n",
      "Params: tensor([  5.2560, -16.6725])\n",
      "Gradient: tensor([-0.0190,  0.1076])\n",
      "Epoch 1955, Loss 2.962666\n",
      "Params: tensor([  5.2562, -16.6735])\n",
      "Gradient: tensor([-0.0190,  0.1074])\n",
      "Epoch 1956, Loss 2.962547\n",
      "Params: tensor([  5.2564, -16.6746])\n",
      "Gradient: tensor([-0.0189,  0.1072])\n",
      "Epoch 1957, Loss 2.962429\n",
      "Params: tensor([  5.2566, -16.6757])\n",
      "Gradient: tensor([-0.0189,  0.1071])\n",
      "Epoch 1958, Loss 2.962312\n",
      "Params: tensor([  5.2568, -16.6767])\n",
      "Gradient: tensor([-0.0189,  0.1069])\n",
      "Epoch 1959, Loss 2.962195\n",
      "Params: tensor([  5.2570, -16.6778])\n",
      "Gradient: tensor([-0.0188,  0.1067])\n",
      "Epoch 1960, Loss 2.962078\n",
      "Params: tensor([  5.2572, -16.6789])\n",
      "Gradient: tensor([-0.0188,  0.1065])\n",
      "Epoch 1961, Loss 2.961958\n",
      "Params: tensor([  5.2573, -16.6799])\n",
      "Gradient: tensor([-0.0188,  0.1063])\n",
      "Epoch 1962, Loss 2.961843\n",
      "Params: tensor([  5.2575, -16.6810])\n",
      "Gradient: tensor([-0.0187,  0.1062])\n",
      "Epoch 1963, Loss 2.961728\n",
      "Params: tensor([  5.2577, -16.6821])\n",
      "Gradient: tensor([-0.0187,  0.1060])\n",
      "Epoch 1964, Loss 2.961611\n",
      "Params: tensor([  5.2579, -16.6831])\n",
      "Gradient: tensor([-0.0187,  0.1058])\n",
      "Epoch 1965, Loss 2.961496\n",
      "Params: tensor([  5.2581, -16.6842])\n",
      "Gradient: tensor([-0.0187,  0.1056])\n",
      "Epoch 1966, Loss 2.961382\n",
      "Params: tensor([  5.2583, -16.6852])\n",
      "Gradient: tensor([-0.0186,  0.1054])\n",
      "Epoch 1967, Loss 2.961268\n",
      "Params: tensor([  5.2585, -16.6863])\n",
      "Gradient: tensor([-0.0186,  0.1052])\n",
      "Epoch 1968, Loss 2.961153\n",
      "Params: tensor([  5.2586, -16.6873])\n",
      "Gradient: tensor([-0.0186,  0.1051])\n",
      "Epoch 1969, Loss 2.961038\n",
      "Params: tensor([  5.2588, -16.6884])\n",
      "Gradient: tensor([-0.0185,  0.1049])\n",
      "Epoch 1970, Loss 2.960926\n",
      "Params: tensor([  5.2590, -16.6894])\n",
      "Gradient: tensor([-0.0185,  0.1047])\n",
      "Epoch 1971, Loss 2.960814\n",
      "Params: tensor([  5.2592, -16.6905])\n",
      "Gradient: tensor([-0.0185,  0.1045])\n",
      "Epoch 1972, Loss 2.960699\n",
      "Params: tensor([  5.2594, -16.6915])\n",
      "Gradient: tensor([-0.0184,  0.1044])\n",
      "Epoch 1973, Loss 2.960587\n",
      "Params: tensor([  5.2596, -16.6926])\n",
      "Gradient: tensor([-0.0184,  0.1042])\n",
      "Epoch 1974, Loss 2.960475\n",
      "Params: tensor([  5.2598, -16.6936])\n",
      "Gradient: tensor([-0.0184,  0.1040])\n",
      "Epoch 1975, Loss 2.960365\n",
      "Params: tensor([  5.2599, -16.6946])\n",
      "Gradient: tensor([-0.0183,  0.1038])\n",
      "Epoch 1976, Loss 2.960254\n",
      "Params: tensor([  5.2601, -16.6957])\n",
      "Gradient: tensor([-0.0183,  0.1037])\n",
      "Epoch 1977, Loss 2.960143\n",
      "Params: tensor([  5.2603, -16.6967])\n",
      "Gradient: tensor([-0.0183,  0.1035])\n",
      "Epoch 1978, Loss 2.960033\n",
      "Params: tensor([  5.2605, -16.6977])\n",
      "Gradient: tensor([-0.0182,  0.1033])\n",
      "Epoch 1979, Loss 2.959923\n",
      "Params: tensor([  5.2607, -16.6988])\n",
      "Gradient: tensor([-0.0182,  0.1031])\n",
      "Epoch 1980, Loss 2.959812\n",
      "Params: tensor([  5.2608, -16.6998])\n",
      "Gradient: tensor([-0.0182,  0.1029])\n",
      "Epoch 1981, Loss 2.959703\n",
      "Params: tensor([  5.2610, -16.7008])\n",
      "Gradient: tensor([-0.0182,  0.1028])\n",
      "Epoch 1982, Loss 2.959594\n",
      "Params: tensor([  5.2612, -16.7019])\n",
      "Gradient: tensor([-0.0181,  0.1026])\n",
      "Epoch 1983, Loss 2.959486\n",
      "Params: tensor([  5.2614, -16.7029])\n",
      "Gradient: tensor([-0.0181,  0.1024])\n",
      "Epoch 1984, Loss 2.959378\n",
      "Params: tensor([  5.2616, -16.7039])\n",
      "Gradient: tensor([-0.0181,  0.1022])\n",
      "Epoch 1985, Loss 2.959271\n",
      "Params: tensor([  5.2618, -16.7049])\n",
      "Gradient: tensor([-0.0180,  0.1021])\n",
      "Epoch 1986, Loss 2.959162\n",
      "Params: tensor([  5.2619, -16.7059])\n",
      "Gradient: tensor([-0.0180,  0.1019])\n",
      "Epoch 1987, Loss 2.959055\n",
      "Params: tensor([  5.2621, -16.7070])\n",
      "Gradient: tensor([-0.0180,  0.1017])\n",
      "Epoch 1988, Loss 2.958951\n",
      "Params: tensor([  5.2623, -16.7080])\n",
      "Gradient: tensor([-0.0179,  0.1016])\n",
      "Epoch 1989, Loss 2.958842\n",
      "Params: tensor([  5.2625, -16.7090])\n",
      "Gradient: tensor([-0.0179,  0.1014])\n",
      "Epoch 1990, Loss 2.958738\n",
      "Params: tensor([  5.2626, -16.7100])\n",
      "Gradient: tensor([-0.0179,  0.1012])\n",
      "Epoch 1991, Loss 2.958632\n",
      "Params: tensor([  5.2628, -16.7110])\n",
      "Gradient: tensor([-0.0179,  0.1010])\n",
      "Epoch 1992, Loss 2.958526\n",
      "Params: tensor([  5.2630, -16.7120])\n",
      "Gradient: tensor([-0.0178,  0.1009])\n",
      "Epoch 1993, Loss 2.958422\n",
      "Params: tensor([  5.2632, -16.7130])\n",
      "Gradient: tensor([-0.0178,  0.1007])\n",
      "Epoch 1994, Loss 2.958317\n",
      "Params: tensor([  5.2634, -16.7140])\n",
      "Gradient: tensor([-0.0178,  0.1005])\n",
      "Epoch 1995, Loss 2.958212\n",
      "Params: tensor([  5.2635, -16.7150])\n",
      "Gradient: tensor([-0.0177,  0.1004])\n",
      "Epoch 1996, Loss 2.958111\n",
      "Params: tensor([  5.2637, -16.7160])\n",
      "Gradient: tensor([-0.0177,  0.1002])\n",
      "Epoch 1997, Loss 2.958006\n",
      "Params: tensor([  5.2639, -16.7170])\n",
      "Gradient: tensor([-0.0177,  0.1000])\n",
      "Epoch 1998, Loss 2.957904\n",
      "Params: tensor([  5.2641, -16.7180])\n",
      "Gradient: tensor([-0.0176,  0.0998])\n",
      "Epoch 1999, Loss 2.957801\n",
      "Params: tensor([  5.2642, -16.7190])\n",
      "Gradient: tensor([-0.0176,  0.0997])\n",
      "Epoch 2000, Loss 2.957697\n",
      "Params: tensor([  5.2644, -16.7200])\n",
      "Gradient: tensor([-0.0176,  0.0995])\n",
      "Epoch 2001, Loss 2.957596\n",
      "Params: tensor([  5.2646, -16.7210])\n",
      "Gradient: tensor([-0.0176,  0.0993])\n",
      "Epoch 2002, Loss 2.957494\n",
      "Params: tensor([  5.2648, -16.7220])\n",
      "Gradient: tensor([-0.0175,  0.0992])\n",
      "Epoch 2003, Loss 2.957392\n",
      "Params: tensor([  5.2649, -16.7230])\n",
      "Gradient: tensor([-0.0175,  0.0990])\n",
      "Epoch 2004, Loss 2.957293\n",
      "Params: tensor([  5.2651, -16.7240])\n",
      "Gradient: tensor([-0.0174,  0.0988])\n",
      "Epoch 2005, Loss 2.957193\n",
      "Params: tensor([  5.2653, -16.7250])\n",
      "Gradient: tensor([-0.0174,  0.0987])\n",
      "Epoch 2006, Loss 2.957091\n",
      "Params: tensor([  5.2655, -16.7260])\n",
      "Gradient: tensor([-0.0174,  0.0985])\n",
      "Epoch 2007, Loss 2.956992\n",
      "Params: tensor([  5.2656, -16.7269])\n",
      "Gradient: tensor([-0.0174,  0.0983])\n",
      "Epoch 2008, Loss 2.956892\n",
      "Params: tensor([  5.2658, -16.7279])\n",
      "Gradient: tensor([-0.0173,  0.0982])\n",
      "Epoch 2009, Loss 2.956792\n",
      "Params: tensor([  5.2660, -16.7289])\n",
      "Gradient: tensor([-0.0173,  0.0980])\n",
      "Epoch 2010, Loss 2.956694\n",
      "Params: tensor([  5.2662, -16.7299])\n",
      "Gradient: tensor([-0.0173,  0.0978])\n",
      "Epoch 2011, Loss 2.956595\n",
      "Params: tensor([  5.2663, -16.7309])\n",
      "Gradient: tensor([-0.0172,  0.0977])\n",
      "Epoch 2012, Loss 2.956496\n",
      "Params: tensor([  5.2665, -16.7318])\n",
      "Gradient: tensor([-0.0172,  0.0975])\n",
      "Epoch 2013, Loss 2.956397\n",
      "Params: tensor([  5.2667, -16.7328])\n",
      "Gradient: tensor([-0.0172,  0.0973])\n",
      "Epoch 2014, Loss 2.956300\n",
      "Params: tensor([  5.2668, -16.7338])\n",
      "Gradient: tensor([-0.0172,  0.0972])\n",
      "Epoch 2015, Loss 2.956204\n",
      "Params: tensor([  5.2670, -16.7348])\n",
      "Gradient: tensor([-0.0171,  0.0970])\n",
      "Epoch 2016, Loss 2.956108\n",
      "Params: tensor([  5.2672, -16.7357])\n",
      "Gradient: tensor([-0.0171,  0.0968])\n",
      "Epoch 2017, Loss 2.956010\n",
      "Params: tensor([  5.2674, -16.7367])\n",
      "Gradient: tensor([-0.0171,  0.0967])\n",
      "Epoch 2018, Loss 2.955914\n",
      "Params: tensor([  5.2675, -16.7377])\n",
      "Gradient: tensor([-0.0171,  0.0965])\n",
      "Epoch 2019, Loss 2.955817\n",
      "Params: tensor([  5.2677, -16.7386])\n",
      "Gradient: tensor([-0.0170,  0.0963])\n",
      "Epoch 2020, Loss 2.955722\n",
      "Params: tensor([  5.2679, -16.7396])\n",
      "Gradient: tensor([-0.0170,  0.0962])\n",
      "Epoch 2021, Loss 2.955627\n",
      "Params: tensor([  5.2680, -16.7405])\n",
      "Gradient: tensor([-0.0170,  0.0960])\n",
      "Epoch 2022, Loss 2.955533\n",
      "Params: tensor([  5.2682, -16.7415])\n",
      "Gradient: tensor([-0.0169,  0.0959])\n",
      "Epoch 2023, Loss 2.955436\n",
      "Params: tensor([  5.2684, -16.7425])\n",
      "Gradient: tensor([-0.0169,  0.0957])\n",
      "Epoch 2024, Loss 2.955343\n",
      "Params: tensor([  5.2686, -16.7434])\n",
      "Gradient: tensor([-0.0169,  0.0955])\n",
      "Epoch 2025, Loss 2.955250\n",
      "Params: tensor([  5.2687, -16.7444])\n",
      "Gradient: tensor([-0.0169,  0.0954])\n",
      "Epoch 2026, Loss 2.955154\n",
      "Params: tensor([  5.2689, -16.7453])\n",
      "Gradient: tensor([-0.0168,  0.0952])\n",
      "Epoch 2027, Loss 2.955063\n",
      "Params: tensor([  5.2691, -16.7463])\n",
      "Gradient: tensor([-0.0168,  0.0950])\n",
      "Epoch 2028, Loss 2.954968\n",
      "Params: tensor([  5.2692, -16.7472])\n",
      "Gradient: tensor([-0.0168,  0.0949])\n",
      "Epoch 2029, Loss 2.954875\n",
      "Params: tensor([  5.2694, -16.7482])\n",
      "Gradient: tensor([-0.0167,  0.0947])\n",
      "Epoch 2030, Loss 2.954783\n",
      "Params: tensor([  5.2696, -16.7491])\n",
      "Gradient: tensor([-0.0167,  0.0946])\n",
      "Epoch 2031, Loss 2.954691\n",
      "Params: tensor([  5.2697, -16.7501])\n",
      "Gradient: tensor([-0.0167,  0.0944])\n",
      "Epoch 2032, Loss 2.954600\n",
      "Params: tensor([  5.2699, -16.7510])\n",
      "Gradient: tensor([-0.0167,  0.0942])\n",
      "Epoch 2033, Loss 2.954507\n",
      "Params: tensor([  5.2701, -16.7519])\n",
      "Gradient: tensor([-0.0166,  0.0941])\n",
      "Epoch 2034, Loss 2.954417\n",
      "Params: tensor([  5.2702, -16.7529])\n",
      "Gradient: tensor([-0.0166,  0.0939])\n",
      "Epoch 2035, Loss 2.954326\n",
      "Params: tensor([  5.2704, -16.7538])\n",
      "Gradient: tensor([-0.0165,  0.0938])\n",
      "Epoch 2036, Loss 2.954235\n",
      "Params: tensor([  5.2706, -16.7547])\n",
      "Gradient: tensor([-0.0165,  0.0936])\n",
      "Epoch 2037, Loss 2.954145\n",
      "Params: tensor([  5.2707, -16.7557])\n",
      "Gradient: tensor([-0.0165,  0.0934])\n",
      "Epoch 2038, Loss 2.954055\n",
      "Params: tensor([  5.2709, -16.7566])\n",
      "Gradient: tensor([-0.0165,  0.0933])\n",
      "Epoch 2039, Loss 2.953966\n",
      "Params: tensor([  5.2710, -16.7575])\n",
      "Gradient: tensor([-0.0164,  0.0931])\n",
      "Epoch 2040, Loss 2.953876\n",
      "Params: tensor([  5.2712, -16.7585])\n",
      "Gradient: tensor([-0.0164,  0.0930])\n",
      "Epoch 2041, Loss 2.953787\n",
      "Params: tensor([  5.2714, -16.7594])\n",
      "Gradient: tensor([-0.0164,  0.0928])\n",
      "Epoch 2042, Loss 2.953698\n",
      "Params: tensor([  5.2715, -16.7603])\n",
      "Gradient: tensor([-0.0164,  0.0926])\n",
      "Epoch 2043, Loss 2.953610\n",
      "Params: tensor([  5.2717, -16.7613])\n",
      "Gradient: tensor([-0.0163,  0.0925])\n",
      "Epoch 2044, Loss 2.953521\n",
      "Params: tensor([  5.2719, -16.7622])\n",
      "Gradient: tensor([-0.0163,  0.0923])\n",
      "Epoch 2045, Loss 2.953434\n",
      "Params: tensor([  5.2720, -16.7631])\n",
      "Gradient: tensor([-0.0163,  0.0922])\n",
      "Epoch 2046, Loss 2.953346\n",
      "Params: tensor([  5.2722, -16.7640])\n",
      "Gradient: tensor([-0.0163,  0.0920])\n",
      "Epoch 2047, Loss 2.953259\n",
      "Params: tensor([  5.2724, -16.7649])\n",
      "Gradient: tensor([-0.0162,  0.0919])\n",
      "Epoch 2048, Loss 2.953171\n",
      "Params: tensor([  5.2725, -16.7659])\n",
      "Gradient: tensor([-0.0162,  0.0917])\n",
      "Epoch 2049, Loss 2.953085\n",
      "Params: tensor([  5.2727, -16.7668])\n",
      "Gradient: tensor([-0.0162,  0.0915])\n",
      "Epoch 2050, Loss 2.952999\n",
      "Params: tensor([  5.2728, -16.7677])\n",
      "Gradient: tensor([-0.0162,  0.0914])\n",
      "Epoch 2051, Loss 2.952913\n",
      "Params: tensor([  5.2730, -16.7686])\n",
      "Gradient: tensor([-0.0161,  0.0912])\n",
      "Epoch 2052, Loss 2.952828\n",
      "Params: tensor([  5.2732, -16.7695])\n",
      "Gradient: tensor([-0.0161,  0.0911])\n",
      "Epoch 2053, Loss 2.952742\n",
      "Params: tensor([  5.2733, -16.7704])\n",
      "Gradient: tensor([-0.0161,  0.0909])\n",
      "Epoch 2054, Loss 2.952657\n",
      "Params: tensor([  5.2735, -16.7713])\n",
      "Gradient: tensor([-0.0160,  0.0908])\n",
      "Epoch 2055, Loss 2.952571\n",
      "Params: tensor([  5.2736, -16.7722])\n",
      "Gradient: tensor([-0.0160,  0.0906])\n",
      "Epoch 2056, Loss 2.952487\n",
      "Params: tensor([  5.2738, -16.7731])\n",
      "Gradient: tensor([-0.0160,  0.0905])\n",
      "Epoch 2057, Loss 2.952403\n",
      "Params: tensor([  5.2740, -16.7740])\n",
      "Gradient: tensor([-0.0160,  0.0903])\n",
      "Epoch 2058, Loss 2.952318\n",
      "Params: tensor([  5.2741, -16.7749])\n",
      "Gradient: tensor([-0.0159,  0.0902])\n",
      "Epoch 2059, Loss 2.952235\n",
      "Params: tensor([  5.2743, -16.7758])\n",
      "Gradient: tensor([-0.0159,  0.0900])\n",
      "Epoch 2060, Loss 2.952152\n",
      "Params: tensor([  5.2744, -16.7767])\n",
      "Gradient: tensor([-0.0159,  0.0899])\n",
      "Epoch 2061, Loss 2.952068\n",
      "Params: tensor([  5.2746, -16.7776])\n",
      "Gradient: tensor([-0.0158,  0.0897])\n",
      "Epoch 2062, Loss 2.951985\n",
      "Params: tensor([  5.2748, -16.7785])\n",
      "Gradient: tensor([-0.0158,  0.0895])\n",
      "Epoch 2063, Loss 2.951902\n",
      "Params: tensor([  5.2749, -16.7794])\n",
      "Gradient: tensor([-0.0158,  0.0894])\n",
      "Epoch 2064, Loss 2.951820\n",
      "Params: tensor([  5.2751, -16.7803])\n",
      "Gradient: tensor([-0.0158,  0.0892])\n",
      "Epoch 2065, Loss 2.951738\n",
      "Params: tensor([  5.2752, -16.7812])\n",
      "Gradient: tensor([-0.0157,  0.0891])\n",
      "Epoch 2066, Loss 2.951656\n",
      "Params: tensor([  5.2754, -16.7821])\n",
      "Gradient: tensor([-0.0157,  0.0889])\n",
      "Epoch 2067, Loss 2.951576\n",
      "Params: tensor([  5.2755, -16.7830])\n",
      "Gradient: tensor([-0.0157,  0.0888])\n",
      "Epoch 2068, Loss 2.951494\n",
      "Params: tensor([  5.2757, -16.7839])\n",
      "Gradient: tensor([-0.0157,  0.0886])\n",
      "Epoch 2069, Loss 2.951413\n",
      "Params: tensor([  5.2759, -16.7848])\n",
      "Gradient: tensor([-0.0157,  0.0885])\n",
      "Epoch 2070, Loss 2.951333\n",
      "Params: tensor([  5.2760, -16.7856])\n",
      "Gradient: tensor([-0.0156,  0.0883])\n",
      "Epoch 2071, Loss 2.951252\n",
      "Params: tensor([  5.2762, -16.7865])\n",
      "Gradient: tensor([-0.0156,  0.0882])\n",
      "Epoch 2072, Loss 2.951171\n",
      "Params: tensor([  5.2763, -16.7874])\n",
      "Gradient: tensor([-0.0155,  0.0880])\n",
      "Epoch 2073, Loss 2.951093\n",
      "Params: tensor([  5.2765, -16.7883])\n",
      "Gradient: tensor([-0.0155,  0.0879])\n",
      "Epoch 2074, Loss 2.951012\n",
      "Params: tensor([  5.2766, -16.7892])\n",
      "Gradient: tensor([-0.0155,  0.0877])\n",
      "Epoch 2075, Loss 2.950932\n",
      "Params: tensor([  5.2768, -16.7900])\n",
      "Gradient: tensor([-0.0155,  0.0876])\n",
      "Epoch 2076, Loss 2.950853\n",
      "Params: tensor([  5.2769, -16.7909])\n",
      "Gradient: tensor([-0.0154,  0.0874])\n",
      "Epoch 2077, Loss 2.950775\n",
      "Params: tensor([  5.2771, -16.7918])\n",
      "Gradient: tensor([-0.0154,  0.0873])\n",
      "Epoch 2078, Loss 2.950697\n",
      "Params: tensor([  5.2772, -16.7927])\n",
      "Gradient: tensor([-0.0154,  0.0871])\n",
      "Epoch 2079, Loss 2.950618\n",
      "Params: tensor([  5.2774, -16.7935])\n",
      "Gradient: tensor([-0.0154,  0.0870])\n",
      "Epoch 2080, Loss 2.950540\n",
      "Params: tensor([  5.2776, -16.7944])\n",
      "Gradient: tensor([-0.0154,  0.0868])\n",
      "Epoch 2081, Loss 2.950463\n",
      "Params: tensor([  5.2777, -16.7953])\n",
      "Gradient: tensor([-0.0153,  0.0867])\n",
      "Epoch 2082, Loss 2.950385\n",
      "Params: tensor([  5.2779, -16.7961])\n",
      "Gradient: tensor([-0.0153,  0.0866])\n",
      "Epoch 2083, Loss 2.950309\n",
      "Params: tensor([  5.2780, -16.7970])\n",
      "Gradient: tensor([-0.0153,  0.0864])\n",
      "Epoch 2084, Loss 2.950231\n",
      "Params: tensor([  5.2782, -16.7979])\n",
      "Gradient: tensor([-0.0152,  0.0863])\n",
      "Epoch 2085, Loss 2.950155\n",
      "Params: tensor([  5.2783, -16.7987])\n",
      "Gradient: tensor([-0.0152,  0.0861])\n",
      "Epoch 2086, Loss 2.950078\n",
      "Params: tensor([  5.2785, -16.7996])\n",
      "Gradient: tensor([-0.0152,  0.0860])\n",
      "Epoch 2087, Loss 2.950004\n",
      "Params: tensor([  5.2786, -16.8004])\n",
      "Gradient: tensor([-0.0152,  0.0858])\n",
      "Epoch 2088, Loss 2.949925\n",
      "Params: tensor([  5.2788, -16.8013])\n",
      "Gradient: tensor([-0.0152,  0.0857])\n",
      "Epoch 2089, Loss 2.949850\n",
      "Params: tensor([  5.2789, -16.8021])\n",
      "Gradient: tensor([-0.0151,  0.0855])\n",
      "Epoch 2090, Loss 2.949776\n",
      "Params: tensor([  5.2791, -16.8030])\n",
      "Gradient: tensor([-0.0151,  0.0854])\n",
      "Epoch 2091, Loss 2.949699\n",
      "Params: tensor([  5.2792, -16.8039])\n",
      "Gradient: tensor([-0.0151,  0.0852])\n",
      "Epoch 2092, Loss 2.949625\n",
      "Params: tensor([  5.2794, -16.8047])\n",
      "Gradient: tensor([-0.0150,  0.0851])\n",
      "Epoch 2093, Loss 2.949550\n",
      "Params: tensor([  5.2795, -16.8056])\n",
      "Gradient: tensor([-0.0150,  0.0850])\n",
      "Epoch 2094, Loss 2.949476\n",
      "Params: tensor([  5.2797, -16.8064])\n",
      "Gradient: tensor([-0.0150,  0.0848])\n",
      "Epoch 2095, Loss 2.949401\n",
      "Params: tensor([  5.2798, -16.8072])\n",
      "Gradient: tensor([-0.0149,  0.0847])\n",
      "Epoch 2096, Loss 2.949328\n",
      "Params: tensor([  5.2800, -16.8081])\n",
      "Gradient: tensor([-0.0150,  0.0845])\n",
      "Epoch 2097, Loss 2.949254\n",
      "Params: tensor([  5.2801, -16.8089])\n",
      "Gradient: tensor([-0.0149,  0.0844])\n",
      "Epoch 2098, Loss 2.949182\n",
      "Params: tensor([  5.2803, -16.8098])\n",
      "Gradient: tensor([-0.0149,  0.0842])\n",
      "Epoch 2099, Loss 2.949108\n",
      "Params: tensor([  5.2804, -16.8106])\n",
      "Gradient: tensor([-0.0149,  0.0841])\n",
      "Epoch 2100, Loss 2.949035\n",
      "Params: tensor([  5.2806, -16.8115])\n",
      "Gradient: tensor([-0.0148,  0.0839])\n",
      "Epoch 2101, Loss 2.948962\n",
      "Params: tensor([  5.2807, -16.8123])\n",
      "Gradient: tensor([-0.0148,  0.0838])\n",
      "Epoch 2102, Loss 2.948890\n",
      "Params: tensor([  5.2809, -16.8131])\n",
      "Gradient: tensor([-0.0148,  0.0837])\n",
      "Epoch 2103, Loss 2.948818\n",
      "Params: tensor([  5.2810, -16.8140])\n",
      "Gradient: tensor([-0.0148,  0.0835])\n",
      "Epoch 2104, Loss 2.948746\n",
      "Params: tensor([  5.2812, -16.8148])\n",
      "Gradient: tensor([-0.0148,  0.0834])\n",
      "Epoch 2105, Loss 2.948675\n",
      "Params: tensor([  5.2813, -16.8156])\n",
      "Gradient: tensor([-0.0147,  0.0832])\n",
      "Epoch 2106, Loss 2.948602\n",
      "Params: tensor([  5.2815, -16.8165])\n",
      "Gradient: tensor([-0.0147,  0.0831])\n",
      "Epoch 2107, Loss 2.948532\n",
      "Params: tensor([  5.2816, -16.8173])\n",
      "Gradient: tensor([-0.0146,  0.0830])\n",
      "Epoch 2108, Loss 2.948462\n",
      "Params: tensor([  5.2817, -16.8181])\n",
      "Gradient: tensor([-0.0146,  0.0828])\n",
      "Epoch 2109, Loss 2.948391\n",
      "Params: tensor([  5.2819, -16.8189])\n",
      "Gradient: tensor([-0.0146,  0.0827])\n",
      "Epoch 2110, Loss 2.948321\n",
      "Params: tensor([  5.2820, -16.8198])\n",
      "Gradient: tensor([-0.0146,  0.0825])\n",
      "Epoch 2111, Loss 2.948250\n",
      "Params: tensor([  5.2822, -16.8206])\n",
      "Gradient: tensor([-0.0145,  0.0824])\n",
      "Epoch 2112, Loss 2.948180\n",
      "Params: tensor([  5.2823, -16.8214])\n",
      "Gradient: tensor([-0.0145,  0.0823])\n",
      "Epoch 2113, Loss 2.948109\n",
      "Params: tensor([  5.2825, -16.8222])\n",
      "Gradient: tensor([-0.0145,  0.0821])\n",
      "Epoch 2114, Loss 2.948041\n",
      "Params: tensor([  5.2826, -16.8231])\n",
      "Gradient: tensor([-0.0145,  0.0820])\n",
      "Epoch 2115, Loss 2.947971\n",
      "Params: tensor([  5.2828, -16.8239])\n",
      "Gradient: tensor([-0.0144,  0.0818])\n",
      "Epoch 2116, Loss 2.947902\n",
      "Params: tensor([  5.2829, -16.8247])\n",
      "Gradient: tensor([-0.0144,  0.0817])\n",
      "Epoch 2117, Loss 2.947833\n",
      "Params: tensor([  5.2831, -16.8255])\n",
      "Gradient: tensor([-0.0144,  0.0816])\n",
      "Epoch 2118, Loss 2.947765\n",
      "Params: tensor([  5.2832, -16.8263])\n",
      "Gradient: tensor([-0.0144,  0.0814])\n",
      "Epoch 2119, Loss 2.947696\n",
      "Params: tensor([  5.2833, -16.8271])\n",
      "Gradient: tensor([-0.0144,  0.0813])\n",
      "Epoch 2120, Loss 2.947628\n",
      "Params: tensor([  5.2835, -16.8280])\n",
      "Gradient: tensor([-0.0143,  0.0811])\n",
      "Epoch 2121, Loss 2.947560\n",
      "Params: tensor([  5.2836, -16.8288])\n",
      "Gradient: tensor([-0.0143,  0.0810])\n",
      "Epoch 2122, Loss 2.947494\n",
      "Params: tensor([  5.2838, -16.8296])\n",
      "Gradient: tensor([-0.0143,  0.0809])\n",
      "Epoch 2123, Loss 2.947426\n",
      "Params: tensor([  5.2839, -16.8304])\n",
      "Gradient: tensor([-0.0143,  0.0807])\n",
      "Epoch 2124, Loss 2.947358\n",
      "Params: tensor([  5.2841, -16.8312])\n",
      "Gradient: tensor([-0.0142,  0.0806])\n",
      "Epoch 2125, Loss 2.947293\n",
      "Params: tensor([  5.2842, -16.8320])\n",
      "Gradient: tensor([-0.0142,  0.0805])\n",
      "Epoch 2126, Loss 2.947225\n",
      "Params: tensor([  5.2843, -16.8328])\n",
      "Gradient: tensor([-0.0142,  0.0803])\n",
      "Epoch 2127, Loss 2.947158\n",
      "Params: tensor([  5.2845, -16.8336])\n",
      "Gradient: tensor([-0.0142,  0.0802])\n",
      "Epoch 2128, Loss 2.947092\n",
      "Params: tensor([  5.2846, -16.8344])\n",
      "Gradient: tensor([-0.0141,  0.0800])\n",
      "Epoch 2129, Loss 2.947026\n",
      "Params: tensor([  5.2848, -16.8352])\n",
      "Gradient: tensor([-0.0141,  0.0799])\n",
      "Epoch 2130, Loss 2.946960\n",
      "Params: tensor([  5.2849, -16.8360])\n",
      "Gradient: tensor([-0.0141,  0.0798])\n",
      "Epoch 2131, Loss 2.946895\n",
      "Params: tensor([  5.2850, -16.8368])\n",
      "Gradient: tensor([-0.0141,  0.0796])\n",
      "Epoch 2132, Loss 2.946830\n",
      "Params: tensor([  5.2852, -16.8376])\n",
      "Gradient: tensor([-0.0141,  0.0795])\n",
      "Epoch 2133, Loss 2.946765\n",
      "Params: tensor([  5.2853, -16.8384])\n",
      "Gradient: tensor([-0.0140,  0.0794])\n",
      "Epoch 2134, Loss 2.946700\n",
      "Params: tensor([  5.2855, -16.8392])\n",
      "Gradient: tensor([-0.0140,  0.0792])\n",
      "Epoch 2135, Loss 2.946635\n",
      "Params: tensor([  5.2856, -16.8400])\n",
      "Gradient: tensor([-0.0140,  0.0791])\n",
      "Epoch 2136, Loss 2.946571\n",
      "Params: tensor([  5.2857, -16.8407])\n",
      "Gradient: tensor([-0.0139,  0.0790])\n",
      "Epoch 2137, Loss 2.946507\n",
      "Params: tensor([  5.2859, -16.8415])\n",
      "Gradient: tensor([-0.0139,  0.0788])\n",
      "Epoch 2138, Loss 2.946442\n",
      "Params: tensor([  5.2860, -16.8423])\n",
      "Gradient: tensor([-0.0139,  0.0787])\n",
      "Epoch 2139, Loss 2.946378\n",
      "Params: tensor([  5.2862, -16.8431])\n",
      "Gradient: tensor([-0.0139,  0.0786])\n",
      "Epoch 2140, Loss 2.946314\n",
      "Params: tensor([  5.2863, -16.8439])\n",
      "Gradient: tensor([-0.0138,  0.0784])\n",
      "Epoch 2141, Loss 2.946251\n",
      "Params: tensor([  5.2864, -16.8447])\n",
      "Gradient: tensor([-0.0138,  0.0783])\n",
      "Epoch 2142, Loss 2.946189\n",
      "Params: tensor([  5.2866, -16.8455])\n",
      "Gradient: tensor([-0.0138,  0.0782])\n",
      "Epoch 2143, Loss 2.946125\n",
      "Params: tensor([  5.2867, -16.8462])\n",
      "Gradient: tensor([-0.0138,  0.0780])\n",
      "Epoch 2144, Loss 2.946063\n",
      "Params: tensor([  5.2869, -16.8470])\n",
      "Gradient: tensor([-0.0138,  0.0779])\n",
      "Epoch 2145, Loss 2.946001\n",
      "Params: tensor([  5.2870, -16.8478])\n",
      "Gradient: tensor([-0.0137,  0.0778])\n",
      "Epoch 2146, Loss 2.945937\n",
      "Params: tensor([  5.2871, -16.8486])\n",
      "Gradient: tensor([-0.0137,  0.0776])\n",
      "Epoch 2147, Loss 2.945876\n",
      "Params: tensor([  5.2873, -16.8493])\n",
      "Gradient: tensor([-0.0137,  0.0775])\n",
      "Epoch 2148, Loss 2.945815\n",
      "Params: tensor([  5.2874, -16.8501])\n",
      "Gradient: tensor([-0.0137,  0.0774])\n",
      "Epoch 2149, Loss 2.945753\n",
      "Params: tensor([  5.2875, -16.8509])\n",
      "Gradient: tensor([-0.0136,  0.0772])\n",
      "Epoch 2150, Loss 2.945690\n",
      "Params: tensor([  5.2877, -16.8517])\n",
      "Gradient: tensor([-0.0136,  0.0771])\n",
      "Epoch 2151, Loss 2.945630\n",
      "Params: tensor([  5.2878, -16.8524])\n",
      "Gradient: tensor([-0.0136,  0.0770])\n",
      "Epoch 2152, Loss 2.945567\n",
      "Params: tensor([  5.2879, -16.8532])\n",
      "Gradient: tensor([-0.0136,  0.0768])\n",
      "Epoch 2153, Loss 2.945509\n",
      "Params: tensor([  5.2881, -16.8540])\n",
      "Gradient: tensor([-0.0135,  0.0767])\n",
      "Epoch 2154, Loss 2.945448\n",
      "Params: tensor([  5.2882, -16.8547])\n",
      "Gradient: tensor([-0.0135,  0.0766])\n",
      "Epoch 2155, Loss 2.945386\n",
      "Params: tensor([  5.2884, -16.8555])\n",
      "Gradient: tensor([-0.0135,  0.0765])\n",
      "Epoch 2156, Loss 2.945326\n",
      "Params: tensor([  5.2885, -16.8563])\n",
      "Gradient: tensor([-0.0135,  0.0763])\n",
      "Epoch 2157, Loss 2.945267\n",
      "Params: tensor([  5.2886, -16.8570])\n",
      "Gradient: tensor([-0.0135,  0.0762])\n",
      "Epoch 2158, Loss 2.945206\n",
      "Params: tensor([  5.2888, -16.8578])\n",
      "Gradient: tensor([-0.0134,  0.0761])\n",
      "Epoch 2159, Loss 2.945146\n",
      "Params: tensor([  5.2889, -16.8585])\n",
      "Gradient: tensor([-0.0134,  0.0759])\n",
      "Epoch 2160, Loss 2.945088\n",
      "Params: tensor([  5.2890, -16.8593])\n",
      "Gradient: tensor([-0.0134,  0.0758])\n",
      "Epoch 2161, Loss 2.945027\n",
      "Params: tensor([  5.2892, -16.8601])\n",
      "Gradient: tensor([-0.0134,  0.0757])\n",
      "Epoch 2162, Loss 2.944969\n",
      "Params: tensor([  5.2893, -16.8608])\n",
      "Gradient: tensor([-0.0133,  0.0755])\n",
      "Epoch 2163, Loss 2.944911\n",
      "Params: tensor([  5.2894, -16.8616])\n",
      "Gradient: tensor([-0.0133,  0.0754])\n",
      "Epoch 2164, Loss 2.944852\n",
      "Params: tensor([  5.2896, -16.8623])\n",
      "Gradient: tensor([-0.0133,  0.0753])\n",
      "Epoch 2165, Loss 2.944792\n",
      "Params: tensor([  5.2897, -16.8631])\n",
      "Gradient: tensor([-0.0133,  0.0752])\n",
      "Epoch 2166, Loss 2.944736\n",
      "Params: tensor([  5.2898, -16.8638])\n",
      "Gradient: tensor([-0.0133,  0.0750])\n",
      "Epoch 2167, Loss 2.944678\n",
      "Params: tensor([  5.2900, -16.8646])\n",
      "Gradient: tensor([-0.0132,  0.0749])\n",
      "Epoch 2168, Loss 2.944619\n",
      "Params: tensor([  5.2901, -16.8653])\n",
      "Gradient: tensor([-0.0132,  0.0748])\n",
      "Epoch 2169, Loss 2.944562\n",
      "Params: tensor([  5.2902, -16.8661])\n",
      "Gradient: tensor([-0.0132,  0.0747])\n",
      "Epoch 2170, Loss 2.944504\n",
      "Params: tensor([  5.2903, -16.8668])\n",
      "Gradient: tensor([-0.0132,  0.0745])\n",
      "Epoch 2171, Loss 2.944447\n",
      "Params: tensor([  5.2905, -16.8676])\n",
      "Gradient: tensor([-0.0132,  0.0744])\n",
      "Epoch 2172, Loss 2.944391\n",
      "Params: tensor([  5.2906, -16.8683])\n",
      "Gradient: tensor([-0.0131,  0.0743])\n",
      "Epoch 2173, Loss 2.944333\n",
      "Params: tensor([  5.2907, -16.8690])\n",
      "Gradient: tensor([-0.0131,  0.0742])\n",
      "Epoch 2174, Loss 2.944276\n",
      "Params: tensor([  5.2909, -16.8698])\n",
      "Gradient: tensor([-0.0131,  0.0740])\n",
      "Epoch 2175, Loss 2.944220\n",
      "Params: tensor([  5.2910, -16.8705])\n",
      "Gradient: tensor([-0.0131,  0.0739])\n",
      "Epoch 2176, Loss 2.944164\n",
      "Params: tensor([  5.2911, -16.8713])\n",
      "Gradient: tensor([-0.0130,  0.0738])\n",
      "Epoch 2177, Loss 2.944108\n",
      "Params: tensor([  5.2913, -16.8720])\n",
      "Gradient: tensor([-0.0130,  0.0736])\n",
      "Epoch 2178, Loss 2.944052\n",
      "Params: tensor([  5.2914, -16.8727])\n",
      "Gradient: tensor([-0.0130,  0.0735])\n",
      "Epoch 2179, Loss 2.943996\n",
      "Params: tensor([  5.2915, -16.8735])\n",
      "Gradient: tensor([-0.0130,  0.0734])\n",
      "Epoch 2180, Loss 2.943941\n",
      "Params: tensor([  5.2917, -16.8742])\n",
      "Gradient: tensor([-0.0129,  0.0733])\n",
      "Epoch 2181, Loss 2.943886\n",
      "Params: tensor([  5.2918, -16.8749])\n",
      "Gradient: tensor([-0.0129,  0.0731])\n",
      "Epoch 2182, Loss 2.943831\n",
      "Params: tensor([  5.2919, -16.8757])\n",
      "Gradient: tensor([-0.0129,  0.0730])\n",
      "Epoch 2183, Loss 2.943775\n",
      "Params: tensor([  5.2920, -16.8764])\n",
      "Gradient: tensor([-0.0129,  0.0729])\n",
      "Epoch 2184, Loss 2.943721\n",
      "Params: tensor([  5.2922, -16.8771])\n",
      "Gradient: tensor([-0.0129,  0.0728])\n",
      "Epoch 2185, Loss 2.943666\n",
      "Params: tensor([  5.2923, -16.8778])\n",
      "Gradient: tensor([-0.0128,  0.0727])\n",
      "Epoch 2186, Loss 2.943613\n",
      "Params: tensor([  5.2924, -16.8786])\n",
      "Gradient: tensor([-0.0128,  0.0725])\n",
      "Epoch 2187, Loss 2.943558\n",
      "Params: tensor([  5.2926, -16.8793])\n",
      "Gradient: tensor([-0.0128,  0.0724])\n",
      "Epoch 2188, Loss 2.943504\n",
      "Params: tensor([  5.2927, -16.8800])\n",
      "Gradient: tensor([-0.0128,  0.0723])\n",
      "Epoch 2189, Loss 2.943451\n",
      "Params: tensor([  5.2928, -16.8807])\n",
      "Gradient: tensor([-0.0127,  0.0722])\n",
      "Epoch 2190, Loss 2.943395\n",
      "Params: tensor([  5.2929, -16.8815])\n",
      "Gradient: tensor([-0.0127,  0.0720])\n",
      "Epoch 2191, Loss 2.943343\n",
      "Params: tensor([  5.2931, -16.8822])\n",
      "Gradient: tensor([-0.0127,  0.0719])\n",
      "Epoch 2192, Loss 2.943290\n",
      "Params: tensor([  5.2932, -16.8829])\n",
      "Gradient: tensor([-0.0127,  0.0718])\n",
      "Epoch 2193, Loss 2.943235\n",
      "Params: tensor([  5.2933, -16.8836])\n",
      "Gradient: tensor([-0.0127,  0.0717])\n",
      "Epoch 2194, Loss 2.943183\n",
      "Params: tensor([  5.2934, -16.8843])\n",
      "Gradient: tensor([-0.0126,  0.0715])\n",
      "Epoch 2195, Loss 2.943130\n",
      "Params: tensor([  5.2936, -16.8850])\n",
      "Gradient: tensor([-0.0126,  0.0714])\n",
      "Epoch 2196, Loss 2.943079\n",
      "Params: tensor([  5.2937, -16.8857])\n",
      "Gradient: tensor([-0.0126,  0.0713])\n",
      "Epoch 2197, Loss 2.943026\n",
      "Params: tensor([  5.2938, -16.8865])\n",
      "Gradient: tensor([-0.0126,  0.0712])\n",
      "Epoch 2198, Loss 2.942973\n",
      "Params: tensor([  5.2939, -16.8872])\n",
      "Gradient: tensor([-0.0126,  0.0711])\n",
      "Epoch 2199, Loss 2.942922\n",
      "Params: tensor([  5.2941, -16.8879])\n",
      "Gradient: tensor([-0.0125,  0.0709])\n",
      "Epoch 2200, Loss 2.942870\n",
      "Params: tensor([  5.2942, -16.8886])\n",
      "Gradient: tensor([-0.0125,  0.0708])\n",
      "Epoch 2201, Loss 2.942818\n",
      "Params: tensor([  5.2943, -16.8893])\n",
      "Gradient: tensor([-0.0125,  0.0707])\n",
      "Epoch 2202, Loss 2.942766\n",
      "Params: tensor([  5.2944, -16.8900])\n",
      "Gradient: tensor([-0.0125,  0.0706])\n",
      "Epoch 2203, Loss 2.942715\n",
      "Params: tensor([  5.2946, -16.8907])\n",
      "Gradient: tensor([-0.0124,  0.0705])\n",
      "Epoch 2204, Loss 2.942665\n",
      "Params: tensor([  5.2947, -16.8914])\n",
      "Gradient: tensor([-0.0124,  0.0703])\n",
      "Epoch 2205, Loss 2.942612\n",
      "Params: tensor([  5.2948, -16.8921])\n",
      "Gradient: tensor([-0.0124,  0.0702])\n",
      "Epoch 2206, Loss 2.942563\n",
      "Params: tensor([  5.2949, -16.8928])\n",
      "Gradient: tensor([-0.0124,  0.0701])\n",
      "Epoch 2207, Loss 2.942510\n",
      "Params: tensor([  5.2951, -16.8935])\n",
      "Gradient: tensor([-0.0124,  0.0700])\n",
      "Epoch 2208, Loss 2.942461\n",
      "Params: tensor([  5.2952, -16.8942])\n",
      "Gradient: tensor([-0.0123,  0.0699])\n",
      "Epoch 2209, Loss 2.942411\n",
      "Params: tensor([  5.2953, -16.8949])\n",
      "Gradient: tensor([-0.0123,  0.0697])\n",
      "Epoch 2210, Loss 2.942361\n",
      "Params: tensor([  5.2954, -16.8956])\n",
      "Gradient: tensor([-0.0123,  0.0696])\n",
      "Epoch 2211, Loss 2.942310\n",
      "Params: tensor([  5.2956, -16.8963])\n",
      "Gradient: tensor([-0.0123,  0.0695])\n",
      "Epoch 2212, Loss 2.942261\n",
      "Params: tensor([  5.2957, -16.8970])\n",
      "Gradient: tensor([-0.0122,  0.0694])\n",
      "Epoch 2213, Loss 2.942211\n",
      "Params: tensor([  5.2958, -16.8977])\n",
      "Gradient: tensor([-0.0122,  0.0693])\n",
      "Epoch 2214, Loss 2.942162\n",
      "Params: tensor([  5.2959, -16.8984])\n",
      "Gradient: tensor([-0.0122,  0.0692])\n",
      "Epoch 2215, Loss 2.942112\n",
      "Params: tensor([  5.2960, -16.8991])\n",
      "Gradient: tensor([-0.0122,  0.0690])\n",
      "Epoch 2216, Loss 2.942062\n",
      "Params: tensor([  5.2962, -16.8998])\n",
      "Gradient: tensor([-0.0122,  0.0689])\n",
      "Epoch 2217, Loss 2.942015\n",
      "Params: tensor([  5.2963, -16.9004])\n",
      "Gradient: tensor([-0.0121,  0.0688])\n",
      "Epoch 2218, Loss 2.941966\n",
      "Params: tensor([  5.2964, -16.9011])\n",
      "Gradient: tensor([-0.0121,  0.0687])\n",
      "Epoch 2219, Loss 2.941918\n",
      "Params: tensor([  5.2965, -16.9018])\n",
      "Gradient: tensor([-0.0121,  0.0686])\n",
      "Epoch 2220, Loss 2.941868\n",
      "Params: tensor([  5.2967, -16.9025])\n",
      "Gradient: tensor([-0.0121,  0.0685])\n",
      "Epoch 2221, Loss 2.941821\n",
      "Params: tensor([  5.2968, -16.9032])\n",
      "Gradient: tensor([-0.0121,  0.0683])\n",
      "Epoch 2222, Loss 2.941773\n",
      "Params: tensor([  5.2969, -16.9039])\n",
      "Gradient: tensor([-0.0120,  0.0682])\n",
      "Epoch 2223, Loss 2.941724\n",
      "Params: tensor([  5.2970, -16.9046])\n",
      "Gradient: tensor([-0.0120,  0.0681])\n",
      "Epoch 2224, Loss 2.941677\n",
      "Params: tensor([  5.2971, -16.9052])\n",
      "Gradient: tensor([-0.0120,  0.0680])\n",
      "Epoch 2225, Loss 2.941628\n",
      "Params: tensor([  5.2973, -16.9059])\n",
      "Gradient: tensor([-0.0120,  0.0679])\n",
      "Epoch 2226, Loss 2.941582\n",
      "Params: tensor([  5.2974, -16.9066])\n",
      "Gradient: tensor([-0.0120,  0.0678])\n",
      "Epoch 2227, Loss 2.941535\n",
      "Params: tensor([  5.2975, -16.9073])\n",
      "Gradient: tensor([-0.0119,  0.0676])\n",
      "Epoch 2228, Loss 2.941488\n",
      "Params: tensor([  5.2976, -16.9079])\n",
      "Gradient: tensor([-0.0119,  0.0675])\n",
      "Epoch 2229, Loss 2.941440\n",
      "Params: tensor([  5.2977, -16.9086])\n",
      "Gradient: tensor([-0.0119,  0.0674])\n",
      "Epoch 2230, Loss 2.941393\n",
      "Params: tensor([  5.2979, -16.9093])\n",
      "Gradient: tensor([-0.0119,  0.0673])\n",
      "Epoch 2231, Loss 2.941346\n",
      "Params: tensor([  5.2980, -16.9100])\n",
      "Gradient: tensor([-0.0119,  0.0672])\n",
      "Epoch 2232, Loss 2.941299\n",
      "Params: tensor([  5.2981, -16.9106])\n",
      "Gradient: tensor([-0.0118,  0.0671])\n",
      "Epoch 2233, Loss 2.941252\n",
      "Params: tensor([  5.2982, -16.9113])\n",
      "Gradient: tensor([-0.0118,  0.0670])\n",
      "Epoch 2234, Loss 2.941206\n",
      "Params: tensor([  5.2983, -16.9120])\n",
      "Gradient: tensor([-0.0118,  0.0668])\n",
      "Epoch 2235, Loss 2.941163\n",
      "Params: tensor([  5.2984, -16.9126])\n",
      "Gradient: tensor([-0.0118,  0.0667])\n",
      "Epoch 2236, Loss 2.941115\n",
      "Params: tensor([  5.2986, -16.9133])\n",
      "Gradient: tensor([-0.0118,  0.0666])\n",
      "Epoch 2237, Loss 2.941070\n",
      "Params: tensor([  5.2987, -16.9140])\n",
      "Gradient: tensor([-0.0117,  0.0665])\n",
      "Epoch 2238, Loss 2.941025\n",
      "Params: tensor([  5.2988, -16.9146])\n",
      "Gradient: tensor([-0.0117,  0.0664])\n",
      "Epoch 2239, Loss 2.940979\n",
      "Params: tensor([  5.2989, -16.9153])\n",
      "Gradient: tensor([-0.0117,  0.0663])\n",
      "Epoch 2240, Loss 2.940933\n",
      "Params: tensor([  5.2990, -16.9160])\n",
      "Gradient: tensor([-0.0117,  0.0662])\n",
      "Epoch 2241, Loss 2.940890\n",
      "Params: tensor([  5.2991, -16.9166])\n",
      "Gradient: tensor([-0.0117,  0.0661])\n",
      "Epoch 2242, Loss 2.940843\n",
      "Params: tensor([  5.2993, -16.9173])\n",
      "Gradient: tensor([-0.0117,  0.0659])\n",
      "Epoch 2243, Loss 2.940798\n",
      "Params: tensor([  5.2994, -16.9179])\n",
      "Gradient: tensor([-0.0116,  0.0658])\n",
      "Epoch 2244, Loss 2.940753\n",
      "Params: tensor([  5.2995, -16.9186])\n",
      "Gradient: tensor([-0.0116,  0.0657])\n",
      "Epoch 2245, Loss 2.940711\n",
      "Params: tensor([  5.2996, -16.9192])\n",
      "Gradient: tensor([-0.0116,  0.0656])\n",
      "Epoch 2246, Loss 2.940666\n",
      "Params: tensor([  5.2997, -16.9199])\n",
      "Gradient: tensor([-0.0116,  0.0655])\n",
      "Epoch 2247, Loss 2.940621\n",
      "Params: tensor([  5.2998, -16.9206])\n",
      "Gradient: tensor([-0.0115,  0.0654])\n",
      "Epoch 2248, Loss 2.940576\n",
      "Params: tensor([  5.3000, -16.9212])\n",
      "Gradient: tensor([-0.0115,  0.0653])\n",
      "Epoch 2249, Loss 2.940533\n",
      "Params: tensor([  5.3001, -16.9219])\n",
      "Gradient: tensor([-0.0115,  0.0652])\n",
      "Epoch 2250, Loss 2.940489\n",
      "Params: tensor([  5.3002, -16.9225])\n",
      "Gradient: tensor([-0.0115,  0.0650])\n",
      "Epoch 2251, Loss 2.940446\n",
      "Params: tensor([  5.3003, -16.9232])\n",
      "Gradient: tensor([-0.0115,  0.0649])\n",
      "Epoch 2252, Loss 2.940403\n",
      "Params: tensor([  5.3004, -16.9238])\n",
      "Gradient: tensor([-0.0114,  0.0648])\n",
      "Epoch 2253, Loss 2.940358\n",
      "Params: tensor([  5.3005, -16.9245])\n",
      "Gradient: tensor([-0.0114,  0.0647])\n",
      "Epoch 2254, Loss 2.940316\n",
      "Params: tensor([  5.3006, -16.9251])\n",
      "Gradient: tensor([-0.0114,  0.0646])\n",
      "Epoch 2255, Loss 2.940274\n",
      "Params: tensor([  5.3008, -16.9257])\n",
      "Gradient: tensor([-0.0114,  0.0645])\n",
      "Epoch 2256, Loss 2.940229\n",
      "Params: tensor([  5.3009, -16.9264])\n",
      "Gradient: tensor([-0.0114,  0.0644])\n",
      "Epoch 2257, Loss 2.940187\n",
      "Params: tensor([  5.3010, -16.9270])\n",
      "Gradient: tensor([-0.0114,  0.0643])\n",
      "Epoch 2258, Loss 2.940144\n",
      "Params: tensor([  5.3011, -16.9277])\n",
      "Gradient: tensor([-0.0114,  0.0642])\n",
      "Epoch 2259, Loss 2.940102\n",
      "Params: tensor([  5.3012, -16.9283])\n",
      "Gradient: tensor([-0.0113,  0.0641])\n",
      "Epoch 2260, Loss 2.940060\n",
      "Params: tensor([  5.3013, -16.9290])\n",
      "Gradient: tensor([-0.0113,  0.0640])\n",
      "Epoch 2261, Loss 2.940018\n",
      "Params: tensor([  5.3014, -16.9296])\n",
      "Gradient: tensor([-0.0113,  0.0638])\n",
      "Epoch 2262, Loss 2.939977\n",
      "Params: tensor([  5.3016, -16.9302])\n",
      "Gradient: tensor([-0.0113,  0.0637])\n",
      "Epoch 2263, Loss 2.939934\n",
      "Params: tensor([  5.3017, -16.9309])\n",
      "Gradient: tensor([-0.0112,  0.0636])\n",
      "Epoch 2264, Loss 2.939892\n",
      "Params: tensor([  5.3018, -16.9315])\n",
      "Gradient: tensor([-0.0112,  0.0635])\n",
      "Epoch 2265, Loss 2.939851\n",
      "Params: tensor([  5.3019, -16.9321])\n",
      "Gradient: tensor([-0.0112,  0.0634])\n",
      "Epoch 2266, Loss 2.939809\n",
      "Params: tensor([  5.3020, -16.9328])\n",
      "Gradient: tensor([-0.0112,  0.0633])\n",
      "Epoch 2267, Loss 2.939770\n",
      "Params: tensor([  5.3021, -16.9334])\n",
      "Gradient: tensor([-0.0112,  0.0632])\n",
      "Epoch 2268, Loss 2.939727\n",
      "Params: tensor([  5.3022, -16.9340])\n",
      "Gradient: tensor([-0.0111,  0.0631])\n",
      "Epoch 2269, Loss 2.939686\n",
      "Params: tensor([  5.3023, -16.9347])\n",
      "Gradient: tensor([-0.0111,  0.0630])\n",
      "Epoch 2270, Loss 2.939646\n",
      "Params: tensor([  5.3024, -16.9353])\n",
      "Gradient: tensor([-0.0111,  0.0629])\n",
      "Epoch 2271, Loss 2.939605\n",
      "Params: tensor([  5.3026, -16.9359])\n",
      "Gradient: tensor([-0.0111,  0.0628])\n",
      "Epoch 2272, Loss 2.939566\n",
      "Params: tensor([  5.3027, -16.9365])\n",
      "Gradient: tensor([-0.0111,  0.0627])\n",
      "Epoch 2273, Loss 2.939522\n",
      "Params: tensor([  5.3028, -16.9372])\n",
      "Gradient: tensor([-0.0111,  0.0626])\n",
      "Epoch 2274, Loss 2.939483\n",
      "Params: tensor([  5.3029, -16.9378])\n",
      "Gradient: tensor([-0.0110,  0.0624])\n",
      "Epoch 2275, Loss 2.939443\n",
      "Params: tensor([  5.3030, -16.9384])\n",
      "Gradient: tensor([-0.0110,  0.0623])\n",
      "Epoch 2276, Loss 2.939403\n",
      "Params: tensor([  5.3031, -16.9390])\n",
      "Gradient: tensor([-0.0110,  0.0622])\n",
      "Epoch 2277, Loss 2.939361\n",
      "Params: tensor([  5.3032, -16.9397])\n",
      "Gradient: tensor([-0.0110,  0.0621])\n",
      "Epoch 2278, Loss 2.939323\n",
      "Params: tensor([  5.3033, -16.9403])\n",
      "Gradient: tensor([-0.0110,  0.0620])\n",
      "Epoch 2279, Loss 2.939282\n",
      "Params: tensor([  5.3034, -16.9409])\n",
      "Gradient: tensor([-0.0109,  0.0619])\n",
      "Epoch 2280, Loss 2.939243\n",
      "Params: tensor([  5.3035, -16.9415])\n",
      "Gradient: tensor([-0.0109,  0.0618])\n",
      "Epoch 2281, Loss 2.939205\n",
      "Params: tensor([  5.3037, -16.9421])\n",
      "Gradient: tensor([-0.0109,  0.0617])\n",
      "Epoch 2282, Loss 2.939165\n",
      "Params: tensor([  5.3038, -16.9428])\n",
      "Gradient: tensor([-0.0109,  0.0616])\n",
      "Epoch 2283, Loss 2.939127\n",
      "Params: tensor([  5.3039, -16.9434])\n",
      "Gradient: tensor([-0.0109,  0.0615])\n",
      "Epoch 2284, Loss 2.939087\n",
      "Params: tensor([  5.3040, -16.9440])\n",
      "Gradient: tensor([-0.0108,  0.0614])\n",
      "Epoch 2285, Loss 2.939049\n",
      "Params: tensor([  5.3041, -16.9446])\n",
      "Gradient: tensor([-0.0108,  0.0613])\n",
      "Epoch 2286, Loss 2.939011\n",
      "Params: tensor([  5.3042, -16.9452])\n",
      "Gradient: tensor([-0.0108,  0.0612])\n",
      "Epoch 2287, Loss 2.938971\n",
      "Params: tensor([  5.3043, -16.9458])\n",
      "Gradient: tensor([-0.0108,  0.0611])\n",
      "Epoch 2288, Loss 2.938933\n",
      "Params: tensor([  5.3044, -16.9464])\n",
      "Gradient: tensor([-0.0108,  0.0610])\n",
      "Epoch 2289, Loss 2.938893\n",
      "Params: tensor([  5.3045, -16.9470])\n",
      "Gradient: tensor([-0.0108,  0.0609])\n",
      "Epoch 2290, Loss 2.938857\n",
      "Params: tensor([  5.3046, -16.9476])\n",
      "Gradient: tensor([-0.0107,  0.0608])\n",
      "Epoch 2291, Loss 2.938820\n",
      "Params: tensor([  5.3047, -16.9482])\n",
      "Gradient: tensor([-0.0107,  0.0607])\n",
      "Epoch 2292, Loss 2.938779\n",
      "Params: tensor([  5.3048, -16.9489])\n",
      "Gradient: tensor([-0.0107,  0.0606])\n",
      "Epoch 2293, Loss 2.938744\n",
      "Params: tensor([  5.3049, -16.9495])\n",
      "Gradient: tensor([-0.0107,  0.0605])\n",
      "Epoch 2294, Loss 2.938705\n",
      "Params: tensor([  5.3051, -16.9501])\n",
      "Gradient: tensor([-0.0107,  0.0604])\n",
      "Epoch 2295, Loss 2.938667\n",
      "Params: tensor([  5.3052, -16.9507])\n",
      "Gradient: tensor([-0.0106,  0.0603])\n",
      "Epoch 2296, Loss 2.938629\n",
      "Params: tensor([  5.3053, -16.9513])\n",
      "Gradient: tensor([-0.0106,  0.0602])\n",
      "Epoch 2297, Loss 2.938593\n",
      "Params: tensor([  5.3054, -16.9519])\n",
      "Gradient: tensor([-0.0106,  0.0601])\n",
      "Epoch 2298, Loss 2.938555\n",
      "Params: tensor([  5.3055, -16.9525])\n",
      "Gradient: tensor([-0.0106,  0.0600])\n",
      "Epoch 2299, Loss 2.938519\n",
      "Params: tensor([  5.3056, -16.9531])\n",
      "Gradient: tensor([-0.0106,  0.0599])\n",
      "Epoch 2300, Loss 2.938481\n",
      "Params: tensor([  5.3057, -16.9537])\n",
      "Gradient: tensor([-0.0106,  0.0597])\n",
      "Epoch 2301, Loss 2.938444\n",
      "Params: tensor([  5.3058, -16.9543])\n",
      "Gradient: tensor([-0.0105,  0.0596])\n",
      "Epoch 2302, Loss 2.938409\n",
      "Params: tensor([  5.3059, -16.9549])\n",
      "Gradient: tensor([-0.0105,  0.0595])\n",
      "Epoch 2303, Loss 2.938371\n",
      "Params: tensor([  5.3060, -16.9554])\n",
      "Gradient: tensor([-0.0105,  0.0594])\n",
      "Epoch 2304, Loss 2.938335\n",
      "Params: tensor([  5.3061, -16.9560])\n",
      "Gradient: tensor([-0.0105,  0.0593])\n",
      "Epoch 2305, Loss 2.938299\n",
      "Params: tensor([  5.3062, -16.9566])\n",
      "Gradient: tensor([-0.0105,  0.0592])\n",
      "Epoch 2306, Loss 2.938262\n",
      "Params: tensor([  5.3063, -16.9572])\n",
      "Gradient: tensor([-0.0105,  0.0591])\n",
      "Epoch 2307, Loss 2.938227\n",
      "Params: tensor([  5.3064, -16.9578])\n",
      "Gradient: tensor([-0.0104,  0.0590])\n",
      "Epoch 2308, Loss 2.938190\n",
      "Params: tensor([  5.3065, -16.9584])\n",
      "Gradient: tensor([-0.0104,  0.0589])\n",
      "Epoch 2309, Loss 2.938155\n",
      "Params: tensor([  5.3066, -16.9590])\n",
      "Gradient: tensor([-0.0104,  0.0588])\n",
      "Epoch 2310, Loss 2.938118\n",
      "Params: tensor([  5.3067, -16.9596])\n",
      "Gradient: tensor([-0.0104,  0.0587])\n",
      "Epoch 2311, Loss 2.938084\n",
      "Params: tensor([  5.3068, -16.9602])\n",
      "Gradient: tensor([-0.0104,  0.0586])\n",
      "Epoch 2312, Loss 2.938049\n",
      "Params: tensor([  5.3069, -16.9608])\n",
      "Gradient: tensor([-0.0103,  0.0585])\n",
      "Epoch 2313, Loss 2.938014\n",
      "Params: tensor([  5.3070, -16.9613])\n",
      "Gradient: tensor([-0.0103,  0.0584])\n",
      "Epoch 2314, Loss 2.937977\n",
      "Params: tensor([  5.3072, -16.9619])\n",
      "Gradient: tensor([-0.0103,  0.0583])\n",
      "Epoch 2315, Loss 2.937944\n",
      "Params: tensor([  5.3073, -16.9625])\n",
      "Gradient: tensor([-0.0103,  0.0582])\n",
      "Epoch 2316, Loss 2.937908\n",
      "Params: tensor([  5.3074, -16.9631])\n",
      "Gradient: tensor([-0.0103,  0.0581])\n",
      "Epoch 2317, Loss 2.937872\n",
      "Params: tensor([  5.3075, -16.9637])\n",
      "Gradient: tensor([-0.0103,  0.0580])\n",
      "Epoch 2318, Loss 2.937839\n",
      "Params: tensor([  5.3076, -16.9642])\n",
      "Gradient: tensor([-0.0102,  0.0580])\n",
      "Epoch 2319, Loss 2.937804\n",
      "Params: tensor([  5.3077, -16.9648])\n",
      "Gradient: tensor([-0.0102,  0.0578])\n",
      "Epoch 2320, Loss 2.937769\n",
      "Params: tensor([  5.3078, -16.9654])\n",
      "Gradient: tensor([-0.0102,  0.0578])\n",
      "Epoch 2321, Loss 2.937734\n",
      "Params: tensor([  5.3079, -16.9660])\n",
      "Gradient: tensor([-0.0102,  0.0577])\n",
      "Epoch 2322, Loss 2.937700\n",
      "Params: tensor([  5.3080, -16.9666])\n",
      "Gradient: tensor([-0.0102,  0.0576])\n",
      "Epoch 2323, Loss 2.937665\n",
      "Params: tensor([  5.3081, -16.9671])\n",
      "Gradient: tensor([-0.0102,  0.0575])\n",
      "Epoch 2324, Loss 2.937632\n",
      "Params: tensor([  5.3082, -16.9677])\n",
      "Gradient: tensor([-0.0101,  0.0574])\n",
      "Epoch 2325, Loss 2.937598\n",
      "Params: tensor([  5.3083, -16.9683])\n",
      "Gradient: tensor([-0.0101,  0.0573])\n",
      "Epoch 2326, Loss 2.937565\n",
      "Params: tensor([  5.3084, -16.9688])\n",
      "Gradient: tensor([-0.0101,  0.0572])\n",
      "Epoch 2327, Loss 2.937531\n",
      "Params: tensor([  5.3085, -16.9694])\n",
      "Gradient: tensor([-0.0101,  0.0571])\n",
      "Epoch 2328, Loss 2.937499\n",
      "Params: tensor([  5.3086, -16.9700])\n",
      "Gradient: tensor([-0.0101,  0.0570])\n",
      "Epoch 2329, Loss 2.937465\n",
      "Params: tensor([  5.3087, -16.9706])\n",
      "Gradient: tensor([-0.0101,  0.0569])\n",
      "Epoch 2330, Loss 2.937430\n",
      "Params: tensor([  5.3088, -16.9711])\n",
      "Gradient: tensor([-0.0100,  0.0568])\n",
      "Epoch 2331, Loss 2.937398\n",
      "Params: tensor([  5.3089, -16.9717])\n",
      "Gradient: tensor([-0.0100,  0.0567])\n",
      "Epoch 2332, Loss 2.937364\n",
      "Params: tensor([  5.3090, -16.9723])\n",
      "Gradient: tensor([-0.0100,  0.0566])\n",
      "Epoch 2333, Loss 2.937332\n",
      "Params: tensor([  5.3091, -16.9728])\n",
      "Gradient: tensor([-0.0100,  0.0565])\n",
      "Epoch 2334, Loss 2.937299\n",
      "Params: tensor([  5.3092, -16.9734])\n",
      "Gradient: tensor([-0.0100,  0.0564])\n",
      "Epoch 2335, Loss 2.937265\n",
      "Params: tensor([  5.3093, -16.9739])\n",
      "Gradient: tensor([-0.0100,  0.0563])\n",
      "Epoch 2336, Loss 2.937233\n",
      "Params: tensor([  5.3094, -16.9745])\n",
      "Gradient: tensor([-0.0099,  0.0562])\n",
      "Epoch 2337, Loss 2.937201\n",
      "Params: tensor([  5.3095, -16.9751])\n",
      "Gradient: tensor([-0.0099,  0.0561])\n",
      "Epoch 2338, Loss 2.937167\n",
      "Params: tensor([  5.3096, -16.9756])\n",
      "Gradient: tensor([-0.0099,  0.0560])\n",
      "Epoch 2339, Loss 2.937134\n",
      "Params: tensor([  5.3097, -16.9762])\n",
      "Gradient: tensor([-0.0099,  0.0559])\n",
      "Epoch 2340, Loss 2.937104\n",
      "Params: tensor([  5.3098, -16.9767])\n",
      "Gradient: tensor([-0.0099,  0.0558])\n",
      "Epoch 2341, Loss 2.937071\n",
      "Params: tensor([  5.3099, -16.9773])\n",
      "Gradient: tensor([-0.0098,  0.0557])\n",
      "Epoch 2342, Loss 2.937039\n",
      "Params: tensor([  5.3100, -16.9779])\n",
      "Gradient: tensor([-0.0098,  0.0556])\n",
      "Epoch 2343, Loss 2.937008\n",
      "Params: tensor([  5.3101, -16.9784])\n",
      "Gradient: tensor([-0.0098,  0.0555])\n",
      "Epoch 2344, Loss 2.936976\n",
      "Params: tensor([  5.3102, -16.9790])\n",
      "Gradient: tensor([-0.0098,  0.0554])\n",
      "Epoch 2345, Loss 2.936945\n",
      "Params: tensor([  5.3103, -16.9795])\n",
      "Gradient: tensor([-0.0098,  0.0553])\n",
      "Epoch 2346, Loss 2.936912\n",
      "Params: tensor([  5.3104, -16.9801])\n",
      "Gradient: tensor([-0.0098,  0.0553])\n",
      "Epoch 2347, Loss 2.936882\n",
      "Params: tensor([  5.3105, -16.9806])\n",
      "Gradient: tensor([-0.0097,  0.0552])\n",
      "Epoch 2348, Loss 2.936850\n",
      "Params: tensor([  5.3106, -16.9812])\n",
      "Gradient: tensor([-0.0097,  0.0551])\n",
      "Epoch 2349, Loss 2.936819\n",
      "Params: tensor([  5.3107, -16.9817])\n",
      "Gradient: tensor([-0.0097,  0.0550])\n",
      "Epoch 2350, Loss 2.936788\n",
      "Params: tensor([  5.3107, -16.9823])\n",
      "Gradient: tensor([-0.0097,  0.0549])\n",
      "Epoch 2351, Loss 2.936757\n",
      "Params: tensor([  5.3108, -16.9828])\n",
      "Gradient: tensor([-0.0097,  0.0548])\n",
      "Epoch 2352, Loss 2.936725\n",
      "Params: tensor([  5.3109, -16.9834])\n",
      "Gradient: tensor([-0.0097,  0.0547])\n",
      "Epoch 2353, Loss 2.936694\n",
      "Params: tensor([  5.3110, -16.9839])\n",
      "Gradient: tensor([-0.0096,  0.0546])\n",
      "Epoch 2354, Loss 2.936665\n",
      "Params: tensor([  5.3111, -16.9845])\n",
      "Gradient: tensor([-0.0096,  0.0545])\n",
      "Epoch 2355, Loss 2.936633\n",
      "Params: tensor([  5.3112, -16.9850])\n",
      "Gradient: tensor([-0.0096,  0.0544])\n",
      "Epoch 2356, Loss 2.936602\n",
      "Params: tensor([  5.3113, -16.9856])\n",
      "Gradient: tensor([-0.0096,  0.0543])\n",
      "Epoch 2357, Loss 2.936572\n",
      "Params: tensor([  5.3114, -16.9861])\n",
      "Gradient: tensor([-0.0096,  0.0542])\n",
      "Epoch 2358, Loss 2.936542\n",
      "Params: tensor([  5.3115, -16.9866])\n",
      "Gradient: tensor([-0.0095,  0.0541])\n",
      "Epoch 2359, Loss 2.936511\n",
      "Params: tensor([  5.3116, -16.9872])\n",
      "Gradient: tensor([-0.0096,  0.0540])\n",
      "Epoch 2360, Loss 2.936481\n",
      "Params: tensor([  5.3117, -16.9877])\n",
      "Gradient: tensor([-0.0095,  0.0540])\n",
      "Epoch 2361, Loss 2.936451\n",
      "Params: tensor([  5.3118, -16.9883])\n",
      "Gradient: tensor([-0.0095,  0.0539])\n",
      "Epoch 2362, Loss 2.936421\n",
      "Params: tensor([  5.3119, -16.9888])\n",
      "Gradient: tensor([-0.0095,  0.0538])\n",
      "Epoch 2363, Loss 2.936392\n",
      "Params: tensor([  5.3120, -16.9893])\n",
      "Gradient: tensor([-0.0095,  0.0537])\n",
      "Epoch 2364, Loss 2.936362\n",
      "Params: tensor([  5.3121, -16.9899])\n",
      "Gradient: tensor([-0.0094,  0.0536])\n",
      "Epoch 2365, Loss 2.936332\n",
      "Params: tensor([  5.3122, -16.9904])\n",
      "Gradient: tensor([-0.0094,  0.0535])\n",
      "Epoch 2366, Loss 2.936304\n",
      "Params: tensor([  5.3123, -16.9909])\n",
      "Gradient: tensor([-0.0094,  0.0534])\n",
      "Epoch 2367, Loss 2.936274\n",
      "Params: tensor([  5.3124, -16.9915])\n",
      "Gradient: tensor([-0.0094,  0.0533])\n",
      "Epoch 2368, Loss 2.936244\n",
      "Params: tensor([  5.3125, -16.9920])\n",
      "Gradient: tensor([-0.0094,  0.0532])\n",
      "Epoch 2369, Loss 2.936216\n",
      "Params: tensor([  5.3126, -16.9925])\n",
      "Gradient: tensor([-0.0094,  0.0531])\n",
      "Epoch 2370, Loss 2.936188\n",
      "Params: tensor([  5.3127, -16.9931])\n",
      "Gradient: tensor([-0.0094,  0.0530])\n",
      "Epoch 2371, Loss 2.936156\n",
      "Params: tensor([  5.3127, -16.9936])\n",
      "Gradient: tensor([-0.0094,  0.0530])\n",
      "Epoch 2372, Loss 2.936128\n",
      "Params: tensor([  5.3128, -16.9941])\n",
      "Gradient: tensor([-0.0093,  0.0529])\n",
      "Epoch 2373, Loss 2.936100\n",
      "Params: tensor([  5.3129, -16.9946])\n",
      "Gradient: tensor([-0.0093,  0.0528])\n",
      "Epoch 2374, Loss 2.936072\n",
      "Params: tensor([  5.3130, -16.9952])\n",
      "Gradient: tensor([-0.0093,  0.0527])\n",
      "Epoch 2375, Loss 2.936042\n",
      "Params: tensor([  5.3131, -16.9957])\n",
      "Gradient: tensor([-0.0093,  0.0526])\n",
      "Epoch 2376, Loss 2.936014\n",
      "Params: tensor([  5.3132, -16.9962])\n",
      "Gradient: tensor([-0.0093,  0.0525])\n",
      "Epoch 2377, Loss 2.935986\n",
      "Params: tensor([  5.3133, -16.9967])\n",
      "Gradient: tensor([-0.0093,  0.0524])\n",
      "Epoch 2378, Loss 2.935957\n",
      "Params: tensor([  5.3134, -16.9973])\n",
      "Gradient: tensor([-0.0093,  0.0523])\n",
      "Epoch 2379, Loss 2.935928\n",
      "Params: tensor([  5.3135, -16.9978])\n",
      "Gradient: tensor([-0.0092,  0.0522])\n",
      "Epoch 2380, Loss 2.935901\n",
      "Params: tensor([  5.3136, -16.9983])\n",
      "Gradient: tensor([-0.0092,  0.0522])\n",
      "Epoch 2381, Loss 2.935873\n",
      "Params: tensor([  5.3137, -16.9988])\n",
      "Gradient: tensor([-0.0092,  0.0521])\n",
      "Epoch 2382, Loss 2.935845\n",
      "Params: tensor([  5.3138, -16.9994])\n",
      "Gradient: tensor([-0.0092,  0.0520])\n",
      "Epoch 2383, Loss 2.935817\n",
      "Params: tensor([  5.3139, -16.9999])\n",
      "Gradient: tensor([-0.0092,  0.0519])\n",
      "Epoch 2384, Loss 2.935789\n",
      "Params: tensor([  5.3139, -17.0004])\n",
      "Gradient: tensor([-0.0092,  0.0518])\n",
      "Epoch 2385, Loss 2.935761\n",
      "Params: tensor([  5.3140, -17.0009])\n",
      "Gradient: tensor([-0.0092,  0.0517])\n",
      "Epoch 2386, Loss 2.935734\n",
      "Params: tensor([  5.3141, -17.0014])\n",
      "Gradient: tensor([-0.0091,  0.0516])\n",
      "Epoch 2387, Loss 2.935707\n",
      "Params: tensor([  5.3142, -17.0019])\n",
      "Gradient: tensor([-0.0091,  0.0515])\n",
      "Epoch 2388, Loss 2.935679\n",
      "Params: tensor([  5.3143, -17.0025])\n",
      "Gradient: tensor([-0.0091,  0.0514])\n",
      "Epoch 2389, Loss 2.935650\n",
      "Params: tensor([  5.3144, -17.0030])\n",
      "Gradient: tensor([-0.0091,  0.0514])\n",
      "Epoch 2390, Loss 2.935626\n",
      "Params: tensor([  5.3145, -17.0035])\n",
      "Gradient: tensor([-0.0090,  0.0513])\n",
      "Epoch 2391, Loss 2.935596\n",
      "Params: tensor([  5.3146, -17.0040])\n",
      "Gradient: tensor([-0.0090,  0.0512])\n",
      "Epoch 2392, Loss 2.935571\n",
      "Params: tensor([  5.3147, -17.0045])\n",
      "Gradient: tensor([-0.0090,  0.0511])\n",
      "Epoch 2393, Loss 2.935544\n",
      "Params: tensor([  5.3148, -17.0050])\n",
      "Gradient: tensor([-0.0090,  0.0510])\n",
      "Epoch 2394, Loss 2.935516\n",
      "Params: tensor([  5.3149, -17.0055])\n",
      "Gradient: tensor([-0.0090,  0.0509])\n",
      "Epoch 2395, Loss 2.935489\n",
      "Params: tensor([  5.3149, -17.0060])\n",
      "Gradient: tensor([-0.0090,  0.0508])\n",
      "Epoch 2396, Loss 2.935464\n",
      "Params: tensor([  5.3150, -17.0065])\n",
      "Gradient: tensor([-0.0090,  0.0507])\n",
      "Epoch 2397, Loss 2.935436\n",
      "Params: tensor([  5.3151, -17.0070])\n",
      "Gradient: tensor([-0.0090,  0.0507])\n",
      "Epoch 2398, Loss 2.935411\n",
      "Params: tensor([  5.3152, -17.0076])\n",
      "Gradient: tensor([-0.0089,  0.0506])\n",
      "Epoch 2399, Loss 2.935385\n",
      "Params: tensor([  5.3153, -17.0081])\n",
      "Gradient: tensor([-0.0089,  0.0505])\n",
      "Epoch 2400, Loss 2.935357\n",
      "Params: tensor([  5.3154, -17.0086])\n",
      "Gradient: tensor([-0.0089,  0.0504])\n",
      "Epoch 2401, Loss 2.935332\n",
      "Params: tensor([  5.3155, -17.0091])\n",
      "Gradient: tensor([-0.0089,  0.0503])\n",
      "Epoch 2402, Loss 2.935305\n",
      "Params: tensor([  5.3156, -17.0096])\n",
      "Gradient: tensor([-0.0089,  0.0502])\n",
      "Epoch 2403, Loss 2.935281\n",
      "Params: tensor([  5.3157, -17.0101])\n",
      "Gradient: tensor([-0.0088,  0.0502])\n",
      "Epoch 2404, Loss 2.935252\n",
      "Params: tensor([  5.3157, -17.0106])\n",
      "Gradient: tensor([-0.0088,  0.0501])\n",
      "Epoch 2405, Loss 2.935228\n",
      "Params: tensor([  5.3158, -17.0111])\n",
      "Gradient: tensor([-0.0088,  0.0500])\n",
      "Epoch 2406, Loss 2.935204\n",
      "Params: tensor([  5.3159, -17.0116])\n",
      "Gradient: tensor([-0.0088,  0.0499])\n",
      "Epoch 2407, Loss 2.935177\n",
      "Params: tensor([  5.3160, -17.0121])\n",
      "Gradient: tensor([-0.0088,  0.0498])\n",
      "Epoch 2408, Loss 2.935151\n",
      "Params: tensor([  5.3161, -17.0126])\n",
      "Gradient: tensor([-0.0088,  0.0497])\n",
      "Epoch 2409, Loss 2.935126\n",
      "Params: tensor([  5.3162, -17.0131])\n",
      "Gradient: tensor([-0.0088,  0.0496])\n",
      "Epoch 2410, Loss 2.935100\n",
      "Params: tensor([  5.3163, -17.0136])\n",
      "Gradient: tensor([-0.0088,  0.0496])\n",
      "Epoch 2411, Loss 2.935075\n",
      "Params: tensor([  5.3164, -17.0140])\n",
      "Gradient: tensor([-0.0087,  0.0495])\n",
      "Epoch 2412, Loss 2.935049\n",
      "Params: tensor([  5.3164, -17.0145])\n",
      "Gradient: tensor([-0.0087,  0.0494])\n",
      "Epoch 2413, Loss 2.935024\n",
      "Params: tensor([  5.3165, -17.0150])\n",
      "Gradient: tensor([-0.0087,  0.0493])\n",
      "Epoch 2414, Loss 2.935001\n",
      "Params: tensor([  5.3166, -17.0155])\n",
      "Gradient: tensor([-0.0087,  0.0492])\n",
      "Epoch 2415, Loss 2.934973\n",
      "Params: tensor([  5.3167, -17.0160])\n",
      "Gradient: tensor([-0.0087,  0.0491])\n",
      "Epoch 2416, Loss 2.934949\n",
      "Params: tensor([  5.3168, -17.0165])\n",
      "Gradient: tensor([-0.0087,  0.0491])\n",
      "Epoch 2417, Loss 2.934925\n",
      "Params: tensor([  5.3169, -17.0170])\n",
      "Gradient: tensor([-0.0086,  0.0490])\n",
      "Epoch 2418, Loss 2.934899\n",
      "Params: tensor([  5.3170, -17.0175])\n",
      "Gradient: tensor([-0.0086,  0.0489])\n",
      "Epoch 2419, Loss 2.934876\n",
      "Params: tensor([  5.3171, -17.0180])\n",
      "Gradient: tensor([-0.0086,  0.0488])\n",
      "Epoch 2420, Loss 2.934852\n",
      "Params: tensor([  5.3171, -17.0185])\n",
      "Gradient: tensor([-0.0086,  0.0487])\n",
      "Epoch 2421, Loss 2.934826\n",
      "Params: tensor([  5.3172, -17.0189])\n",
      "Gradient: tensor([-0.0086,  0.0486])\n",
      "Epoch 2422, Loss 2.934802\n",
      "Params: tensor([  5.3173, -17.0194])\n",
      "Gradient: tensor([-0.0086,  0.0486])\n",
      "Epoch 2423, Loss 2.934777\n",
      "Params: tensor([  5.3174, -17.0199])\n",
      "Gradient: tensor([-0.0086,  0.0485])\n",
      "Epoch 2424, Loss 2.934753\n",
      "Params: tensor([  5.3175, -17.0204])\n",
      "Gradient: tensor([-0.0086,  0.0484])\n",
      "Epoch 2425, Loss 2.934730\n",
      "Params: tensor([  5.3176, -17.0209])\n",
      "Gradient: tensor([-0.0086,  0.0483])\n",
      "Epoch 2426, Loss 2.934705\n",
      "Params: tensor([  5.3177, -17.0214])\n",
      "Gradient: tensor([-0.0085,  0.0482])\n",
      "Epoch 2427, Loss 2.934681\n",
      "Params: tensor([  5.3177, -17.0219])\n",
      "Gradient: tensor([-0.0085,  0.0481])\n",
      "Epoch 2428, Loss 2.934657\n",
      "Params: tensor([  5.3178, -17.0223])\n",
      "Gradient: tensor([-0.0085,  0.0481])\n",
      "Epoch 2429, Loss 2.934635\n",
      "Params: tensor([  5.3179, -17.0228])\n",
      "Gradient: tensor([-0.0085,  0.0480])\n",
      "Epoch 2430, Loss 2.934609\n",
      "Params: tensor([  5.3180, -17.0233])\n",
      "Gradient: tensor([-0.0085,  0.0479])\n",
      "Epoch 2431, Loss 2.934585\n",
      "Params: tensor([  5.3181, -17.0238])\n",
      "Gradient: tensor([-0.0084,  0.0478])\n",
      "Epoch 2432, Loss 2.934563\n",
      "Params: tensor([  5.3182, -17.0242])\n",
      "Gradient: tensor([-0.0084,  0.0477])\n",
      "Epoch 2433, Loss 2.934541\n",
      "Params: tensor([  5.3182, -17.0247])\n",
      "Gradient: tensor([-0.0084,  0.0477])\n",
      "Epoch 2434, Loss 2.934516\n",
      "Params: tensor([  5.3183, -17.0252])\n",
      "Gradient: tensor([-0.0084,  0.0476])\n",
      "Epoch 2435, Loss 2.934493\n",
      "Params: tensor([  5.3184, -17.0257])\n",
      "Gradient: tensor([-0.0084,  0.0475])\n",
      "Epoch 2436, Loss 2.934469\n",
      "Params: tensor([  5.3185, -17.0261])\n",
      "Gradient: tensor([-0.0084,  0.0474])\n",
      "Epoch 2437, Loss 2.934446\n",
      "Params: tensor([  5.3186, -17.0266])\n",
      "Gradient: tensor([-0.0084,  0.0473])\n",
      "Epoch 2438, Loss 2.934423\n",
      "Params: tensor([  5.3187, -17.0271])\n",
      "Gradient: tensor([-0.0083,  0.0473])\n",
      "Epoch 2439, Loss 2.934400\n",
      "Params: tensor([  5.3187, -17.0276])\n",
      "Gradient: tensor([-0.0083,  0.0472])\n",
      "Epoch 2440, Loss 2.934377\n",
      "Params: tensor([  5.3188, -17.0280])\n",
      "Gradient: tensor([-0.0083,  0.0471])\n",
      "Epoch 2441, Loss 2.934355\n",
      "Params: tensor([  5.3189, -17.0285])\n",
      "Gradient: tensor([-0.0083,  0.0470])\n",
      "Epoch 2442, Loss 2.934331\n",
      "Params: tensor([  5.3190, -17.0290])\n",
      "Gradient: tensor([-0.0083,  0.0469])\n",
      "Epoch 2443, Loss 2.934309\n",
      "Params: tensor([  5.3191, -17.0294])\n",
      "Gradient: tensor([-0.0083,  0.0469])\n",
      "Epoch 2444, Loss 2.934287\n",
      "Params: tensor([  5.3192, -17.0299])\n",
      "Gradient: tensor([-0.0083,  0.0468])\n",
      "Epoch 2445, Loss 2.934264\n",
      "Params: tensor([  5.3192, -17.0304])\n",
      "Gradient: tensor([-0.0083,  0.0467])\n",
      "Epoch 2446, Loss 2.934242\n",
      "Params: tensor([  5.3193, -17.0308])\n",
      "Gradient: tensor([-0.0083,  0.0466])\n",
      "Epoch 2447, Loss 2.934219\n",
      "Params: tensor([  5.3194, -17.0313])\n",
      "Gradient: tensor([-0.0082,  0.0465])\n",
      "Epoch 2448, Loss 2.934198\n",
      "Params: tensor([  5.3195, -17.0318])\n",
      "Gradient: tensor([-0.0082,  0.0465])\n",
      "Epoch 2449, Loss 2.934175\n",
      "Params: tensor([  5.3196, -17.0322])\n",
      "Gradient: tensor([-0.0082,  0.0464])\n",
      "Epoch 2450, Loss 2.934151\n",
      "Params: tensor([  5.3197, -17.0327])\n",
      "Gradient: tensor([-0.0082,  0.0463])\n",
      "Epoch 2451, Loss 2.934129\n",
      "Params: tensor([  5.3197, -17.0332])\n",
      "Gradient: tensor([-0.0082,  0.0462])\n",
      "Epoch 2452, Loss 2.934108\n",
      "Params: tensor([  5.3198, -17.0336])\n",
      "Gradient: tensor([-0.0082,  0.0461])\n",
      "Epoch 2453, Loss 2.934084\n",
      "Params: tensor([  5.3199, -17.0341])\n",
      "Gradient: tensor([-0.0081,  0.0461])\n",
      "Epoch 2454, Loss 2.934065\n",
      "Params: tensor([  5.3200, -17.0345])\n",
      "Gradient: tensor([-0.0081,  0.0460])\n",
      "Epoch 2455, Loss 2.934043\n",
      "Params: tensor([  5.3201, -17.0350])\n",
      "Gradient: tensor([-0.0081,  0.0459])\n",
      "Epoch 2456, Loss 2.934020\n",
      "Params: tensor([  5.3201, -17.0355])\n",
      "Gradient: tensor([-0.0081,  0.0458])\n",
      "Epoch 2457, Loss 2.934000\n",
      "Params: tensor([  5.3202, -17.0359])\n",
      "Gradient: tensor([-0.0081,  0.0457])\n",
      "Epoch 2458, Loss 2.933978\n",
      "Params: tensor([  5.3203, -17.0364])\n",
      "Gradient: tensor([-0.0081,  0.0457])\n",
      "Epoch 2459, Loss 2.933956\n",
      "Params: tensor([  5.3204, -17.0368])\n",
      "Gradient: tensor([-0.0080,  0.0456])\n",
      "Epoch 2460, Loss 2.933935\n",
      "Params: tensor([  5.3205, -17.0373])\n",
      "Gradient: tensor([-0.0080,  0.0455])\n",
      "Epoch 2461, Loss 2.933913\n",
      "Params: tensor([  5.3205, -17.0377])\n",
      "Gradient: tensor([-0.0080,  0.0454])\n",
      "Epoch 2462, Loss 2.933893\n",
      "Params: tensor([  5.3206, -17.0382])\n",
      "Gradient: tensor([-0.0080,  0.0454])\n",
      "Epoch 2463, Loss 2.933871\n",
      "Params: tensor([  5.3207, -17.0386])\n",
      "Gradient: tensor([-0.0080,  0.0453])\n",
      "Epoch 2464, Loss 2.933849\n",
      "Params: tensor([  5.3208, -17.0391])\n",
      "Gradient: tensor([-0.0080,  0.0452])\n",
      "Epoch 2465, Loss 2.933828\n",
      "Params: tensor([  5.3209, -17.0396])\n",
      "Gradient: tensor([-0.0080,  0.0451])\n",
      "Epoch 2466, Loss 2.933806\n",
      "Params: tensor([  5.3209, -17.0400])\n",
      "Gradient: tensor([-0.0080,  0.0451])\n",
      "Epoch 2467, Loss 2.933787\n",
      "Params: tensor([  5.3210, -17.0405])\n",
      "Gradient: tensor([-0.0079,  0.0450])\n",
      "Epoch 2468, Loss 2.933766\n",
      "Params: tensor([  5.3211, -17.0409])\n",
      "Gradient: tensor([-0.0079,  0.0449])\n",
      "Epoch 2469, Loss 2.933745\n",
      "Params: tensor([  5.3212, -17.0413])\n",
      "Gradient: tensor([-0.0079,  0.0448])\n",
      "Epoch 2470, Loss 2.933723\n",
      "Params: tensor([  5.3213, -17.0418])\n",
      "Gradient: tensor([-0.0079,  0.0448])\n",
      "Epoch 2471, Loss 2.933704\n",
      "Params: tensor([  5.3213, -17.0422])\n",
      "Gradient: tensor([-0.0079,  0.0447])\n",
      "Epoch 2472, Loss 2.933682\n",
      "Params: tensor([  5.3214, -17.0427])\n",
      "Gradient: tensor([-0.0079,  0.0446])\n",
      "Epoch 2473, Loss 2.933663\n",
      "Params: tensor([  5.3215, -17.0431])\n",
      "Gradient: tensor([-0.0079,  0.0445])\n",
      "Epoch 2474, Loss 2.933643\n",
      "Params: tensor([  5.3216, -17.0436])\n",
      "Gradient: tensor([-0.0079,  0.0444])\n",
      "Epoch 2475, Loss 2.933622\n",
      "Params: tensor([  5.3217, -17.0440])\n",
      "Gradient: tensor([-0.0078,  0.0444])\n",
      "Epoch 2476, Loss 2.933602\n",
      "Params: tensor([  5.3217, -17.0445])\n",
      "Gradient: tensor([-0.0078,  0.0443])\n",
      "Epoch 2477, Loss 2.933583\n",
      "Params: tensor([  5.3218, -17.0449])\n",
      "Gradient: tensor([-0.0078,  0.0442])\n",
      "Epoch 2478, Loss 2.933561\n",
      "Params: tensor([  5.3219, -17.0453])\n",
      "Gradient: tensor([-0.0078,  0.0441])\n",
      "Epoch 2479, Loss 2.933541\n",
      "Params: tensor([  5.3220, -17.0458])\n",
      "Gradient: tensor([-0.0078,  0.0441])\n",
      "Epoch 2480, Loss 2.933522\n",
      "Params: tensor([  5.3220, -17.0462])\n",
      "Gradient: tensor([-0.0078,  0.0440])\n",
      "Epoch 2481, Loss 2.933501\n",
      "Params: tensor([  5.3221, -17.0467])\n",
      "Gradient: tensor([-0.0078,  0.0439])\n",
      "Epoch 2482, Loss 2.933480\n",
      "Params: tensor([  5.3222, -17.0471])\n",
      "Gradient: tensor([-0.0077,  0.0438])\n",
      "Epoch 2483, Loss 2.933463\n",
      "Params: tensor([  5.3223, -17.0475])\n",
      "Gradient: tensor([-0.0077,  0.0438])\n",
      "Epoch 2484, Loss 2.933442\n",
      "Params: tensor([  5.3224, -17.0480])\n",
      "Gradient: tensor([-0.0077,  0.0437])\n",
      "Epoch 2485, Loss 2.933422\n",
      "Params: tensor([  5.3224, -17.0484])\n",
      "Gradient: tensor([-0.0077,  0.0436])\n",
      "Epoch 2486, Loss 2.933403\n",
      "Params: tensor([  5.3225, -17.0489])\n",
      "Gradient: tensor([-0.0077,  0.0436])\n",
      "Epoch 2487, Loss 2.933382\n",
      "Params: tensor([  5.3226, -17.0493])\n",
      "Gradient: tensor([-0.0077,  0.0435])\n",
      "Epoch 2488, Loss 2.933365\n",
      "Params: tensor([  5.3227, -17.0497])\n",
      "Gradient: tensor([-0.0077,  0.0434])\n",
      "Epoch 2489, Loss 2.933345\n",
      "Params: tensor([  5.3227, -17.0502])\n",
      "Gradient: tensor([-0.0077,  0.0433])\n",
      "Epoch 2490, Loss 2.933325\n",
      "Params: tensor([  5.3228, -17.0506])\n",
      "Gradient: tensor([-0.0076,  0.0433])\n",
      "Epoch 2491, Loss 2.933306\n",
      "Params: tensor([  5.3229, -17.0510])\n",
      "Gradient: tensor([-0.0076,  0.0432])\n",
      "Epoch 2492, Loss 2.933287\n",
      "Params: tensor([  5.3230, -17.0515])\n",
      "Gradient: tensor([-0.0076,  0.0431])\n",
      "Epoch 2493, Loss 2.933266\n",
      "Params: tensor([  5.3230, -17.0519])\n",
      "Gradient: tensor([-0.0076,  0.0430])\n",
      "Epoch 2494, Loss 2.933249\n",
      "Params: tensor([  5.3231, -17.0523])\n",
      "Gradient: tensor([-0.0076,  0.0430])\n",
      "Epoch 2495, Loss 2.933228\n",
      "Params: tensor([  5.3232, -17.0527])\n",
      "Gradient: tensor([-0.0076,  0.0429])\n",
      "Epoch 2496, Loss 2.933209\n",
      "Params: tensor([  5.3233, -17.0532])\n",
      "Gradient: tensor([-0.0076,  0.0428])\n",
      "Epoch 2497, Loss 2.933191\n",
      "Params: tensor([  5.3233, -17.0536])\n",
      "Gradient: tensor([-0.0075,  0.0427])\n",
      "Epoch 2498, Loss 2.933172\n",
      "Params: tensor([  5.3234, -17.0540])\n",
      "Gradient: tensor([-0.0075,  0.0427])\n",
      "Epoch 2499, Loss 2.933154\n",
      "Params: tensor([  5.3235, -17.0544])\n",
      "Gradient: tensor([-0.0075,  0.0426])\n",
      "Epoch 2500, Loss 2.933134\n",
      "Params: tensor([  5.3236, -17.0549])\n",
      "Gradient: tensor([-0.0075,  0.0425])\n",
      "Epoch 2501, Loss 2.933116\n",
      "Params: tensor([  5.3236, -17.0553])\n",
      "Gradient: tensor([-0.0075,  0.0425])\n",
      "Epoch 2502, Loss 2.933096\n",
      "Params: tensor([  5.3237, -17.0557])\n",
      "Gradient: tensor([-0.0075,  0.0424])\n",
      "Epoch 2503, Loss 2.933079\n",
      "Params: tensor([  5.3238, -17.0561])\n",
      "Gradient: tensor([-0.0075,  0.0423])\n",
      "Epoch 2504, Loss 2.933060\n",
      "Params: tensor([  5.3239, -17.0566])\n",
      "Gradient: tensor([-0.0075,  0.0422])\n",
      "Epoch 2505, Loss 2.933043\n",
      "Params: tensor([  5.3239, -17.0570])\n",
      "Gradient: tensor([-0.0074,  0.0422])\n",
      "Epoch 2506, Loss 2.933025\n",
      "Params: tensor([  5.3240, -17.0574])\n",
      "Gradient: tensor([-0.0074,  0.0421])\n",
      "Epoch 2507, Loss 2.933007\n",
      "Params: tensor([  5.3241, -17.0578])\n",
      "Gradient: tensor([-0.0074,  0.0420])\n",
      "Epoch 2508, Loss 2.932988\n",
      "Params: tensor([  5.3242, -17.0582])\n",
      "Gradient: tensor([-0.0074,  0.0420])\n",
      "Epoch 2509, Loss 2.932971\n",
      "Params: tensor([  5.3242, -17.0587])\n",
      "Gradient: tensor([-0.0074,  0.0419])\n",
      "Epoch 2510, Loss 2.932952\n",
      "Params: tensor([  5.3243, -17.0591])\n",
      "Gradient: tensor([-0.0074,  0.0418])\n",
      "Epoch 2511, Loss 2.932932\n",
      "Params: tensor([  5.3244, -17.0595])\n",
      "Gradient: tensor([-0.0074,  0.0417])\n",
      "Epoch 2512, Loss 2.932915\n",
      "Params: tensor([  5.3245, -17.0599])\n",
      "Gradient: tensor([-0.0073,  0.0417])\n",
      "Epoch 2513, Loss 2.932898\n",
      "Params: tensor([  5.3245, -17.0603])\n",
      "Gradient: tensor([-0.0073,  0.0416])\n",
      "Epoch 2514, Loss 2.932880\n",
      "Params: tensor([  5.3246, -17.0608])\n",
      "Gradient: tensor([-0.0073,  0.0415])\n",
      "Epoch 2515, Loss 2.932862\n",
      "Params: tensor([  5.3247, -17.0612])\n",
      "Gradient: tensor([-0.0073,  0.0415])\n",
      "Epoch 2516, Loss 2.932846\n",
      "Params: tensor([  5.3248, -17.0616])\n",
      "Gradient: tensor([-0.0073,  0.0414])\n",
      "Epoch 2517, Loss 2.932826\n",
      "Params: tensor([  5.3248, -17.0620])\n",
      "Gradient: tensor([-0.0073,  0.0413])\n",
      "Epoch 2518, Loss 2.932810\n",
      "Params: tensor([  5.3249, -17.0624])\n",
      "Gradient: tensor([-0.0073,  0.0412])\n",
      "Epoch 2519, Loss 2.932791\n",
      "Params: tensor([  5.3250, -17.0628])\n",
      "Gradient: tensor([-0.0073,  0.0412])\n",
      "Epoch 2520, Loss 2.932774\n",
      "Params: tensor([  5.3250, -17.0632])\n",
      "Gradient: tensor([-0.0073,  0.0411])\n",
      "Epoch 2521, Loss 2.932758\n",
      "Params: tensor([  5.3251, -17.0636])\n",
      "Gradient: tensor([-0.0073,  0.0410])\n",
      "Epoch 2522, Loss 2.932739\n",
      "Params: tensor([  5.3252, -17.0640])\n",
      "Gradient: tensor([-0.0073,  0.0410])\n",
      "Epoch 2523, Loss 2.932723\n",
      "Params: tensor([  5.3253, -17.0645])\n",
      "Gradient: tensor([-0.0072,  0.0409])\n",
      "Epoch 2524, Loss 2.932706\n",
      "Params: tensor([  5.3253, -17.0649])\n",
      "Gradient: tensor([-0.0072,  0.0408])\n",
      "Epoch 2525, Loss 2.932689\n",
      "Params: tensor([  5.3254, -17.0653])\n",
      "Gradient: tensor([-0.0072,  0.0408])\n",
      "Epoch 2526, Loss 2.932671\n",
      "Params: tensor([  5.3255, -17.0657])\n",
      "Gradient: tensor([-0.0072,  0.0407])\n",
      "Epoch 2527, Loss 2.932654\n",
      "Params: tensor([  5.3256, -17.0661])\n",
      "Gradient: tensor([-0.0072,  0.0406])\n",
      "Epoch 2528, Loss 2.932637\n",
      "Params: tensor([  5.3256, -17.0665])\n",
      "Gradient: tensor([-0.0072,  0.0405])\n",
      "Epoch 2529, Loss 2.932619\n",
      "Params: tensor([  5.3257, -17.0669])\n",
      "Gradient: tensor([-0.0072,  0.0405])\n",
      "Epoch 2530, Loss 2.932603\n",
      "Params: tensor([  5.3258, -17.0673])\n",
      "Gradient: tensor([-0.0071,  0.0404])\n",
      "Epoch 2531, Loss 2.932585\n",
      "Params: tensor([  5.3258, -17.0677])\n",
      "Gradient: tensor([-0.0071,  0.0403])\n",
      "Epoch 2532, Loss 2.932569\n",
      "Params: tensor([  5.3259, -17.0681])\n",
      "Gradient: tensor([-0.0071,  0.0403])\n",
      "Epoch 2533, Loss 2.932553\n",
      "Params: tensor([  5.3260, -17.0685])\n",
      "Gradient: tensor([-0.0071,  0.0402])\n",
      "Epoch 2534, Loss 2.932535\n",
      "Params: tensor([  5.3261, -17.0689])\n",
      "Gradient: tensor([-0.0071,  0.0401])\n",
      "Epoch 2535, Loss 2.932519\n",
      "Params: tensor([  5.3261, -17.0693])\n",
      "Gradient: tensor([-0.0071,  0.0401])\n",
      "Epoch 2536, Loss 2.932502\n",
      "Params: tensor([  5.3262, -17.0697])\n",
      "Gradient: tensor([-0.0071,  0.0400])\n",
      "Epoch 2537, Loss 2.932487\n",
      "Params: tensor([  5.3263, -17.0701])\n",
      "Gradient: tensor([-0.0071,  0.0399])\n",
      "Epoch 2538, Loss 2.932469\n",
      "Params: tensor([  5.3263, -17.0705])\n",
      "Gradient: tensor([-0.0070,  0.0399])\n",
      "Epoch 2539, Loss 2.932456\n",
      "Params: tensor([  5.3264, -17.0709])\n",
      "Gradient: tensor([-0.0070,  0.0398])\n",
      "Epoch 2540, Loss 2.932438\n",
      "Params: tensor([  5.3265, -17.0713])\n",
      "Gradient: tensor([-0.0070,  0.0397])\n",
      "Epoch 2541, Loss 2.932421\n",
      "Params: tensor([  5.3265, -17.0717])\n",
      "Gradient: tensor([-0.0070,  0.0397])\n",
      "Epoch 2542, Loss 2.932404\n",
      "Params: tensor([  5.3266, -17.0721])\n",
      "Gradient: tensor([-0.0070,  0.0396])\n",
      "Epoch 2543, Loss 2.932387\n",
      "Params: tensor([  5.3267, -17.0725])\n",
      "Gradient: tensor([-0.0070,  0.0395])\n",
      "Epoch 2544, Loss 2.932370\n",
      "Params: tensor([  5.3268, -17.0729])\n",
      "Gradient: tensor([-0.0070,  0.0395])\n",
      "Epoch 2545, Loss 2.932358\n",
      "Params: tensor([  5.3268, -17.0733])\n",
      "Gradient: tensor([-0.0070,  0.0394])\n",
      "Epoch 2546, Loss 2.932340\n",
      "Params: tensor([  5.3269, -17.0737])\n",
      "Gradient: tensor([-0.0069,  0.0393])\n",
      "Epoch 2547, Loss 2.932324\n",
      "Params: tensor([  5.3270, -17.0741])\n",
      "Gradient: tensor([-0.0069,  0.0393])\n",
      "Epoch 2548, Loss 2.932309\n",
      "Params: tensor([  5.3270, -17.0745])\n",
      "Gradient: tensor([-0.0069,  0.0392])\n",
      "Epoch 2549, Loss 2.932293\n",
      "Params: tensor([  5.3271, -17.0749])\n",
      "Gradient: tensor([-0.0069,  0.0391])\n",
      "Epoch 2550, Loss 2.932277\n",
      "Params: tensor([  5.3272, -17.0752])\n",
      "Gradient: tensor([-0.0069,  0.0391])\n",
      "Epoch 2551, Loss 2.932261\n",
      "Params: tensor([  5.3272, -17.0756])\n",
      "Gradient: tensor([-0.0069,  0.0390])\n",
      "Epoch 2552, Loss 2.932246\n",
      "Params: tensor([  5.3273, -17.0760])\n",
      "Gradient: tensor([-0.0069,  0.0389])\n",
      "Epoch 2553, Loss 2.932229\n",
      "Params: tensor([  5.3274, -17.0764])\n",
      "Gradient: tensor([-0.0069,  0.0389])\n",
      "Epoch 2554, Loss 2.932215\n",
      "Params: tensor([  5.3274, -17.0768])\n",
      "Gradient: tensor([-0.0069,  0.0388])\n",
      "Epoch 2555, Loss 2.932198\n",
      "Params: tensor([  5.3275, -17.0772])\n",
      "Gradient: tensor([-0.0068,  0.0387])\n",
      "Epoch 2556, Loss 2.932184\n",
      "Params: tensor([  5.3276, -17.0776])\n",
      "Gradient: tensor([-0.0068,  0.0387])\n",
      "Epoch 2557, Loss 2.932168\n",
      "Params: tensor([  5.3276, -17.0780])\n",
      "Gradient: tensor([-0.0068,  0.0386])\n",
      "Epoch 2558, Loss 2.932153\n",
      "Params: tensor([  5.3277, -17.0783])\n",
      "Gradient: tensor([-0.0068,  0.0385])\n",
      "Epoch 2559, Loss 2.932137\n",
      "Params: tensor([  5.3278, -17.0787])\n",
      "Gradient: tensor([-0.0068,  0.0385])\n",
      "Epoch 2560, Loss 2.932122\n",
      "Params: tensor([  5.3279, -17.0791])\n",
      "Gradient: tensor([-0.0068,  0.0384])\n",
      "Epoch 2561, Loss 2.932107\n",
      "Params: tensor([  5.3279, -17.0795])\n",
      "Gradient: tensor([-0.0068,  0.0383])\n",
      "Epoch 2562, Loss 2.932092\n",
      "Params: tensor([  5.3280, -17.0799])\n",
      "Gradient: tensor([-0.0068,  0.0383])\n",
      "Epoch 2563, Loss 2.932076\n",
      "Params: tensor([  5.3281, -17.0803])\n",
      "Gradient: tensor([-0.0067,  0.0382])\n",
      "Epoch 2564, Loss 2.932061\n",
      "Params: tensor([  5.3281, -17.0806])\n",
      "Gradient: tensor([-0.0067,  0.0381])\n",
      "Epoch 2565, Loss 2.932047\n",
      "Params: tensor([  5.3282, -17.0810])\n",
      "Gradient: tensor([-0.0067,  0.0381])\n",
      "Epoch 2566, Loss 2.932031\n",
      "Params: tensor([  5.3283, -17.0814])\n",
      "Gradient: tensor([-0.0067,  0.0380])\n",
      "Epoch 2567, Loss 2.932018\n",
      "Params: tensor([  5.3283, -17.0818])\n",
      "Gradient: tensor([-0.0067,  0.0379])\n",
      "Epoch 2568, Loss 2.932002\n",
      "Params: tensor([  5.3284, -17.0822])\n",
      "Gradient: tensor([-0.0067,  0.0379])\n",
      "Epoch 2569, Loss 2.931986\n",
      "Params: tensor([  5.3285, -17.0825])\n",
      "Gradient: tensor([-0.0067,  0.0378])\n",
      "Epoch 2570, Loss 2.931972\n",
      "Params: tensor([  5.3285, -17.0829])\n",
      "Gradient: tensor([-0.0067,  0.0378])\n",
      "Epoch 2571, Loss 2.931957\n",
      "Params: tensor([  5.3286, -17.0833])\n",
      "Gradient: tensor([-0.0067,  0.0377])\n",
      "Epoch 2572, Loss 2.931941\n",
      "Params: tensor([  5.3287, -17.0837])\n",
      "Gradient: tensor([-0.0067,  0.0376])\n",
      "Epoch 2573, Loss 2.931929\n",
      "Params: tensor([  5.3287, -17.0840])\n",
      "Gradient: tensor([-0.0066,  0.0376])\n",
      "Epoch 2574, Loss 2.931915\n",
      "Params: tensor([  5.3288, -17.0844])\n",
      "Gradient: tensor([-0.0066,  0.0375])\n",
      "Epoch 2575, Loss 2.931899\n",
      "Params: tensor([  5.3289, -17.0848])\n",
      "Gradient: tensor([-0.0066,  0.0374])\n",
      "Epoch 2576, Loss 2.931885\n",
      "Params: tensor([  5.3289, -17.0852])\n",
      "Gradient: tensor([-0.0066,  0.0374])\n",
      "Epoch 2577, Loss 2.931870\n",
      "Params: tensor([  5.3290, -17.0855])\n",
      "Gradient: tensor([-0.0066,  0.0373])\n",
      "Epoch 2578, Loss 2.931856\n",
      "Params: tensor([  5.3291, -17.0859])\n",
      "Gradient: tensor([-0.0066,  0.0372])\n",
      "Epoch 2579, Loss 2.931843\n",
      "Params: tensor([  5.3291, -17.0863])\n",
      "Gradient: tensor([-0.0066,  0.0372])\n",
      "Epoch 2580, Loss 2.931828\n",
      "Params: tensor([  5.3292, -17.0867])\n",
      "Gradient: tensor([-0.0066,  0.0371])\n",
      "Epoch 2581, Loss 2.931813\n",
      "Params: tensor([  5.3293, -17.0870])\n",
      "Gradient: tensor([-0.0065,  0.0371])\n",
      "Epoch 2582, Loss 2.931799\n",
      "Params: tensor([  5.3293, -17.0874])\n",
      "Gradient: tensor([-0.0065,  0.0370])\n",
      "Epoch 2583, Loss 2.931786\n",
      "Params: tensor([  5.3294, -17.0878])\n",
      "Gradient: tensor([-0.0065,  0.0369])\n",
      "Epoch 2584, Loss 2.931771\n",
      "Params: tensor([  5.3294, -17.0881])\n",
      "Gradient: tensor([-0.0065,  0.0369])\n",
      "Epoch 2585, Loss 2.931759\n",
      "Params: tensor([  5.3295, -17.0885])\n",
      "Gradient: tensor([-0.0065,  0.0368])\n",
      "Epoch 2586, Loss 2.931742\n",
      "Params: tensor([  5.3296, -17.0889])\n",
      "Gradient: tensor([-0.0065,  0.0367])\n",
      "Epoch 2587, Loss 2.931729\n",
      "Params: tensor([  5.3296, -17.0892])\n",
      "Gradient: tensor([-0.0065,  0.0367])\n",
      "Epoch 2588, Loss 2.931717\n",
      "Params: tensor([  5.3297, -17.0896])\n",
      "Gradient: tensor([-0.0065,  0.0366])\n",
      "Epoch 2589, Loss 2.931701\n",
      "Params: tensor([  5.3298, -17.0900])\n",
      "Gradient: tensor([-0.0065,  0.0366])\n",
      "Epoch 2590, Loss 2.931687\n",
      "Params: tensor([  5.3298, -17.0903])\n",
      "Gradient: tensor([-0.0065,  0.0365])\n",
      "Epoch 2591, Loss 2.931674\n",
      "Params: tensor([  5.3299, -17.0907])\n",
      "Gradient: tensor([-0.0064,  0.0364])\n",
      "Epoch 2592, Loss 2.931660\n",
      "Params: tensor([  5.3300, -17.0911])\n",
      "Gradient: tensor([-0.0064,  0.0364])\n",
      "Epoch 2593, Loss 2.931648\n",
      "Params: tensor([  5.3300, -17.0914])\n",
      "Gradient: tensor([-0.0064,  0.0363])\n",
      "Epoch 2594, Loss 2.931632\n",
      "Params: tensor([  5.3301, -17.0918])\n",
      "Gradient: tensor([-0.0064,  0.0362])\n",
      "Epoch 2595, Loss 2.931619\n",
      "Params: tensor([  5.3302, -17.0921])\n",
      "Gradient: tensor([-0.0064,  0.0362])\n",
      "Epoch 2596, Loss 2.931606\n",
      "Params: tensor([  5.3302, -17.0925])\n",
      "Gradient: tensor([-0.0064,  0.0361])\n",
      "Epoch 2597, Loss 2.931594\n",
      "Params: tensor([  5.3303, -17.0929])\n",
      "Gradient: tensor([-0.0064,  0.0361])\n",
      "Epoch 2598, Loss 2.931580\n",
      "Params: tensor([  5.3303, -17.0932])\n",
      "Gradient: tensor([-0.0064,  0.0360])\n",
      "Epoch 2599, Loss 2.931566\n",
      "Params: tensor([  5.3304, -17.0936])\n",
      "Gradient: tensor([-0.0064,  0.0359])\n",
      "Epoch 2600, Loss 2.931554\n",
      "Params: tensor([  5.3305, -17.0939])\n",
      "Gradient: tensor([-0.0064,  0.0359])\n",
      "Epoch 2601, Loss 2.931538\n",
      "Params: tensor([  5.3305, -17.0943])\n",
      "Gradient: tensor([-0.0063,  0.0358])\n",
      "Epoch 2602, Loss 2.931526\n",
      "Params: tensor([  5.3306, -17.0947])\n",
      "Gradient: tensor([-0.0063,  0.0358])\n",
      "Epoch 2603, Loss 2.931512\n",
      "Params: tensor([  5.3307, -17.0950])\n",
      "Gradient: tensor([-0.0063,  0.0357])\n",
      "Epoch 2604, Loss 2.931500\n",
      "Params: tensor([  5.3307, -17.0954])\n",
      "Gradient: tensor([-0.0063,  0.0356])\n",
      "Epoch 2605, Loss 2.931488\n",
      "Params: tensor([  5.3308, -17.0957])\n",
      "Gradient: tensor([-0.0063,  0.0356])\n",
      "Epoch 2606, Loss 2.931474\n",
      "Params: tensor([  5.3309, -17.0961])\n",
      "Gradient: tensor([-0.0063,  0.0355])\n",
      "Epoch 2607, Loss 2.931462\n",
      "Params: tensor([  5.3309, -17.0964])\n",
      "Gradient: tensor([-0.0062,  0.0355])\n",
      "Epoch 2608, Loss 2.931448\n",
      "Params: tensor([  5.3310, -17.0968])\n",
      "Gradient: tensor([-0.0062,  0.0354])\n",
      "Epoch 2609, Loss 2.931436\n",
      "Params: tensor([  5.3310, -17.0971])\n",
      "Gradient: tensor([-0.0062,  0.0353])\n",
      "Epoch 2610, Loss 2.931423\n",
      "Params: tensor([  5.3311, -17.0975])\n",
      "Gradient: tensor([-0.0062,  0.0353])\n",
      "Epoch 2611, Loss 2.931411\n",
      "Params: tensor([  5.3312, -17.0979])\n",
      "Gradient: tensor([-0.0062,  0.0352])\n",
      "Epoch 2612, Loss 2.931398\n",
      "Params: tensor([  5.3312, -17.0982])\n",
      "Gradient: tensor([-0.0062,  0.0352])\n",
      "Epoch 2613, Loss 2.931384\n",
      "Params: tensor([  5.3313, -17.0986])\n",
      "Gradient: tensor([-0.0062,  0.0351])\n",
      "Epoch 2614, Loss 2.931371\n",
      "Params: tensor([  5.3313, -17.0989])\n",
      "Gradient: tensor([-0.0062,  0.0350])\n",
      "Epoch 2615, Loss 2.931358\n",
      "Params: tensor([  5.3314, -17.0993])\n",
      "Gradient: tensor([-0.0062,  0.0350])\n",
      "Epoch 2616, Loss 2.931346\n",
      "Params: tensor([  5.3315, -17.0996])\n",
      "Gradient: tensor([-0.0062,  0.0349])\n",
      "Epoch 2617, Loss 2.931334\n",
      "Params: tensor([  5.3315, -17.1000])\n",
      "Gradient: tensor([-0.0062,  0.0349])\n",
      "Epoch 2618, Loss 2.931322\n",
      "Params: tensor([  5.3316, -17.1003])\n",
      "Gradient: tensor([-0.0062,  0.0348])\n",
      "Epoch 2619, Loss 2.931309\n",
      "Params: tensor([  5.3317, -17.1006])\n",
      "Gradient: tensor([-0.0061,  0.0347])\n",
      "Epoch 2620, Loss 2.931296\n",
      "Params: tensor([  5.3317, -17.1010])\n",
      "Gradient: tensor([-0.0061,  0.0347])\n",
      "Epoch 2621, Loss 2.931282\n",
      "Params: tensor([  5.3318, -17.1013])\n",
      "Gradient: tensor([-0.0061,  0.0346])\n",
      "Epoch 2622, Loss 2.931272\n",
      "Params: tensor([  5.3318, -17.1017])\n",
      "Gradient: tensor([-0.0061,  0.0346])\n",
      "Epoch 2623, Loss 2.931259\n",
      "Params: tensor([  5.3319, -17.1020])\n",
      "Gradient: tensor([-0.0061,  0.0345])\n",
      "Epoch 2624, Loss 2.931245\n",
      "Params: tensor([  5.3320, -17.1024])\n",
      "Gradient: tensor([-0.0061,  0.0344])\n",
      "Epoch 2625, Loss 2.931235\n",
      "Params: tensor([  5.3320, -17.1027])\n",
      "Gradient: tensor([-0.0061,  0.0344])\n",
      "Epoch 2626, Loss 2.931222\n",
      "Params: tensor([  5.3321, -17.1031])\n",
      "Gradient: tensor([-0.0061,  0.0343])\n",
      "Epoch 2627, Loss 2.931211\n",
      "Params: tensor([  5.3321, -17.1034])\n",
      "Gradient: tensor([-0.0060,  0.0343])\n",
      "Epoch 2628, Loss 2.931196\n",
      "Params: tensor([  5.3322, -17.1038])\n",
      "Gradient: tensor([-0.0060,  0.0342])\n",
      "Epoch 2629, Loss 2.931185\n",
      "Params: tensor([  5.3323, -17.1041])\n",
      "Gradient: tensor([-0.0060,  0.0342])\n",
      "Epoch 2630, Loss 2.931173\n",
      "Params: tensor([  5.3323, -17.1044])\n",
      "Gradient: tensor([-0.0060,  0.0341])\n",
      "Epoch 2631, Loss 2.931162\n",
      "Params: tensor([  5.3324, -17.1048])\n",
      "Gradient: tensor([-0.0060,  0.0340])\n",
      "Epoch 2632, Loss 2.931149\n",
      "Params: tensor([  5.3324, -17.1051])\n",
      "Gradient: tensor([-0.0060,  0.0340])\n",
      "Epoch 2633, Loss 2.931139\n",
      "Params: tensor([  5.3325, -17.1055])\n",
      "Gradient: tensor([-0.0060,  0.0339])\n",
      "Epoch 2634, Loss 2.931126\n",
      "Params: tensor([  5.3326, -17.1058])\n",
      "Gradient: tensor([-0.0060,  0.0339])\n",
      "Epoch 2635, Loss 2.931114\n",
      "Params: tensor([  5.3326, -17.1061])\n",
      "Gradient: tensor([-0.0060,  0.0338])\n",
      "Epoch 2636, Loss 2.931101\n",
      "Params: tensor([  5.3327, -17.1065])\n",
      "Gradient: tensor([-0.0060,  0.0337])\n",
      "Epoch 2637, Loss 2.931090\n",
      "Params: tensor([  5.3327, -17.1068])\n",
      "Gradient: tensor([-0.0059,  0.0337])\n",
      "Epoch 2638, Loss 2.931078\n",
      "Params: tensor([  5.3328, -17.1071])\n",
      "Gradient: tensor([-0.0059,  0.0336])\n",
      "Epoch 2639, Loss 2.931067\n",
      "Params: tensor([  5.3329, -17.1075])\n",
      "Gradient: tensor([-0.0059,  0.0336])\n",
      "Epoch 2640, Loss 2.931055\n",
      "Params: tensor([  5.3329, -17.1078])\n",
      "Gradient: tensor([-0.0059,  0.0335])\n",
      "Epoch 2641, Loss 2.931044\n",
      "Params: tensor([  5.3330, -17.1081])\n",
      "Gradient: tensor([-0.0059,  0.0335])\n",
      "Epoch 2642, Loss 2.931034\n",
      "Params: tensor([  5.3330, -17.1085])\n",
      "Gradient: tensor([-0.0059,  0.0334])\n",
      "Epoch 2643, Loss 2.931021\n",
      "Params: tensor([  5.3331, -17.1088])\n",
      "Gradient: tensor([-0.0059,  0.0333])\n",
      "Epoch 2644, Loss 2.931010\n",
      "Params: tensor([  5.3332, -17.1091])\n",
      "Gradient: tensor([-0.0059,  0.0333])\n",
      "Epoch 2645, Loss 2.930999\n",
      "Params: tensor([  5.3332, -17.1095])\n",
      "Gradient: tensor([-0.0059,  0.0332])\n",
      "Epoch 2646, Loss 2.930987\n",
      "Params: tensor([  5.3333, -17.1098])\n",
      "Gradient: tensor([-0.0059,  0.0332])\n",
      "Epoch 2647, Loss 2.930976\n",
      "Params: tensor([  5.3333, -17.1101])\n",
      "Gradient: tensor([-0.0059,  0.0331])\n",
      "Epoch 2648, Loss 2.930964\n",
      "Params: tensor([  5.3334, -17.1105])\n",
      "Gradient: tensor([-0.0059,  0.0331])\n",
      "Epoch 2649, Loss 2.930953\n",
      "Params: tensor([  5.3335, -17.1108])\n",
      "Gradient: tensor([-0.0058,  0.0330])\n",
      "Epoch 2650, Loss 2.930941\n",
      "Params: tensor([  5.3335, -17.1111])\n",
      "Gradient: tensor([-0.0058,  0.0330])\n",
      "Epoch 2651, Loss 2.930931\n",
      "Params: tensor([  5.3336, -17.1115])\n",
      "Gradient: tensor([-0.0058,  0.0329])\n",
      "Epoch 2652, Loss 2.930920\n",
      "Params: tensor([  5.3336, -17.1118])\n",
      "Gradient: tensor([-0.0058,  0.0328])\n",
      "Epoch 2653, Loss 2.930908\n",
      "Params: tensor([  5.3337, -17.1121])\n",
      "Gradient: tensor([-0.0058,  0.0328])\n",
      "Epoch 2654, Loss 2.930899\n",
      "Params: tensor([  5.3337, -17.1124])\n",
      "Gradient: tensor([-0.0058,  0.0327])\n",
      "Epoch 2655, Loss 2.930885\n",
      "Params: tensor([  5.3338, -17.1128])\n",
      "Gradient: tensor([-0.0058,  0.0327])\n",
      "Epoch 2656, Loss 2.930876\n",
      "Params: tensor([  5.3339, -17.1131])\n",
      "Gradient: tensor([-0.0058,  0.0326])\n",
      "Epoch 2657, Loss 2.930863\n",
      "Params: tensor([  5.3339, -17.1134])\n",
      "Gradient: tensor([-0.0057,  0.0326])\n",
      "Epoch 2658, Loss 2.930854\n",
      "Params: tensor([  5.3340, -17.1137])\n",
      "Gradient: tensor([-0.0057,  0.0325])\n",
      "Epoch 2659, Loss 2.930841\n",
      "Params: tensor([  5.3340, -17.1141])\n",
      "Gradient: tensor([-0.0057,  0.0325])\n",
      "Epoch 2660, Loss 2.930833\n",
      "Params: tensor([  5.3341, -17.1144])\n",
      "Gradient: tensor([-0.0057,  0.0324])\n",
      "Epoch 2661, Loss 2.930821\n",
      "Params: tensor([  5.3341, -17.1147])\n",
      "Gradient: tensor([-0.0057,  0.0323])\n",
      "Epoch 2662, Loss 2.930811\n",
      "Params: tensor([  5.3342, -17.1150])\n",
      "Gradient: tensor([-0.0057,  0.0323])\n",
      "Epoch 2663, Loss 2.930801\n",
      "Params: tensor([  5.3343, -17.1154])\n",
      "Gradient: tensor([-0.0057,  0.0322])\n",
      "Epoch 2664, Loss 2.930788\n",
      "Params: tensor([  5.3343, -17.1157])\n",
      "Gradient: tensor([-0.0057,  0.0322])\n",
      "Epoch 2665, Loss 2.930778\n",
      "Params: tensor([  5.3344, -17.1160])\n",
      "Gradient: tensor([-0.0057,  0.0321])\n",
      "Epoch 2666, Loss 2.930767\n",
      "Params: tensor([  5.3344, -17.1163])\n",
      "Gradient: tensor([-0.0057,  0.0321])\n",
      "Epoch 2667, Loss 2.930757\n",
      "Params: tensor([  5.3345, -17.1166])\n",
      "Gradient: tensor([-0.0057,  0.0320])\n",
      "Epoch 2668, Loss 2.930746\n",
      "Params: tensor([  5.3345, -17.1170])\n",
      "Gradient: tensor([-0.0056,  0.0320])\n",
      "Epoch 2669, Loss 2.930736\n",
      "Params: tensor([  5.3346, -17.1173])\n",
      "Gradient: tensor([-0.0056,  0.0319])\n",
      "Epoch 2670, Loss 2.930724\n",
      "Params: tensor([  5.3347, -17.1176])\n",
      "Gradient: tensor([-0.0056,  0.0319])\n",
      "Epoch 2671, Loss 2.930715\n",
      "Params: tensor([  5.3347, -17.1179])\n",
      "Gradient: tensor([-0.0056,  0.0318])\n",
      "Epoch 2672, Loss 2.930704\n",
      "Params: tensor([  5.3348, -17.1182])\n",
      "Gradient: tensor([-0.0056,  0.0317])\n",
      "Epoch 2673, Loss 2.930694\n",
      "Params: tensor([  5.3348, -17.1186])\n",
      "Gradient: tensor([-0.0056,  0.0317])\n",
      "Epoch 2674, Loss 2.930685\n",
      "Params: tensor([  5.3349, -17.1189])\n",
      "Gradient: tensor([-0.0056,  0.0316])\n",
      "Epoch 2675, Loss 2.930674\n",
      "Params: tensor([  5.3349, -17.1192])\n",
      "Gradient: tensor([-0.0056,  0.0316])\n",
      "Epoch 2676, Loss 2.930663\n",
      "Params: tensor([  5.3350, -17.1195])\n",
      "Gradient: tensor([-0.0056,  0.0315])\n",
      "Epoch 2677, Loss 2.930654\n",
      "Params: tensor([  5.3350, -17.1198])\n",
      "Gradient: tensor([-0.0056,  0.0315])\n",
      "Epoch 2678, Loss 2.930644\n",
      "Params: tensor([  5.3351, -17.1201])\n",
      "Gradient: tensor([-0.0055,  0.0314])\n",
      "Epoch 2679, Loss 2.930631\n",
      "Params: tensor([  5.3352, -17.1204])\n",
      "Gradient: tensor([-0.0055,  0.0314])\n",
      "Epoch 2680, Loss 2.930621\n",
      "Params: tensor([  5.3352, -17.1208])\n",
      "Gradient: tensor([-0.0055,  0.0313])\n",
      "Epoch 2681, Loss 2.930613\n",
      "Params: tensor([  5.3353, -17.1211])\n",
      "Gradient: tensor([-0.0055,  0.0313])\n",
      "Epoch 2682, Loss 2.930603\n",
      "Params: tensor([  5.3353, -17.1214])\n",
      "Gradient: tensor([-0.0055,  0.0312])\n",
      "Epoch 2683, Loss 2.930593\n",
      "Params: tensor([  5.3354, -17.1217])\n",
      "Gradient: tensor([-0.0055,  0.0312])\n",
      "Epoch 2684, Loss 2.930582\n",
      "Params: tensor([  5.3354, -17.1220])\n",
      "Gradient: tensor([-0.0055,  0.0311])\n",
      "Epoch 2685, Loss 2.930572\n",
      "Params: tensor([  5.3355, -17.1223])\n",
      "Gradient: tensor([-0.0055,  0.0310])\n",
      "Epoch 2686, Loss 2.930562\n",
      "Params: tensor([  5.3355, -17.1226])\n",
      "Gradient: tensor([-0.0055,  0.0310])\n",
      "Epoch 2687, Loss 2.930552\n",
      "Params: tensor([  5.3356, -17.1229])\n",
      "Gradient: tensor([-0.0055,  0.0309])\n",
      "Epoch 2688, Loss 2.930543\n",
      "Params: tensor([  5.3356, -17.1232])\n",
      "Gradient: tensor([-0.0055,  0.0309])\n",
      "Epoch 2689, Loss 2.930534\n",
      "Params: tensor([  5.3357, -17.1236])\n",
      "Gradient: tensor([-0.0055,  0.0308])\n",
      "Epoch 2690, Loss 2.930524\n",
      "Params: tensor([  5.3358, -17.1239])\n",
      "Gradient: tensor([-0.0054,  0.0308])\n",
      "Epoch 2691, Loss 2.930514\n",
      "Params: tensor([  5.3358, -17.1242])\n",
      "Gradient: tensor([-0.0054,  0.0307])\n",
      "Epoch 2692, Loss 2.930502\n",
      "Params: tensor([  5.3359, -17.1245])\n",
      "Gradient: tensor([-0.0054,  0.0307])\n",
      "Epoch 2693, Loss 2.930493\n",
      "Params: tensor([  5.3359, -17.1248])\n",
      "Gradient: tensor([-0.0054,  0.0306])\n",
      "Epoch 2694, Loss 2.930482\n",
      "Params: tensor([  5.3360, -17.1251])\n",
      "Gradient: tensor([-0.0054,  0.0306])\n",
      "Epoch 2695, Loss 2.930474\n",
      "Params: tensor([  5.3360, -17.1254])\n",
      "Gradient: tensor([-0.0054,  0.0305])\n",
      "Epoch 2696, Loss 2.930464\n",
      "Params: tensor([  5.3361, -17.1257])\n",
      "Gradient: tensor([-0.0054,  0.0305])\n",
      "Epoch 2697, Loss 2.930454\n",
      "Params: tensor([  5.3361, -17.1260])\n",
      "Gradient: tensor([-0.0054,  0.0304])\n",
      "Epoch 2698, Loss 2.930445\n",
      "Params: tensor([  5.3362, -17.1263])\n",
      "Gradient: tensor([-0.0054,  0.0304])\n",
      "Epoch 2699, Loss 2.930436\n",
      "Params: tensor([  5.3362, -17.1266])\n",
      "Gradient: tensor([-0.0054,  0.0303])\n",
      "Epoch 2700, Loss 2.930426\n",
      "Params: tensor([  5.3363, -17.1269])\n",
      "Gradient: tensor([-0.0054,  0.0303])\n",
      "Epoch 2701, Loss 2.930416\n",
      "Params: tensor([  5.3364, -17.1272])\n",
      "Gradient: tensor([-0.0054,  0.0302])\n",
      "Epoch 2702, Loss 2.930408\n",
      "Params: tensor([  5.3364, -17.1275])\n",
      "Gradient: tensor([-0.0053,  0.0302])\n",
      "Epoch 2703, Loss 2.930398\n",
      "Params: tensor([  5.3365, -17.1278])\n",
      "Gradient: tensor([-0.0053,  0.0301])\n",
      "Epoch 2704, Loss 2.930388\n",
      "Params: tensor([  5.3365, -17.1281])\n",
      "Gradient: tensor([-0.0053,  0.0301])\n",
      "Epoch 2705, Loss 2.930380\n",
      "Params: tensor([  5.3366, -17.1284])\n",
      "Gradient: tensor([-0.0053,  0.0300])\n",
      "Epoch 2706, Loss 2.930370\n",
      "Params: tensor([  5.3366, -17.1287])\n",
      "Gradient: tensor([-0.0053,  0.0300])\n",
      "Epoch 2707, Loss 2.930360\n",
      "Params: tensor([  5.3367, -17.1290])\n",
      "Gradient: tensor([-0.0053,  0.0299])\n",
      "Epoch 2708, Loss 2.930352\n",
      "Params: tensor([  5.3367, -17.1293])\n",
      "Gradient: tensor([-0.0053,  0.0299])\n",
      "Epoch 2709, Loss 2.930342\n",
      "Params: tensor([  5.3368, -17.1296])\n",
      "Gradient: tensor([-0.0053,  0.0298])\n",
      "Epoch 2710, Loss 2.930334\n",
      "Params: tensor([  5.3368, -17.1299])\n",
      "Gradient: tensor([-0.0053,  0.0298])\n",
      "Epoch 2711, Loss 2.930325\n",
      "Params: tensor([  5.3369, -17.1302])\n",
      "Gradient: tensor([-0.0053,  0.0297])\n",
      "Epoch 2712, Loss 2.930315\n",
      "Params: tensor([  5.3369, -17.1305])\n",
      "Gradient: tensor([-0.0053,  0.0297])\n",
      "Epoch 2713, Loss 2.930306\n",
      "Params: tensor([  5.3370, -17.1308])\n",
      "Gradient: tensor([-0.0052,  0.0296])\n",
      "Epoch 2714, Loss 2.930297\n",
      "Params: tensor([  5.3370, -17.1311])\n",
      "Gradient: tensor([-0.0052,  0.0296])\n",
      "Epoch 2715, Loss 2.930288\n",
      "Params: tensor([  5.3371, -17.1314])\n",
      "Gradient: tensor([-0.0052,  0.0295])\n",
      "Epoch 2716, Loss 2.930279\n",
      "Params: tensor([  5.3371, -17.1317])\n",
      "Gradient: tensor([-0.0052,  0.0295])\n",
      "Epoch 2717, Loss 2.930270\n",
      "Params: tensor([  5.3372, -17.1320])\n",
      "Gradient: tensor([-0.0052,  0.0294])\n",
      "Epoch 2718, Loss 2.930262\n",
      "Params: tensor([  5.3372, -17.1323])\n",
      "Gradient: tensor([-0.0052,  0.0294])\n",
      "Epoch 2719, Loss 2.930254\n",
      "Params: tensor([  5.3373, -17.1326])\n",
      "Gradient: tensor([-0.0052,  0.0293])\n",
      "Epoch 2720, Loss 2.930244\n",
      "Params: tensor([  5.3373, -17.1329])\n",
      "Gradient: tensor([-0.0052,  0.0293])\n",
      "Epoch 2721, Loss 2.930235\n",
      "Params: tensor([  5.3374, -17.1332])\n",
      "Gradient: tensor([-0.0052,  0.0292])\n",
      "Epoch 2722, Loss 2.930226\n",
      "Params: tensor([  5.3375, -17.1334])\n",
      "Gradient: tensor([-0.0052,  0.0292])\n",
      "Epoch 2723, Loss 2.930218\n",
      "Params: tensor([  5.3375, -17.1337])\n",
      "Gradient: tensor([-0.0051,  0.0291])\n",
      "Epoch 2724, Loss 2.930209\n",
      "Params: tensor([  5.3376, -17.1340])\n",
      "Gradient: tensor([-0.0051,  0.0291])\n",
      "Epoch 2725, Loss 2.930201\n",
      "Params: tensor([  5.3376, -17.1343])\n",
      "Gradient: tensor([-0.0051,  0.0290])\n",
      "Epoch 2726, Loss 2.930190\n",
      "Params: tensor([  5.3377, -17.1346])\n",
      "Gradient: tensor([-0.0051,  0.0290])\n",
      "Epoch 2727, Loss 2.930183\n",
      "Params: tensor([  5.3377, -17.1349])\n",
      "Gradient: tensor([-0.0051,  0.0289])\n",
      "Epoch 2728, Loss 2.930173\n",
      "Params: tensor([  5.3378, -17.1352])\n",
      "Gradient: tensor([-0.0051,  0.0289])\n",
      "Epoch 2729, Loss 2.930166\n",
      "Params: tensor([  5.3378, -17.1355])\n",
      "Gradient: tensor([-0.0051,  0.0288])\n",
      "Epoch 2730, Loss 2.930156\n",
      "Params: tensor([  5.3379, -17.1358])\n",
      "Gradient: tensor([-0.0051,  0.0288])\n",
      "Epoch 2731, Loss 2.930149\n",
      "Params: tensor([  5.3379, -17.1360])\n",
      "Gradient: tensor([-0.0051,  0.0287])\n",
      "Epoch 2732, Loss 2.930139\n",
      "Params: tensor([  5.3380, -17.1363])\n",
      "Gradient: tensor([-0.0051,  0.0287])\n",
      "Epoch 2733, Loss 2.930131\n",
      "Params: tensor([  5.3380, -17.1366])\n",
      "Gradient: tensor([-0.0050,  0.0286])\n",
      "Epoch 2734, Loss 2.930123\n",
      "Params: tensor([  5.3381, -17.1369])\n",
      "Gradient: tensor([-0.0050,  0.0286])\n",
      "Epoch 2735, Loss 2.930113\n",
      "Params: tensor([  5.3381, -17.1372])\n",
      "Gradient: tensor([-0.0050,  0.0285])\n",
      "Epoch 2736, Loss 2.930107\n",
      "Params: tensor([  5.3382, -17.1375])\n",
      "Gradient: tensor([-0.0051,  0.0285])\n",
      "Epoch 2737, Loss 2.930099\n",
      "Params: tensor([  5.3382, -17.1378])\n",
      "Gradient: tensor([-0.0050,  0.0284])\n",
      "Epoch 2738, Loss 2.930090\n",
      "Params: tensor([  5.3383, -17.1380])\n",
      "Gradient: tensor([-0.0050,  0.0284])\n",
      "Epoch 2739, Loss 2.930081\n",
      "Params: tensor([  5.3383, -17.1383])\n",
      "Gradient: tensor([-0.0050,  0.0283])\n",
      "Epoch 2740, Loss 2.930073\n",
      "Params: tensor([  5.3384, -17.1386])\n",
      "Gradient: tensor([-0.0050,  0.0283])\n",
      "Epoch 2741, Loss 2.930065\n",
      "Params: tensor([  5.3384, -17.1389])\n",
      "Gradient: tensor([-0.0050,  0.0282])\n",
      "Epoch 2742, Loss 2.930056\n",
      "Params: tensor([  5.3385, -17.1392])\n",
      "Gradient: tensor([-0.0050,  0.0282])\n",
      "Epoch 2743, Loss 2.930048\n",
      "Params: tensor([  5.3385, -17.1395])\n",
      "Gradient: tensor([-0.0050,  0.0281])\n",
      "Epoch 2744, Loss 2.930041\n",
      "Params: tensor([  5.3386, -17.1397])\n",
      "Gradient: tensor([-0.0050,  0.0281])\n",
      "Epoch 2745, Loss 2.930032\n",
      "Params: tensor([  5.3386, -17.1400])\n",
      "Gradient: tensor([-0.0050,  0.0280])\n",
      "Epoch 2746, Loss 2.930022\n",
      "Params: tensor([  5.3387, -17.1403])\n",
      "Gradient: tensor([-0.0050,  0.0280])\n",
      "Epoch 2747, Loss 2.930016\n",
      "Params: tensor([  5.3387, -17.1406])\n",
      "Gradient: tensor([-0.0049,  0.0279])\n",
      "Epoch 2748, Loss 2.930008\n",
      "Params: tensor([  5.3388, -17.1409])\n",
      "Gradient: tensor([-0.0049,  0.0279])\n",
      "Epoch 2749, Loss 2.930000\n",
      "Params: tensor([  5.3388, -17.1411])\n",
      "Gradient: tensor([-0.0049,  0.0279])\n",
      "Epoch 2750, Loss 2.929992\n",
      "Params: tensor([  5.3389, -17.1414])\n",
      "Gradient: tensor([-0.0049,  0.0278])\n",
      "Epoch 2751, Loss 2.929983\n",
      "Params: tensor([  5.3389, -17.1417])\n",
      "Gradient: tensor([-0.0049,  0.0278])\n",
      "Epoch 2752, Loss 2.929975\n",
      "Params: tensor([  5.3390, -17.1420])\n",
      "Gradient: tensor([-0.0049,  0.0277])\n",
      "Epoch 2753, Loss 2.929968\n",
      "Params: tensor([  5.3390, -17.1422])\n",
      "Gradient: tensor([-0.0049,  0.0277])\n",
      "Epoch 2754, Loss 2.929961\n",
      "Params: tensor([  5.3391, -17.1425])\n",
      "Gradient: tensor([-0.0049,  0.0276])\n",
      "Epoch 2755, Loss 2.929953\n",
      "Params: tensor([  5.3391, -17.1428])\n",
      "Gradient: tensor([-0.0049,  0.0276])\n",
      "Epoch 2756, Loss 2.929945\n",
      "Params: tensor([  5.3392, -17.1431])\n",
      "Gradient: tensor([-0.0049,  0.0275])\n",
      "Epoch 2757, Loss 2.929936\n",
      "Params: tensor([  5.3392, -17.1433])\n",
      "Gradient: tensor([-0.0049,  0.0275])\n",
      "Epoch 2758, Loss 2.929929\n",
      "Params: tensor([  5.3392, -17.1436])\n",
      "Gradient: tensor([-0.0049,  0.0274])\n",
      "Epoch 2759, Loss 2.929921\n",
      "Params: tensor([  5.3393, -17.1439])\n",
      "Gradient: tensor([-0.0048,  0.0274])\n",
      "Epoch 2760, Loss 2.929914\n",
      "Params: tensor([  5.3393, -17.1442])\n",
      "Gradient: tensor([-0.0049,  0.0273])\n",
      "Epoch 2761, Loss 2.929905\n",
      "Params: tensor([  5.3394, -17.1444])\n",
      "Gradient: tensor([-0.0048,  0.0273])\n",
      "Epoch 2762, Loss 2.929896\n",
      "Params: tensor([  5.3394, -17.1447])\n",
      "Gradient: tensor([-0.0048,  0.0272])\n",
      "Epoch 2763, Loss 2.929891\n",
      "Params: tensor([  5.3395, -17.1450])\n",
      "Gradient: tensor([-0.0048,  0.0272])\n",
      "Epoch 2764, Loss 2.929882\n",
      "Params: tensor([  5.3395, -17.1453])\n",
      "Gradient: tensor([-0.0048,  0.0271])\n",
      "Epoch 2765, Loss 2.929875\n",
      "Params: tensor([  5.3396, -17.1455])\n",
      "Gradient: tensor([-0.0048,  0.0271])\n",
      "Epoch 2766, Loss 2.929868\n",
      "Params: tensor([  5.3396, -17.1458])\n",
      "Gradient: tensor([-0.0048,  0.0271])\n",
      "Epoch 2767, Loss 2.929859\n",
      "Params: tensor([  5.3397, -17.1461])\n",
      "Gradient: tensor([-0.0048,  0.0270])\n",
      "Epoch 2768, Loss 2.929852\n",
      "Params: tensor([  5.3397, -17.1463])\n",
      "Gradient: tensor([-0.0048,  0.0270])\n",
      "Epoch 2769, Loss 2.929845\n",
      "Params: tensor([  5.3398, -17.1466])\n",
      "Gradient: tensor([-0.0048,  0.0269])\n",
      "Epoch 2770, Loss 2.929838\n",
      "Params: tensor([  5.3398, -17.1469])\n",
      "Gradient: tensor([-0.0047,  0.0269])\n",
      "Epoch 2771, Loss 2.929830\n",
      "Params: tensor([  5.3399, -17.1471])\n",
      "Gradient: tensor([-0.0047,  0.0268])\n",
      "Epoch 2772, Loss 2.929822\n",
      "Params: tensor([  5.3399, -17.1474])\n",
      "Gradient: tensor([-0.0047,  0.0268])\n",
      "Epoch 2773, Loss 2.929816\n",
      "Params: tensor([  5.3400, -17.1477])\n",
      "Gradient: tensor([-0.0047,  0.0267])\n",
      "Epoch 2774, Loss 2.929806\n",
      "Params: tensor([  5.3400, -17.1479])\n",
      "Gradient: tensor([-0.0047,  0.0267])\n",
      "Epoch 2775, Loss 2.929799\n",
      "Params: tensor([  5.3401, -17.1482])\n",
      "Gradient: tensor([-0.0047,  0.0266])\n",
      "Epoch 2776, Loss 2.929794\n",
      "Params: tensor([  5.3401, -17.1485])\n",
      "Gradient: tensor([-0.0047,  0.0266])\n",
      "Epoch 2777, Loss 2.929786\n",
      "Params: tensor([  5.3402, -17.1487])\n",
      "Gradient: tensor([-0.0047,  0.0266])\n",
      "Epoch 2778, Loss 2.929778\n",
      "Params: tensor([  5.3402, -17.1490])\n",
      "Gradient: tensor([-0.0047,  0.0265])\n",
      "Epoch 2779, Loss 2.929771\n",
      "Params: tensor([  5.3402, -17.1493])\n",
      "Gradient: tensor([-0.0047,  0.0265])\n",
      "Epoch 2780, Loss 2.929765\n",
      "Params: tensor([  5.3403, -17.1495])\n",
      "Gradient: tensor([-0.0047,  0.0264])\n",
      "Epoch 2781, Loss 2.929757\n",
      "Params: tensor([  5.3403, -17.1498])\n",
      "Gradient: tensor([-0.0047,  0.0264])\n",
      "Epoch 2782, Loss 2.929750\n",
      "Params: tensor([  5.3404, -17.1501])\n",
      "Gradient: tensor([-0.0046,  0.0263])\n",
      "Epoch 2783, Loss 2.929743\n",
      "Params: tensor([  5.3404, -17.1503])\n",
      "Gradient: tensor([-0.0046,  0.0263])\n",
      "Epoch 2784, Loss 2.929735\n",
      "Params: tensor([  5.3405, -17.1506])\n",
      "Gradient: tensor([-0.0046,  0.0262])\n",
      "Epoch 2785, Loss 2.929729\n",
      "Params: tensor([  5.3405, -17.1508])\n",
      "Gradient: tensor([-0.0047,  0.0262])\n",
      "Epoch 2786, Loss 2.929722\n",
      "Params: tensor([  5.3406, -17.1511])\n",
      "Gradient: tensor([-0.0046,  0.0262])\n",
      "Epoch 2787, Loss 2.929714\n",
      "Params: tensor([  5.3406, -17.1514])\n",
      "Gradient: tensor([-0.0046,  0.0261])\n",
      "Epoch 2788, Loss 2.929707\n",
      "Params: tensor([  5.3407, -17.1516])\n",
      "Gradient: tensor([-0.0046,  0.0261])\n",
      "Epoch 2789, Loss 2.929701\n",
      "Params: tensor([  5.3407, -17.1519])\n",
      "Gradient: tensor([-0.0046,  0.0260])\n",
      "Epoch 2790, Loss 2.929692\n",
      "Params: tensor([  5.3408, -17.1522])\n",
      "Gradient: tensor([-0.0046,  0.0260])\n",
      "Epoch 2791, Loss 2.929685\n",
      "Params: tensor([  5.3408, -17.1524])\n",
      "Gradient: tensor([-0.0046,  0.0259])\n",
      "Epoch 2792, Loss 2.929681\n",
      "Params: tensor([  5.3408, -17.1527])\n",
      "Gradient: tensor([-0.0046,  0.0259])\n",
      "Epoch 2793, Loss 2.929672\n",
      "Params: tensor([  5.3409, -17.1529])\n",
      "Gradient: tensor([-0.0046,  0.0258])\n",
      "Epoch 2794, Loss 2.929666\n",
      "Params: tensor([  5.3409, -17.1532])\n",
      "Gradient: tensor([-0.0046,  0.0258])\n",
      "Epoch 2795, Loss 2.929658\n",
      "Params: tensor([  5.3410, -17.1534])\n",
      "Gradient: tensor([-0.0045,  0.0258])\n",
      "Epoch 2796, Loss 2.929652\n",
      "Params: tensor([  5.3410, -17.1537])\n",
      "Gradient: tensor([-0.0045,  0.0257])\n",
      "Epoch 2797, Loss 2.929646\n",
      "Params: tensor([  5.3411, -17.1540])\n",
      "Gradient: tensor([-0.0045,  0.0257])\n",
      "Epoch 2798, Loss 2.929638\n",
      "Params: tensor([  5.3411, -17.1542])\n",
      "Gradient: tensor([-0.0045,  0.0256])\n",
      "Epoch 2799, Loss 2.929632\n",
      "Params: tensor([  5.3412, -17.1545])\n",
      "Gradient: tensor([-0.0045,  0.0256])\n",
      "Epoch 2800, Loss 2.929626\n",
      "Params: tensor([  5.3412, -17.1547])\n",
      "Gradient: tensor([-0.0045,  0.0255])\n",
      "Epoch 2801, Loss 2.929620\n",
      "Params: tensor([  5.3413, -17.1550])\n",
      "Gradient: tensor([-0.0045,  0.0255])\n",
      "Epoch 2802, Loss 2.929611\n",
      "Params: tensor([  5.3413, -17.1552])\n",
      "Gradient: tensor([-0.0045,  0.0254])\n",
      "Epoch 2803, Loss 2.929605\n",
      "Params: tensor([  5.3413, -17.1555])\n",
      "Gradient: tensor([-0.0045,  0.0254])\n",
      "Epoch 2804, Loss 2.929600\n",
      "Params: tensor([  5.3414, -17.1557])\n",
      "Gradient: tensor([-0.0045,  0.0254])\n",
      "Epoch 2805, Loss 2.929593\n",
      "Params: tensor([  5.3414, -17.1560])\n",
      "Gradient: tensor([-0.0045,  0.0253])\n",
      "Epoch 2806, Loss 2.929586\n",
      "Params: tensor([  5.3415, -17.1562])\n",
      "Gradient: tensor([-0.0045,  0.0253])\n",
      "Epoch 2807, Loss 2.929579\n",
      "Params: tensor([  5.3415, -17.1565])\n",
      "Gradient: tensor([-0.0045,  0.0252])\n",
      "Epoch 2808, Loss 2.929572\n",
      "Params: tensor([  5.3416, -17.1568])\n",
      "Gradient: tensor([-0.0044,  0.0252])\n",
      "Epoch 2809, Loss 2.929566\n",
      "Params: tensor([  5.3416, -17.1570])\n",
      "Gradient: tensor([-0.0044,  0.0251])\n",
      "Epoch 2810, Loss 2.929559\n",
      "Params: tensor([  5.3417, -17.1573])\n",
      "Gradient: tensor([-0.0044,  0.0251])\n",
      "Epoch 2811, Loss 2.929551\n",
      "Params: tensor([  5.3417, -17.1575])\n",
      "Gradient: tensor([-0.0044,  0.0251])\n",
      "Epoch 2812, Loss 2.929545\n",
      "Params: tensor([  5.3417, -17.1578])\n",
      "Gradient: tensor([-0.0044,  0.0250])\n",
      "Epoch 2813, Loss 2.929540\n",
      "Params: tensor([  5.3418, -17.1580])\n",
      "Gradient: tensor([-0.0044,  0.0250])\n",
      "Epoch 2814, Loss 2.929534\n",
      "Params: tensor([  5.3418, -17.1583])\n",
      "Gradient: tensor([-0.0044,  0.0249])\n",
      "Epoch 2815, Loss 2.929526\n",
      "Params: tensor([  5.3419, -17.1585])\n",
      "Gradient: tensor([-0.0044,  0.0249])\n",
      "Epoch 2816, Loss 2.929520\n",
      "Params: tensor([  5.3419, -17.1588])\n",
      "Gradient: tensor([-0.0044,  0.0248])\n",
      "Epoch 2817, Loss 2.929513\n",
      "Params: tensor([  5.3420, -17.1590])\n",
      "Gradient: tensor([-0.0044,  0.0248])\n",
      "Epoch 2818, Loss 2.929507\n",
      "Params: tensor([  5.3420, -17.1592])\n",
      "Gradient: tensor([-0.0044,  0.0248])\n",
      "Epoch 2819, Loss 2.929502\n",
      "Params: tensor([  5.3421, -17.1595])\n",
      "Gradient: tensor([-0.0044,  0.0247])\n",
      "Epoch 2820, Loss 2.929496\n",
      "Params: tensor([  5.3421, -17.1597])\n",
      "Gradient: tensor([-0.0043,  0.0247])\n",
      "Epoch 2821, Loss 2.929489\n",
      "Params: tensor([  5.3421, -17.1600])\n",
      "Gradient: tensor([-0.0043,  0.0246])\n",
      "Epoch 2822, Loss 2.929482\n",
      "Params: tensor([  5.3422, -17.1602])\n",
      "Gradient: tensor([-0.0043,  0.0246])\n",
      "Epoch 2823, Loss 2.929477\n",
      "Params: tensor([  5.3422, -17.1605])\n",
      "Gradient: tensor([-0.0043,  0.0246])\n",
      "Epoch 2824, Loss 2.929469\n",
      "Params: tensor([  5.3423, -17.1607])\n",
      "Gradient: tensor([-0.0043,  0.0245])\n",
      "Epoch 2825, Loss 2.929465\n",
      "Params: tensor([  5.3423, -17.1610])\n",
      "Gradient: tensor([-0.0043,  0.0245])\n",
      "Epoch 2826, Loss 2.929458\n",
      "Params: tensor([  5.3424, -17.1612])\n",
      "Gradient: tensor([-0.0043,  0.0244])\n",
      "Epoch 2827, Loss 2.929452\n",
      "Params: tensor([  5.3424, -17.1615])\n",
      "Gradient: tensor([-0.0043,  0.0244])\n",
      "Epoch 2828, Loss 2.929445\n",
      "Params: tensor([  5.3424, -17.1617])\n",
      "Gradient: tensor([-0.0043,  0.0243])\n",
      "Epoch 2829, Loss 2.929439\n",
      "Params: tensor([  5.3425, -17.1619])\n",
      "Gradient: tensor([-0.0043,  0.0243])\n",
      "Epoch 2830, Loss 2.929432\n",
      "Params: tensor([  5.3425, -17.1622])\n",
      "Gradient: tensor([-0.0043,  0.0243])\n",
      "Epoch 2831, Loss 2.929426\n",
      "Params: tensor([  5.3426, -17.1624])\n",
      "Gradient: tensor([-0.0043,  0.0242])\n",
      "Epoch 2832, Loss 2.929421\n",
      "Params: tensor([  5.3426, -17.1627])\n",
      "Gradient: tensor([-0.0043,  0.0242])\n",
      "Epoch 2833, Loss 2.929415\n",
      "Params: tensor([  5.3427, -17.1629])\n",
      "Gradient: tensor([-0.0043,  0.0241])\n",
      "Epoch 2834, Loss 2.929409\n",
      "Params: tensor([  5.3427, -17.1632])\n",
      "Gradient: tensor([-0.0043,  0.0241])\n",
      "Epoch 2835, Loss 2.929404\n",
      "Params: tensor([  5.3427, -17.1634])\n",
      "Gradient: tensor([-0.0042,  0.0241])\n",
      "Epoch 2836, Loss 2.929396\n",
      "Params: tensor([  5.3428, -17.1636])\n",
      "Gradient: tensor([-0.0042,  0.0240])\n",
      "Epoch 2837, Loss 2.929390\n",
      "Params: tensor([  5.3428, -17.1639])\n",
      "Gradient: tensor([-0.0042,  0.0240])\n",
      "Epoch 2838, Loss 2.929385\n",
      "Params: tensor([  5.3429, -17.1641])\n",
      "Gradient: tensor([-0.0042,  0.0239])\n",
      "Epoch 2839, Loss 2.929378\n",
      "Params: tensor([  5.3429, -17.1644])\n",
      "Gradient: tensor([-0.0042,  0.0239])\n",
      "Epoch 2840, Loss 2.929374\n",
      "Params: tensor([  5.3430, -17.1646])\n",
      "Gradient: tensor([-0.0042,  0.0239])\n",
      "Epoch 2841, Loss 2.929367\n",
      "Params: tensor([  5.3430, -17.1648])\n",
      "Gradient: tensor([-0.0042,  0.0238])\n",
      "Epoch 2842, Loss 2.929362\n",
      "Params: tensor([  5.3430, -17.1651])\n",
      "Gradient: tensor([-0.0042,  0.0238])\n",
      "Epoch 2843, Loss 2.929356\n",
      "Params: tensor([  5.3431, -17.1653])\n",
      "Gradient: tensor([-0.0042,  0.0237])\n",
      "Epoch 2844, Loss 2.929350\n",
      "Params: tensor([  5.3431, -17.1655])\n",
      "Gradient: tensor([-0.0042,  0.0237])\n",
      "Epoch 2845, Loss 2.929345\n",
      "Params: tensor([  5.3432, -17.1658])\n",
      "Gradient: tensor([-0.0042,  0.0237])\n",
      "Epoch 2846, Loss 2.929337\n",
      "Params: tensor([  5.3432, -17.1660])\n",
      "Gradient: tensor([-0.0042,  0.0236])\n",
      "Epoch 2847, Loss 2.929332\n",
      "Params: tensor([  5.3432, -17.1663])\n",
      "Gradient: tensor([-0.0042,  0.0236])\n",
      "Epoch 2848, Loss 2.929327\n",
      "Params: tensor([  5.3433, -17.1665])\n",
      "Gradient: tensor([-0.0042,  0.0235])\n",
      "Epoch 2849, Loss 2.929320\n",
      "Params: tensor([  5.3433, -17.1667])\n",
      "Gradient: tensor([-0.0042,  0.0235])\n",
      "Epoch 2850, Loss 2.929315\n",
      "Params: tensor([  5.3434, -17.1670])\n",
      "Gradient: tensor([-0.0041,  0.0235])\n",
      "Epoch 2851, Loss 2.929311\n",
      "Params: tensor([  5.3434, -17.1672])\n",
      "Gradient: tensor([-0.0041,  0.0234])\n",
      "Epoch 2852, Loss 2.929303\n",
      "Params: tensor([  5.3435, -17.1674])\n",
      "Gradient: tensor([-0.0041,  0.0234])\n",
      "Epoch 2853, Loss 2.929300\n",
      "Params: tensor([  5.3435, -17.1677])\n",
      "Gradient: tensor([-0.0041,  0.0233])\n",
      "Epoch 2854, Loss 2.929293\n",
      "Params: tensor([  5.3435, -17.1679])\n",
      "Gradient: tensor([-0.0041,  0.0233])\n",
      "Epoch 2855, Loss 2.929288\n",
      "Params: tensor([  5.3436, -17.1681])\n",
      "Gradient: tensor([-0.0041,  0.0233])\n",
      "Epoch 2856, Loss 2.929282\n",
      "Params: tensor([  5.3436, -17.1684])\n",
      "Gradient: tensor([-0.0041,  0.0232])\n",
      "Epoch 2857, Loss 2.929278\n",
      "Params: tensor([  5.3437, -17.1686])\n",
      "Gradient: tensor([-0.0041,  0.0232])\n",
      "Epoch 2858, Loss 2.929271\n",
      "Params: tensor([  5.3437, -17.1688])\n",
      "Gradient: tensor([-0.0041,  0.0231])\n",
      "Epoch 2859, Loss 2.929267\n",
      "Params: tensor([  5.3437, -17.1690])\n",
      "Gradient: tensor([-0.0041,  0.0231])\n",
      "Epoch 2860, Loss 2.929261\n",
      "Params: tensor([  5.3438, -17.1693])\n",
      "Gradient: tensor([-0.0041,  0.0231])\n",
      "Epoch 2861, Loss 2.929255\n",
      "Params: tensor([  5.3438, -17.1695])\n",
      "Gradient: tensor([-0.0041,  0.0230])\n",
      "Epoch 2862, Loss 2.929250\n",
      "Params: tensor([  5.3439, -17.1697])\n",
      "Gradient: tensor([-0.0041,  0.0230])\n",
      "Epoch 2863, Loss 2.929244\n",
      "Params: tensor([  5.3439, -17.1700])\n",
      "Gradient: tensor([-0.0041,  0.0229])\n",
      "Epoch 2864, Loss 2.929237\n",
      "Params: tensor([  5.3439, -17.1702])\n",
      "Gradient: tensor([-0.0040,  0.0229])\n",
      "Epoch 2865, Loss 2.929232\n",
      "Params: tensor([  5.3440, -17.1704])\n",
      "Gradient: tensor([-0.0040,  0.0229])\n",
      "Epoch 2866, Loss 2.929228\n",
      "Params: tensor([  5.3440, -17.1707])\n",
      "Gradient: tensor([-0.0040,  0.0228])\n",
      "Epoch 2867, Loss 2.929221\n",
      "Params: tensor([  5.3441, -17.1709])\n",
      "Gradient: tensor([-0.0040,  0.0228])\n",
      "Epoch 2868, Loss 2.929216\n",
      "Params: tensor([  5.3441, -17.1711])\n",
      "Gradient: tensor([-0.0040,  0.0228])\n",
      "Epoch 2869, Loss 2.929212\n",
      "Params: tensor([  5.3441, -17.1713])\n",
      "Gradient: tensor([-0.0040,  0.0227])\n",
      "Epoch 2870, Loss 2.929206\n",
      "Params: tensor([  5.3442, -17.1716])\n",
      "Gradient: tensor([-0.0040,  0.0227])\n",
      "Epoch 2871, Loss 2.929201\n",
      "Params: tensor([  5.3442, -17.1718])\n",
      "Gradient: tensor([-0.0040,  0.0226])\n",
      "Epoch 2872, Loss 2.929196\n",
      "Params: tensor([  5.3443, -17.1720])\n",
      "Gradient: tensor([-0.0040,  0.0226])\n",
      "Epoch 2873, Loss 2.929191\n",
      "Params: tensor([  5.3443, -17.1722])\n",
      "Gradient: tensor([-0.0040,  0.0226])\n",
      "Epoch 2874, Loss 2.929186\n",
      "Params: tensor([  5.3443, -17.1725])\n",
      "Gradient: tensor([-0.0040,  0.0225])\n",
      "Epoch 2875, Loss 2.929181\n",
      "Params: tensor([  5.3444, -17.1727])\n",
      "Gradient: tensor([-0.0040,  0.0225])\n",
      "Epoch 2876, Loss 2.929174\n",
      "Params: tensor([  5.3444, -17.1729])\n",
      "Gradient: tensor([-0.0040,  0.0224])\n",
      "Epoch 2877, Loss 2.929170\n",
      "Params: tensor([  5.3445, -17.1731])\n",
      "Gradient: tensor([-0.0040,  0.0224])\n",
      "Epoch 2878, Loss 2.929166\n",
      "Params: tensor([  5.3445, -17.1734])\n",
      "Gradient: tensor([-0.0040,  0.0224])\n",
      "Epoch 2879, Loss 2.929159\n",
      "Params: tensor([  5.3445, -17.1736])\n",
      "Gradient: tensor([-0.0039,  0.0223])\n",
      "Epoch 2880, Loss 2.929154\n",
      "Params: tensor([  5.3446, -17.1738])\n",
      "Gradient: tensor([-0.0039,  0.0223])\n",
      "Epoch 2881, Loss 2.929150\n",
      "Params: tensor([  5.3446, -17.1740])\n",
      "Gradient: tensor([-0.0039,  0.0222])\n",
      "Epoch 2882, Loss 2.929145\n",
      "Params: tensor([  5.3447, -17.1743])\n",
      "Gradient: tensor([-0.0039,  0.0222])\n",
      "Epoch 2883, Loss 2.929138\n",
      "Params: tensor([  5.3447, -17.1745])\n",
      "Gradient: tensor([-0.0039,  0.0222])\n",
      "Epoch 2884, Loss 2.929133\n",
      "Params: tensor([  5.3447, -17.1747])\n",
      "Gradient: tensor([-0.0039,  0.0221])\n",
      "Epoch 2885, Loss 2.929127\n",
      "Params: tensor([  5.3448, -17.1749])\n",
      "Gradient: tensor([-0.0039,  0.0221])\n",
      "Epoch 2886, Loss 2.929123\n",
      "Params: tensor([  5.3448, -17.1751])\n",
      "Gradient: tensor([-0.0039,  0.0221])\n",
      "Epoch 2887, Loss 2.929119\n",
      "Params: tensor([  5.3449, -17.1754])\n",
      "Gradient: tensor([-0.0039,  0.0220])\n",
      "Epoch 2888, Loss 2.929113\n",
      "Params: tensor([  5.3449, -17.1756])\n",
      "Gradient: tensor([-0.0039,  0.0220])\n",
      "Epoch 2889, Loss 2.929108\n",
      "Params: tensor([  5.3449, -17.1758])\n",
      "Gradient: tensor([-0.0039,  0.0220])\n",
      "Epoch 2890, Loss 2.929103\n",
      "Params: tensor([  5.3450, -17.1760])\n",
      "Gradient: tensor([-0.0039,  0.0219])\n",
      "Epoch 2891, Loss 2.929097\n",
      "Params: tensor([  5.3450, -17.1762])\n",
      "Gradient: tensor([-0.0039,  0.0219])\n",
      "Epoch 2892, Loss 2.929094\n",
      "Params: tensor([  5.3450, -17.1765])\n",
      "Gradient: tensor([-0.0039,  0.0218])\n",
      "Epoch 2893, Loss 2.929090\n",
      "Params: tensor([  5.3451, -17.1767])\n",
      "Gradient: tensor([-0.0038,  0.0218])\n",
      "Epoch 2894, Loss 2.929083\n",
      "Params: tensor([  5.3451, -17.1769])\n",
      "Gradient: tensor([-0.0038,  0.0218])\n",
      "Epoch 2895, Loss 2.929079\n",
      "Params: tensor([  5.3452, -17.1771])\n",
      "Gradient: tensor([-0.0038,  0.0217])\n",
      "Epoch 2896, Loss 2.929075\n",
      "Params: tensor([  5.3452, -17.1773])\n",
      "Gradient: tensor([-0.0038,  0.0217])\n",
      "Epoch 2897, Loss 2.929069\n",
      "Params: tensor([  5.3452, -17.1775])\n",
      "Gradient: tensor([-0.0038,  0.0217])\n",
      "Epoch 2898, Loss 2.929063\n",
      "Params: tensor([  5.3453, -17.1778])\n",
      "Gradient: tensor([-0.0038,  0.0216])\n",
      "Epoch 2899, Loss 2.929060\n",
      "Params: tensor([  5.3453, -17.1780])\n",
      "Gradient: tensor([-0.0038,  0.0216])\n",
      "Epoch 2900, Loss 2.929054\n",
      "Params: tensor([  5.3454, -17.1782])\n",
      "Gradient: tensor([-0.0038,  0.0215])\n",
      "Epoch 2901, Loss 2.929049\n",
      "Params: tensor([  5.3454, -17.1784])\n",
      "Gradient: tensor([-0.0038,  0.0215])\n",
      "Epoch 2902, Loss 2.929044\n",
      "Params: tensor([  5.3454, -17.1786])\n",
      "Gradient: tensor([-0.0038,  0.0215])\n",
      "Epoch 2903, Loss 2.929040\n",
      "Params: tensor([  5.3455, -17.1788])\n",
      "Gradient: tensor([-0.0038,  0.0214])\n",
      "Epoch 2904, Loss 2.929035\n",
      "Params: tensor([  5.3455, -17.1790])\n",
      "Gradient: tensor([-0.0038,  0.0214])\n",
      "Epoch 2905, Loss 2.929031\n",
      "Params: tensor([  5.3455, -17.1793])\n",
      "Gradient: tensor([-0.0038,  0.0214])\n",
      "Epoch 2906, Loss 2.929025\n",
      "Params: tensor([  5.3456, -17.1795])\n",
      "Gradient: tensor([-0.0038,  0.0213])\n",
      "Epoch 2907, Loss 2.929022\n",
      "Params: tensor([  5.3456, -17.1797])\n",
      "Gradient: tensor([-0.0038,  0.0213])\n",
      "Epoch 2908, Loss 2.929016\n",
      "Params: tensor([  5.3457, -17.1799])\n",
      "Gradient: tensor([-0.0038,  0.0212])\n",
      "Epoch 2909, Loss 2.929012\n",
      "Params: tensor([  5.3457, -17.1801])\n",
      "Gradient: tensor([-0.0037,  0.0212])\n",
      "Epoch 2910, Loss 2.929006\n",
      "Params: tensor([  5.3457, -17.1803])\n",
      "Gradient: tensor([-0.0037,  0.0212])\n",
      "Epoch 2911, Loss 2.929004\n",
      "Params: tensor([  5.3458, -17.1805])\n",
      "Gradient: tensor([-0.0037,  0.0211])\n",
      "Epoch 2912, Loss 2.928999\n",
      "Params: tensor([  5.3458, -17.1807])\n",
      "Gradient: tensor([-0.0037,  0.0211])\n",
      "Epoch 2913, Loss 2.928993\n",
      "Params: tensor([  5.3458, -17.1810])\n",
      "Gradient: tensor([-0.0037,  0.0211])\n",
      "Epoch 2914, Loss 2.928989\n",
      "Params: tensor([  5.3459, -17.1812])\n",
      "Gradient: tensor([-0.0037,  0.0210])\n",
      "Epoch 2915, Loss 2.928984\n",
      "Params: tensor([  5.3459, -17.1814])\n",
      "Gradient: tensor([-0.0037,  0.0210])\n",
      "Epoch 2916, Loss 2.928979\n",
      "Params: tensor([  5.3460, -17.1816])\n",
      "Gradient: tensor([-0.0037,  0.0210])\n",
      "Epoch 2917, Loss 2.928975\n",
      "Params: tensor([  5.3460, -17.1818])\n",
      "Gradient: tensor([-0.0037,  0.0209])\n",
      "Epoch 2918, Loss 2.928971\n",
      "Params: tensor([  5.3460, -17.1820])\n",
      "Gradient: tensor([-0.0037,  0.0209])\n",
      "Epoch 2919, Loss 2.928967\n",
      "Params: tensor([  5.3461, -17.1822])\n",
      "Gradient: tensor([-0.0037,  0.0209])\n",
      "Epoch 2920, Loss 2.928962\n",
      "Params: tensor([  5.3461, -17.1824])\n",
      "Gradient: tensor([-0.0037,  0.0208])\n",
      "Epoch 2921, Loss 2.928957\n",
      "Params: tensor([  5.3461, -17.1826])\n",
      "Gradient: tensor([-0.0037,  0.0208])\n",
      "Epoch 2922, Loss 2.928951\n",
      "Params: tensor([  5.3462, -17.1828])\n",
      "Gradient: tensor([-0.0037,  0.0208])\n",
      "Epoch 2923, Loss 2.928950\n",
      "Params: tensor([  5.3462, -17.1830])\n",
      "Gradient: tensor([-0.0037,  0.0207])\n",
      "Epoch 2924, Loss 2.928945\n",
      "Params: tensor([  5.3462, -17.1832])\n",
      "Gradient: tensor([-0.0037,  0.0207])\n",
      "Epoch 2925, Loss 2.928941\n",
      "Params: tensor([  5.3463, -17.1835])\n",
      "Gradient: tensor([-0.0036,  0.0206])\n",
      "Epoch 2926, Loss 2.928935\n",
      "Params: tensor([  5.3463, -17.1837])\n",
      "Gradient: tensor([-0.0036,  0.0206])\n",
      "Epoch 2927, Loss 2.928931\n",
      "Params: tensor([  5.3464, -17.1839])\n",
      "Gradient: tensor([-0.0036,  0.0206])\n",
      "Epoch 2928, Loss 2.928927\n",
      "Params: tensor([  5.3464, -17.1841])\n",
      "Gradient: tensor([-0.0036,  0.0205])\n",
      "Epoch 2929, Loss 2.928922\n",
      "Params: tensor([  5.3464, -17.1843])\n",
      "Gradient: tensor([-0.0036,  0.0205])\n",
      "Epoch 2930, Loss 2.928919\n",
      "Params: tensor([  5.3465, -17.1845])\n",
      "Gradient: tensor([-0.0036,  0.0205])\n",
      "Epoch 2931, Loss 2.928915\n",
      "Params: tensor([  5.3465, -17.1847])\n",
      "Gradient: tensor([-0.0036,  0.0204])\n",
      "Epoch 2932, Loss 2.928910\n",
      "Params: tensor([  5.3465, -17.1849])\n",
      "Gradient: tensor([-0.0036,  0.0204])\n",
      "Epoch 2933, Loss 2.928905\n",
      "Params: tensor([  5.3466, -17.1851])\n",
      "Gradient: tensor([-0.0036,  0.0204])\n",
      "Epoch 2934, Loss 2.928901\n",
      "Params: tensor([  5.3466, -17.1853])\n",
      "Gradient: tensor([-0.0036,  0.0203])\n",
      "Epoch 2935, Loss 2.928897\n",
      "Params: tensor([  5.3466, -17.1855])\n",
      "Gradient: tensor([-0.0036,  0.0203])\n",
      "Epoch 2936, Loss 2.928893\n",
      "Params: tensor([  5.3467, -17.1857])\n",
      "Gradient: tensor([-0.0036,  0.0203])\n",
      "Epoch 2937, Loss 2.928887\n",
      "Params: tensor([  5.3467, -17.1859])\n",
      "Gradient: tensor([-0.0036,  0.0202])\n",
      "Epoch 2938, Loss 2.928883\n",
      "Params: tensor([  5.3468, -17.1861])\n",
      "Gradient: tensor([-0.0036,  0.0202])\n",
      "Epoch 2939, Loss 2.928880\n",
      "Params: tensor([  5.3468, -17.1863])\n",
      "Gradient: tensor([-0.0036,  0.0202])\n",
      "Epoch 2940, Loss 2.928876\n",
      "Params: tensor([  5.3468, -17.1865])\n",
      "Gradient: tensor([-0.0035,  0.0201])\n",
      "Epoch 2941, Loss 2.928871\n",
      "Params: tensor([  5.3469, -17.1867])\n",
      "Gradient: tensor([-0.0036,  0.0201])\n",
      "Epoch 2942, Loss 2.928868\n",
      "Params: tensor([  5.3469, -17.1869])\n",
      "Gradient: tensor([-0.0035,  0.0201])\n",
      "Epoch 2943, Loss 2.928864\n",
      "Params: tensor([  5.3469, -17.1871])\n",
      "Gradient: tensor([-0.0035,  0.0200])\n",
      "Epoch 2944, Loss 2.928858\n",
      "Params: tensor([  5.3470, -17.1873])\n",
      "Gradient: tensor([-0.0035,  0.0200])\n",
      "Epoch 2945, Loss 2.928855\n",
      "Params: tensor([  5.3470, -17.1875])\n",
      "Gradient: tensor([-0.0035,  0.0200])\n",
      "Epoch 2946, Loss 2.928850\n",
      "Params: tensor([  5.3470, -17.1877])\n",
      "Gradient: tensor([-0.0035,  0.0199])\n",
      "Epoch 2947, Loss 2.928847\n",
      "Params: tensor([  5.3471, -17.1879])\n",
      "Gradient: tensor([-0.0035,  0.0199])\n",
      "Epoch 2948, Loss 2.928842\n",
      "Params: tensor([  5.3471, -17.1881])\n",
      "Gradient: tensor([-0.0035,  0.0199])\n",
      "Epoch 2949, Loss 2.928838\n",
      "Params: tensor([  5.3471, -17.1883])\n",
      "Gradient: tensor([-0.0035,  0.0198])\n",
      "Epoch 2950, Loss 2.928834\n",
      "Params: tensor([  5.3472, -17.1885])\n",
      "Gradient: tensor([-0.0035,  0.0198])\n",
      "Epoch 2951, Loss 2.928829\n",
      "Params: tensor([  5.3472, -17.1887])\n",
      "Gradient: tensor([-0.0035,  0.0198])\n",
      "Epoch 2952, Loss 2.928827\n",
      "Params: tensor([  5.3472, -17.1889])\n",
      "Gradient: tensor([-0.0035,  0.0197])\n",
      "Epoch 2953, Loss 2.928822\n",
      "Params: tensor([  5.3473, -17.1891])\n",
      "Gradient: tensor([-0.0035,  0.0197])\n",
      "Epoch 2954, Loss 2.928818\n",
      "Params: tensor([  5.3473, -17.1893])\n",
      "Gradient: tensor([-0.0035,  0.0197])\n",
      "Epoch 2955, Loss 2.928815\n",
      "Params: tensor([  5.3474, -17.1895])\n",
      "Gradient: tensor([-0.0035,  0.0196])\n",
      "Epoch 2956, Loss 2.928811\n",
      "Params: tensor([  5.3474, -17.1897])\n",
      "Gradient: tensor([-0.0035,  0.0196])\n",
      "Epoch 2957, Loss 2.928805\n",
      "Params: tensor([  5.3474, -17.1899])\n",
      "Gradient: tensor([-0.0035,  0.0196])\n",
      "Epoch 2958, Loss 2.928803\n",
      "Params: tensor([  5.3475, -17.1901])\n",
      "Gradient: tensor([-0.0035,  0.0195])\n",
      "Epoch 2959, Loss 2.928799\n",
      "Params: tensor([  5.3475, -17.1903])\n",
      "Gradient: tensor([-0.0035,  0.0195])\n",
      "Epoch 2960, Loss 2.928795\n",
      "Params: tensor([  5.3475, -17.1905])\n",
      "Gradient: tensor([-0.0034,  0.0195])\n",
      "Epoch 2961, Loss 2.928789\n",
      "Params: tensor([  5.3476, -17.1907])\n",
      "Gradient: tensor([-0.0034,  0.0194])\n",
      "Epoch 2962, Loss 2.928789\n",
      "Params: tensor([  5.3476, -17.1909])\n",
      "Gradient: tensor([-0.0034,  0.0194])\n",
      "Epoch 2963, Loss 2.928783\n",
      "Params: tensor([  5.3476, -17.1910])\n",
      "Gradient: tensor([-0.0034,  0.0194])\n",
      "Epoch 2964, Loss 2.928779\n",
      "Params: tensor([  5.3477, -17.1912])\n",
      "Gradient: tensor([-0.0034,  0.0193])\n",
      "Epoch 2965, Loss 2.928775\n",
      "Params: tensor([  5.3477, -17.1914])\n",
      "Gradient: tensor([-0.0034,  0.0193])\n",
      "Epoch 2966, Loss 2.928770\n",
      "Params: tensor([  5.3477, -17.1916])\n",
      "Gradient: tensor([-0.0034,  0.0193])\n",
      "Epoch 2967, Loss 2.928768\n",
      "Params: tensor([  5.3478, -17.1918])\n",
      "Gradient: tensor([-0.0034,  0.0192])\n",
      "Epoch 2968, Loss 2.928762\n",
      "Params: tensor([  5.3478, -17.1920])\n",
      "Gradient: tensor([-0.0034,  0.0192])\n",
      "Epoch 2969, Loss 2.928760\n",
      "Params: tensor([  5.3478, -17.1922])\n",
      "Gradient: tensor([-0.0034,  0.0192])\n",
      "Epoch 2970, Loss 2.928758\n",
      "Params: tensor([  5.3479, -17.1924])\n",
      "Gradient: tensor([-0.0034,  0.0191])\n",
      "Epoch 2971, Loss 2.928752\n",
      "Params: tensor([  5.3479, -17.1926])\n",
      "Gradient: tensor([-0.0034,  0.0191])\n",
      "Epoch 2972, Loss 2.928749\n",
      "Params: tensor([  5.3479, -17.1928])\n",
      "Gradient: tensor([-0.0034,  0.0191])\n",
      "Epoch 2973, Loss 2.928745\n",
      "Params: tensor([  5.3480, -17.1930])\n",
      "Gradient: tensor([-0.0034,  0.0190])\n",
      "Epoch 2974, Loss 2.928741\n",
      "Params: tensor([  5.3480, -17.1932])\n",
      "Gradient: tensor([-0.0034,  0.0190])\n",
      "Epoch 2975, Loss 2.928737\n",
      "Params: tensor([  5.3480, -17.1933])\n",
      "Gradient: tensor([-0.0034,  0.0190])\n",
      "Epoch 2976, Loss 2.928734\n",
      "Params: tensor([  5.3481, -17.1935])\n",
      "Gradient: tensor([-0.0033,  0.0189])\n",
      "Epoch 2977, Loss 2.928731\n",
      "Params: tensor([  5.3481, -17.1937])\n",
      "Gradient: tensor([-0.0033,  0.0189])\n",
      "Epoch 2978, Loss 2.928726\n",
      "Params: tensor([  5.3481, -17.1939])\n",
      "Gradient: tensor([-0.0033,  0.0189])\n",
      "Epoch 2979, Loss 2.928722\n",
      "Params: tensor([  5.3482, -17.1941])\n",
      "Gradient: tensor([-0.0033,  0.0188])\n",
      "Epoch 2980, Loss 2.928719\n",
      "Params: tensor([  5.3482, -17.1943])\n",
      "Gradient: tensor([-0.0033,  0.0188])\n",
      "Epoch 2981, Loss 2.928715\n",
      "Params: tensor([  5.3482, -17.1945])\n",
      "Gradient: tensor([-0.0033,  0.0188])\n",
      "Epoch 2982, Loss 2.928712\n",
      "Params: tensor([  5.3483, -17.1947])\n",
      "Gradient: tensor([-0.0033,  0.0187])\n",
      "Epoch 2983, Loss 2.928708\n",
      "Params: tensor([  5.3483, -17.1948])\n",
      "Gradient: tensor([-0.0033,  0.0187])\n",
      "Epoch 2984, Loss 2.928705\n",
      "Params: tensor([  5.3483, -17.1950])\n",
      "Gradient: tensor([-0.0033,  0.0187])\n",
      "Epoch 2985, Loss 2.928700\n",
      "Params: tensor([  5.3484, -17.1952])\n",
      "Gradient: tensor([-0.0033,  0.0186])\n",
      "Epoch 2986, Loss 2.928698\n",
      "Params: tensor([  5.3484, -17.1954])\n",
      "Gradient: tensor([-0.0033,  0.0186])\n",
      "Epoch 2987, Loss 2.928695\n",
      "Params: tensor([  5.3484, -17.1956])\n",
      "Gradient: tensor([-0.0033,  0.0186])\n",
      "Epoch 2988, Loss 2.928690\n",
      "Params: tensor([  5.3485, -17.1958])\n",
      "Gradient: tensor([-0.0033,  0.0186])\n",
      "Epoch 2989, Loss 2.928687\n",
      "Params: tensor([  5.3485, -17.1960])\n",
      "Gradient: tensor([-0.0033,  0.0185])\n",
      "Epoch 2990, Loss 2.928684\n",
      "Params: tensor([  5.3485, -17.1961])\n",
      "Gradient: tensor([-0.0033,  0.0185])\n",
      "Epoch 2991, Loss 2.928679\n",
      "Params: tensor([  5.3486, -17.1963])\n",
      "Gradient: tensor([-0.0033,  0.0185])\n",
      "Epoch 2992, Loss 2.928678\n",
      "Params: tensor([  5.3486, -17.1965])\n",
      "Gradient: tensor([-0.0032,  0.0184])\n",
      "Epoch 2993, Loss 2.928674\n",
      "Params: tensor([  5.3486, -17.1967])\n",
      "Gradient: tensor([-0.0033,  0.0184])\n",
      "Epoch 2994, Loss 2.928669\n",
      "Params: tensor([  5.3487, -17.1969])\n",
      "Gradient: tensor([-0.0032,  0.0184])\n",
      "Epoch 2995, Loss 2.928666\n",
      "Params: tensor([  5.3487, -17.1971])\n",
      "Gradient: tensor([-0.0032,  0.0183])\n",
      "Epoch 2996, Loss 2.928662\n",
      "Params: tensor([  5.3487, -17.1972])\n",
      "Gradient: tensor([-0.0032,  0.0183])\n",
      "Epoch 2997, Loss 2.928659\n",
      "Params: tensor([  5.3488, -17.1974])\n",
      "Gradient: tensor([-0.0032,  0.0183])\n",
      "Epoch 2998, Loss 2.928656\n",
      "Params: tensor([  5.3488, -17.1976])\n",
      "Gradient: tensor([-0.0032,  0.0182])\n",
      "Epoch 2999, Loss 2.928652\n",
      "Params: tensor([  5.3488, -17.1978])\n",
      "Gradient: tensor([-0.0032,  0.0182])\n",
      "Epoch 3000, Loss 2.928648\n",
      "Params: tensor([  5.3489, -17.1980])\n",
      "Gradient: tensor([-0.0032,  0.0182])\n",
      "Epoch 3001, Loss 2.928645\n",
      "Params: tensor([  5.3489, -17.1982])\n",
      "Gradient: tensor([-0.0032,  0.0181])\n",
      "Epoch 3002, Loss 2.928642\n",
      "Params: tensor([  5.3489, -17.1983])\n",
      "Gradient: tensor([-0.0032,  0.0181])\n",
      "Epoch 3003, Loss 2.928640\n",
      "Params: tensor([  5.3489, -17.1985])\n",
      "Gradient: tensor([-0.0032,  0.0181])\n",
      "Epoch 3004, Loss 2.928635\n",
      "Params: tensor([  5.3490, -17.1987])\n",
      "Gradient: tensor([-0.0032,  0.0181])\n",
      "Epoch 3005, Loss 2.928632\n",
      "Params: tensor([  5.3490, -17.1989])\n",
      "Gradient: tensor([-0.0032,  0.0180])\n",
      "Epoch 3006, Loss 2.928629\n",
      "Params: tensor([  5.3490, -17.1991])\n",
      "Gradient: tensor([-0.0032,  0.0180])\n",
      "Epoch 3007, Loss 2.928625\n",
      "Params: tensor([  5.3491, -17.1992])\n",
      "Gradient: tensor([-0.0032,  0.0180])\n",
      "Epoch 3008, Loss 2.928621\n",
      "Params: tensor([  5.3491, -17.1994])\n",
      "Gradient: tensor([-0.0032,  0.0179])\n",
      "Epoch 3009, Loss 2.928619\n",
      "Params: tensor([  5.3491, -17.1996])\n",
      "Gradient: tensor([-0.0032,  0.0179])\n",
      "Epoch 3010, Loss 2.928616\n",
      "Params: tensor([  5.3492, -17.1998])\n",
      "Gradient: tensor([-0.0032,  0.0179])\n",
      "Epoch 3011, Loss 2.928612\n",
      "Params: tensor([  5.3492, -17.2000])\n",
      "Gradient: tensor([-0.0032,  0.0178])\n",
      "Epoch 3012, Loss 2.928608\n",
      "Params: tensor([  5.3492, -17.2001])\n",
      "Gradient: tensor([-0.0032,  0.0178])\n",
      "Epoch 3013, Loss 2.928606\n",
      "Params: tensor([  5.3493, -17.2003])\n",
      "Gradient: tensor([-0.0031,  0.0178])\n",
      "Epoch 3014, Loss 2.928603\n",
      "Params: tensor([  5.3493, -17.2005])\n",
      "Gradient: tensor([-0.0031,  0.0177])\n",
      "Epoch 3015, Loss 2.928599\n",
      "Params: tensor([  5.3493, -17.2007])\n",
      "Gradient: tensor([-0.0031,  0.0177])\n",
      "Epoch 3016, Loss 2.928596\n",
      "Params: tensor([  5.3494, -17.2008])\n",
      "Gradient: tensor([-0.0031,  0.0177])\n",
      "Epoch 3017, Loss 2.928593\n",
      "Params: tensor([  5.3494, -17.2010])\n",
      "Gradient: tensor([-0.0031,  0.0177])\n",
      "Epoch 3018, Loss 2.928589\n",
      "Params: tensor([  5.3494, -17.2012])\n",
      "Gradient: tensor([-0.0031,  0.0176])\n",
      "Epoch 3019, Loss 2.928586\n",
      "Params: tensor([  5.3495, -17.2014])\n",
      "Gradient: tensor([-0.0031,  0.0176])\n",
      "Epoch 3020, Loss 2.928583\n",
      "Params: tensor([  5.3495, -17.2015])\n",
      "Gradient: tensor([-0.0031,  0.0176])\n",
      "Epoch 3021, Loss 2.928580\n",
      "Params: tensor([  5.3495, -17.2017])\n",
      "Gradient: tensor([-0.0031,  0.0175])\n",
      "Epoch 3022, Loss 2.928576\n",
      "Params: tensor([  5.3495, -17.2019])\n",
      "Gradient: tensor([-0.0031,  0.0175])\n",
      "Epoch 3023, Loss 2.928575\n",
      "Params: tensor([  5.3496, -17.2021])\n",
      "Gradient: tensor([-0.0031,  0.0175])\n",
      "Epoch 3024, Loss 2.928569\n",
      "Params: tensor([  5.3496, -17.2022])\n",
      "Gradient: tensor([-0.0031,  0.0174])\n",
      "Epoch 3025, Loss 2.928567\n",
      "Params: tensor([  5.3496, -17.2024])\n",
      "Gradient: tensor([-0.0031,  0.0174])\n",
      "Epoch 3026, Loss 2.928564\n",
      "Params: tensor([  5.3497, -17.2026])\n",
      "Gradient: tensor([-0.0031,  0.0174])\n",
      "Epoch 3027, Loss 2.928560\n",
      "Params: tensor([  5.3497, -17.2028])\n",
      "Gradient: tensor([-0.0031,  0.0174])\n",
      "Epoch 3028, Loss 2.928556\n",
      "Params: tensor([  5.3497, -17.2029])\n",
      "Gradient: tensor([-0.0031,  0.0173])\n",
      "Epoch 3029, Loss 2.928555\n",
      "Params: tensor([  5.3498, -17.2031])\n",
      "Gradient: tensor([-0.0031,  0.0173])\n",
      "Epoch 3030, Loss 2.928551\n",
      "Params: tensor([  5.3498, -17.2033])\n",
      "Gradient: tensor([-0.0031,  0.0173])\n",
      "Epoch 3031, Loss 2.928547\n",
      "Params: tensor([  5.3498, -17.2035])\n",
      "Gradient: tensor([-0.0031,  0.0172])\n",
      "Epoch 3032, Loss 2.928545\n",
      "Params: tensor([  5.3499, -17.2036])\n",
      "Gradient: tensor([-0.0030,  0.0172])\n",
      "Epoch 3033, Loss 2.928543\n",
      "Params: tensor([  5.3499, -17.2038])\n",
      "Gradient: tensor([-0.0030,  0.0172])\n",
      "Epoch 3034, Loss 2.928539\n",
      "Params: tensor([  5.3499, -17.2040])\n",
      "Gradient: tensor([-0.0030,  0.0172])\n",
      "Epoch 3035, Loss 2.928535\n",
      "Params: tensor([  5.3499, -17.2041])\n",
      "Gradient: tensor([-0.0030,  0.0171])\n",
      "Epoch 3036, Loss 2.928533\n",
      "Params: tensor([  5.3500, -17.2043])\n",
      "Gradient: tensor([-0.0030,  0.0171])\n",
      "Epoch 3037, Loss 2.928529\n",
      "Params: tensor([  5.3500, -17.2045])\n",
      "Gradient: tensor([-0.0030,  0.0171])\n",
      "Epoch 3038, Loss 2.928528\n",
      "Params: tensor([  5.3500, -17.2047])\n",
      "Gradient: tensor([-0.0030,  0.0170])\n",
      "Epoch 3039, Loss 2.928526\n",
      "Params: tensor([  5.3501, -17.2048])\n",
      "Gradient: tensor([-0.0030,  0.0170])\n",
      "Epoch 3040, Loss 2.928521\n",
      "Params: tensor([  5.3501, -17.2050])\n",
      "Gradient: tensor([-0.0030,  0.0170])\n",
      "Epoch 3041, Loss 2.928518\n",
      "Params: tensor([  5.3501, -17.2052])\n",
      "Gradient: tensor([-0.0030,  0.0170])\n",
      "Epoch 3042, Loss 2.928515\n",
      "Params: tensor([  5.3502, -17.2053])\n",
      "Gradient: tensor([-0.0030,  0.0169])\n",
      "Epoch 3043, Loss 2.928511\n",
      "Params: tensor([  5.3502, -17.2055])\n",
      "Gradient: tensor([-0.0030,  0.0169])\n",
      "Epoch 3044, Loss 2.928511\n",
      "Params: tensor([  5.3502, -17.2057])\n",
      "Gradient: tensor([-0.0030,  0.0169])\n",
      "Epoch 3045, Loss 2.928506\n",
      "Params: tensor([  5.3502, -17.2058])\n",
      "Gradient: tensor([-0.0030,  0.0168])\n",
      "Epoch 3046, Loss 2.928506\n",
      "Params: tensor([  5.3503, -17.2060])\n",
      "Gradient: tensor([-0.0030,  0.0168])\n",
      "Epoch 3047, Loss 2.928501\n",
      "Params: tensor([  5.3503, -17.2062])\n",
      "Gradient: tensor([-0.0030,  0.0168])\n",
      "Epoch 3048, Loss 2.928498\n",
      "Params: tensor([  5.3503, -17.2063])\n",
      "Gradient: tensor([-0.0030,  0.0168])\n",
      "Epoch 3049, Loss 2.928494\n",
      "Params: tensor([  5.3504, -17.2065])\n",
      "Gradient: tensor([-0.0030,  0.0167])\n",
      "Epoch 3050, Loss 2.928493\n",
      "Params: tensor([  5.3504, -17.2067])\n",
      "Gradient: tensor([-0.0030,  0.0167])\n",
      "Epoch 3051, Loss 2.928491\n",
      "Params: tensor([  5.3504, -17.2068])\n",
      "Gradient: tensor([-0.0030,  0.0167])\n",
      "Epoch 3052, Loss 2.928487\n",
      "Params: tensor([  5.3504, -17.2070])\n",
      "Gradient: tensor([-0.0029,  0.0166])\n",
      "Epoch 3053, Loss 2.928485\n",
      "Params: tensor([  5.3505, -17.2072])\n",
      "Gradient: tensor([-0.0029,  0.0166])\n",
      "Epoch 3054, Loss 2.928480\n",
      "Params: tensor([  5.3505, -17.2073])\n",
      "Gradient: tensor([-0.0029,  0.0166])\n",
      "Epoch 3055, Loss 2.928477\n",
      "Params: tensor([  5.3505, -17.2075])\n",
      "Gradient: tensor([-0.0029,  0.0166])\n",
      "Epoch 3056, Loss 2.928474\n",
      "Params: tensor([  5.3506, -17.2077])\n",
      "Gradient: tensor([-0.0029,  0.0165])\n",
      "Epoch 3057, Loss 2.928472\n",
      "Params: tensor([  5.3506, -17.2078])\n",
      "Gradient: tensor([-0.0029,  0.0165])\n",
      "Epoch 3058, Loss 2.928469\n",
      "Params: tensor([  5.3506, -17.2080])\n",
      "Gradient: tensor([-0.0029,  0.0165])\n",
      "Epoch 3059, Loss 2.928468\n",
      "Params: tensor([  5.3507, -17.2082])\n",
      "Gradient: tensor([-0.0029,  0.0164])\n",
      "Epoch 3060, Loss 2.928464\n",
      "Params: tensor([  5.3507, -17.2083])\n",
      "Gradient: tensor([-0.0029,  0.0164])\n",
      "Epoch 3061, Loss 2.928461\n",
      "Params: tensor([  5.3507, -17.2085])\n",
      "Gradient: tensor([-0.0029,  0.0164])\n",
      "Epoch 3062, Loss 2.928457\n",
      "Params: tensor([  5.3507, -17.2087])\n",
      "Gradient: tensor([-0.0029,  0.0164])\n",
      "Epoch 3063, Loss 2.928455\n",
      "Params: tensor([  5.3508, -17.2088])\n",
      "Gradient: tensor([-0.0029,  0.0163])\n",
      "Epoch 3064, Loss 2.928452\n",
      "Params: tensor([  5.3508, -17.2090])\n",
      "Gradient: tensor([-0.0029,  0.0163])\n",
      "Epoch 3065, Loss 2.928450\n",
      "Params: tensor([  5.3508, -17.2092])\n",
      "Gradient: tensor([-0.0029,  0.0163])\n",
      "Epoch 3066, Loss 2.928447\n",
      "Params: tensor([  5.3509, -17.2093])\n",
      "Gradient: tensor([-0.0029,  0.0162])\n",
      "Epoch 3067, Loss 2.928445\n",
      "Params: tensor([  5.3509, -17.2095])\n",
      "Gradient: tensor([-0.0029,  0.0162])\n",
      "Epoch 3068, Loss 2.928442\n",
      "Params: tensor([  5.3509, -17.2096])\n",
      "Gradient: tensor([-0.0029,  0.0162])\n",
      "Epoch 3069, Loss 2.928440\n",
      "Params: tensor([  5.3509, -17.2098])\n",
      "Gradient: tensor([-0.0029,  0.0162])\n",
      "Epoch 3070, Loss 2.928437\n",
      "Params: tensor([  5.3510, -17.2100])\n",
      "Gradient: tensor([-0.0029,  0.0161])\n",
      "Epoch 3071, Loss 2.928434\n",
      "Params: tensor([  5.3510, -17.2101])\n",
      "Gradient: tensor([-0.0029,  0.0161])\n",
      "Epoch 3072, Loss 2.928430\n",
      "Params: tensor([  5.3510, -17.2103])\n",
      "Gradient: tensor([-0.0028,  0.0161])\n",
      "Epoch 3073, Loss 2.928428\n",
      "Params: tensor([  5.3511, -17.2104])\n",
      "Gradient: tensor([-0.0028,  0.0161])\n",
      "Epoch 3074, Loss 2.928427\n",
      "Params: tensor([  5.3511, -17.2106])\n",
      "Gradient: tensor([-0.0028,  0.0160])\n",
      "Epoch 3075, Loss 2.928423\n",
      "Params: tensor([  5.3511, -17.2108])\n",
      "Gradient: tensor([-0.0028,  0.0160])\n",
      "Epoch 3076, Loss 2.928421\n",
      "Params: tensor([  5.3511, -17.2109])\n",
      "Gradient: tensor([-0.0028,  0.0160])\n",
      "Epoch 3077, Loss 2.928416\n",
      "Params: tensor([  5.3512, -17.2111])\n",
      "Gradient: tensor([-0.0028,  0.0159])\n",
      "Epoch 3078, Loss 2.928416\n",
      "Params: tensor([  5.3512, -17.2112])\n",
      "Gradient: tensor([-0.0028,  0.0159])\n",
      "Epoch 3079, Loss 2.928411\n",
      "Params: tensor([  5.3512, -17.2114])\n",
      "Gradient: tensor([-0.0028,  0.0159])\n",
      "Epoch 3080, Loss 2.928410\n",
      "Params: tensor([  5.3513, -17.2116])\n",
      "Gradient: tensor([-0.0028,  0.0159])\n",
      "Epoch 3081, Loss 2.928407\n",
      "Params: tensor([  5.3513, -17.2117])\n",
      "Gradient: tensor([-0.0028,  0.0158])\n",
      "Epoch 3082, Loss 2.928404\n",
      "Params: tensor([  5.3513, -17.2119])\n",
      "Gradient: tensor([-0.0028,  0.0158])\n",
      "Epoch 3083, Loss 2.928404\n",
      "Params: tensor([  5.3513, -17.2120])\n",
      "Gradient: tensor([-0.0028,  0.0158])\n",
      "Epoch 3084, Loss 2.928399\n",
      "Params: tensor([  5.3514, -17.2122])\n",
      "Gradient: tensor([-0.0028,  0.0158])\n",
      "Epoch 3085, Loss 2.928398\n",
      "Params: tensor([  5.3514, -17.2123])\n",
      "Gradient: tensor([-0.0028,  0.0157])\n",
      "Epoch 3086, Loss 2.928395\n",
      "Params: tensor([  5.3514, -17.2125])\n",
      "Gradient: tensor([-0.0028,  0.0157])\n",
      "Epoch 3087, Loss 2.928392\n",
      "Params: tensor([  5.3514, -17.2127])\n",
      "Gradient: tensor([-0.0028,  0.0157])\n",
      "Epoch 3088, Loss 2.928389\n",
      "Params: tensor([  5.3515, -17.2128])\n",
      "Gradient: tensor([-0.0028,  0.0156])\n",
      "Epoch 3089, Loss 2.928386\n",
      "Params: tensor([  5.3515, -17.2130])\n",
      "Gradient: tensor([-0.0028,  0.0156])\n",
      "Epoch 3090, Loss 2.928384\n",
      "Params: tensor([  5.3515, -17.2131])\n",
      "Gradient: tensor([-0.0028,  0.0156])\n",
      "Epoch 3091, Loss 2.928382\n",
      "Params: tensor([  5.3516, -17.2133])\n",
      "Gradient: tensor([-0.0027,  0.0156])\n",
      "Epoch 3092, Loss 2.928378\n",
      "Params: tensor([  5.3516, -17.2134])\n",
      "Gradient: tensor([-0.0027,  0.0155])\n",
      "Epoch 3093, Loss 2.928378\n",
      "Params: tensor([  5.3516, -17.2136])\n",
      "Gradient: tensor([-0.0028,  0.0155])\n",
      "Epoch 3094, Loss 2.928375\n",
      "Params: tensor([  5.3516, -17.2138])\n",
      "Gradient: tensor([-0.0027,  0.0155])\n",
      "Epoch 3095, Loss 2.928371\n",
      "Params: tensor([  5.3517, -17.2139])\n",
      "Gradient: tensor([-0.0027,  0.0155])\n",
      "Epoch 3096, Loss 2.928370\n",
      "Params: tensor([  5.3517, -17.2141])\n",
      "Gradient: tensor([-0.0027,  0.0154])\n",
      "Epoch 3097, Loss 2.928367\n",
      "Params: tensor([  5.3517, -17.2142])\n",
      "Gradient: tensor([-0.0027,  0.0154])\n",
      "Epoch 3098, Loss 2.928363\n",
      "Params: tensor([  5.3517, -17.2144])\n",
      "Gradient: tensor([-0.0027,  0.0154])\n",
      "Epoch 3099, Loss 2.928362\n",
      "Params: tensor([  5.3518, -17.2145])\n",
      "Gradient: tensor([-0.0027,  0.0154])\n",
      "Epoch 3100, Loss 2.928360\n",
      "Params: tensor([  5.3518, -17.2147])\n",
      "Gradient: tensor([-0.0027,  0.0153])\n",
      "Epoch 3101, Loss 2.928357\n",
      "Params: tensor([  5.3518, -17.2148])\n",
      "Gradient: tensor([-0.0027,  0.0153])\n",
      "Epoch 3102, Loss 2.928354\n",
      "Params: tensor([  5.3519, -17.2150])\n",
      "Gradient: tensor([-0.0027,  0.0153])\n",
      "Epoch 3103, Loss 2.928353\n",
      "Params: tensor([  5.3519, -17.2151])\n",
      "Gradient: tensor([-0.0027,  0.0153])\n",
      "Epoch 3104, Loss 2.928350\n",
      "Params: tensor([  5.3519, -17.2153])\n",
      "Gradient: tensor([-0.0027,  0.0152])\n",
      "Epoch 3105, Loss 2.928348\n",
      "Params: tensor([  5.3519, -17.2154])\n",
      "Gradient: tensor([-0.0027,  0.0152])\n",
      "Epoch 3106, Loss 2.928345\n",
      "Params: tensor([  5.3520, -17.2156])\n",
      "Gradient: tensor([-0.0027,  0.0152])\n",
      "Epoch 3107, Loss 2.928342\n",
      "Params: tensor([  5.3520, -17.2157])\n",
      "Gradient: tensor([-0.0027,  0.0152])\n",
      "Epoch 3108, Loss 2.928341\n",
      "Params: tensor([  5.3520, -17.2159])\n",
      "Gradient: tensor([-0.0027,  0.0151])\n",
      "Epoch 3109, Loss 2.928337\n",
      "Params: tensor([  5.3520, -17.2160])\n",
      "Gradient: tensor([-0.0027,  0.0151])\n",
      "Epoch 3110, Loss 2.928335\n",
      "Params: tensor([  5.3521, -17.2162])\n",
      "Gradient: tensor([-0.0027,  0.0151])\n",
      "Epoch 3111, Loss 2.928333\n",
      "Params: tensor([  5.3521, -17.2163])\n",
      "Gradient: tensor([-0.0026,  0.0151])\n",
      "Epoch 3112, Loss 2.928332\n",
      "Params: tensor([  5.3521, -17.2165])\n",
      "Gradient: tensor([-0.0027,  0.0150])\n",
      "Epoch 3113, Loss 2.928329\n",
      "Params: tensor([  5.3521, -17.2166])\n",
      "Gradient: tensor([-0.0026,  0.0150])\n",
      "Epoch 3114, Loss 2.928326\n",
      "Params: tensor([  5.3522, -17.2168])\n",
      "Gradient: tensor([-0.0026,  0.0150])\n",
      "Epoch 3115, Loss 2.928325\n",
      "Params: tensor([  5.3522, -17.2169])\n",
      "Gradient: tensor([-0.0027,  0.0149])\n",
      "Epoch 3116, Loss 2.928321\n",
      "Params: tensor([  5.3522, -17.2171])\n",
      "Gradient: tensor([-0.0026,  0.0149])\n",
      "Epoch 3117, Loss 2.928319\n",
      "Params: tensor([  5.3523, -17.2172])\n",
      "Gradient: tensor([-0.0026,  0.0149])\n",
      "Epoch 3118, Loss 2.928317\n",
      "Params: tensor([  5.3523, -17.2174])\n",
      "Gradient: tensor([-0.0026,  0.0149])\n",
      "Epoch 3119, Loss 2.928315\n",
      "Params: tensor([  5.3523, -17.2175])\n",
      "Gradient: tensor([-0.0026,  0.0148])\n",
      "Epoch 3120, Loss 2.928313\n",
      "Params: tensor([  5.3523, -17.2177])\n",
      "Gradient: tensor([-0.0026,  0.0148])\n",
      "Epoch 3121, Loss 2.928311\n",
      "Params: tensor([  5.3524, -17.2178])\n",
      "Gradient: tensor([-0.0026,  0.0148])\n",
      "Epoch 3122, Loss 2.928308\n",
      "Params: tensor([  5.3524, -17.2180])\n",
      "Gradient: tensor([-0.0026,  0.0148])\n",
      "Epoch 3123, Loss 2.928306\n",
      "Params: tensor([  5.3524, -17.2181])\n",
      "Gradient: tensor([-0.0026,  0.0147])\n",
      "Epoch 3124, Loss 2.928304\n",
      "Params: tensor([  5.3524, -17.2183])\n",
      "Gradient: tensor([-0.0026,  0.0147])\n",
      "Epoch 3125, Loss 2.928301\n",
      "Params: tensor([  5.3525, -17.2184])\n",
      "Gradient: tensor([-0.0026,  0.0147])\n",
      "Epoch 3126, Loss 2.928299\n",
      "Params: tensor([  5.3525, -17.2186])\n",
      "Gradient: tensor([-0.0026,  0.0147])\n",
      "Epoch 3127, Loss 2.928298\n",
      "Params: tensor([  5.3525, -17.2187])\n",
      "Gradient: tensor([-0.0026,  0.0146])\n",
      "Epoch 3128, Loss 2.928296\n",
      "Params: tensor([  5.3525, -17.2189])\n",
      "Gradient: tensor([-0.0026,  0.0146])\n",
      "Epoch 3129, Loss 2.928292\n",
      "Params: tensor([  5.3526, -17.2190])\n",
      "Gradient: tensor([-0.0026,  0.0146])\n",
      "Epoch 3130, Loss 2.928290\n",
      "Params: tensor([  5.3526, -17.2192])\n",
      "Gradient: tensor([-0.0026,  0.0146])\n",
      "Epoch 3131, Loss 2.928289\n",
      "Params: tensor([  5.3526, -17.2193])\n",
      "Gradient: tensor([-0.0026,  0.0145])\n",
      "Epoch 3132, Loss 2.928285\n",
      "Params: tensor([  5.3526, -17.2194])\n",
      "Gradient: tensor([-0.0026,  0.0145])\n",
      "Epoch 3133, Loss 2.928283\n",
      "Params: tensor([  5.3527, -17.2196])\n",
      "Gradient: tensor([-0.0026,  0.0145])\n",
      "Epoch 3134, Loss 2.928281\n",
      "Params: tensor([  5.3527, -17.2197])\n",
      "Gradient: tensor([-0.0025,  0.0145])\n",
      "Epoch 3135, Loss 2.928280\n",
      "Params: tensor([  5.3527, -17.2199])\n",
      "Gradient: tensor([-0.0026,  0.0144])\n",
      "Epoch 3136, Loss 2.928278\n",
      "Params: tensor([  5.3527, -17.2200])\n",
      "Gradient: tensor([-0.0025,  0.0144])\n",
      "Epoch 3137, Loss 2.928276\n",
      "Params: tensor([  5.3528, -17.2202])\n",
      "Gradient: tensor([-0.0026,  0.0144])\n",
      "Epoch 3138, Loss 2.928272\n",
      "Params: tensor([  5.3528, -17.2203])\n",
      "Gradient: tensor([-0.0025,  0.0144])\n",
      "Epoch 3139, Loss 2.928272\n",
      "Params: tensor([  5.3528, -17.2205])\n",
      "Gradient: tensor([-0.0025,  0.0144])\n",
      "Epoch 3140, Loss 2.928270\n",
      "Params: tensor([  5.3528, -17.2206])\n",
      "Gradient: tensor([-0.0025,  0.0143])\n",
      "Epoch 3141, Loss 2.928267\n",
      "Params: tensor([  5.3529, -17.2207])\n",
      "Gradient: tensor([-0.0025,  0.0143])\n",
      "Epoch 3142, Loss 2.928265\n",
      "Params: tensor([  5.3529, -17.2209])\n",
      "Gradient: tensor([-0.0025,  0.0143])\n",
      "Epoch 3143, Loss 2.928264\n",
      "Params: tensor([  5.3529, -17.2210])\n",
      "Gradient: tensor([-0.0025,  0.0143])\n",
      "Epoch 3144, Loss 2.928261\n",
      "Params: tensor([  5.3529, -17.2212])\n",
      "Gradient: tensor([-0.0025,  0.0142])\n",
      "Epoch 3145, Loss 2.928258\n",
      "Params: tensor([  5.3530, -17.2213])\n",
      "Gradient: tensor([-0.0025,  0.0142])\n",
      "Epoch 3146, Loss 2.928257\n",
      "Params: tensor([  5.3530, -17.2215])\n",
      "Gradient: tensor([-0.0025,  0.0142])\n",
      "Epoch 3147, Loss 2.928255\n",
      "Params: tensor([  5.3530, -17.2216])\n",
      "Gradient: tensor([-0.0025,  0.0142])\n",
      "Epoch 3148, Loss 2.928252\n",
      "Params: tensor([  5.3530, -17.2217])\n",
      "Gradient: tensor([-0.0025,  0.0141])\n",
      "Epoch 3149, Loss 2.928249\n",
      "Params: tensor([  5.3531, -17.2219])\n",
      "Gradient: tensor([-0.0025,  0.0141])\n",
      "Epoch 3150, Loss 2.928248\n",
      "Params: tensor([  5.3531, -17.2220])\n",
      "Gradient: tensor([-0.0025,  0.0141])\n",
      "Epoch 3151, Loss 2.928246\n",
      "Params: tensor([  5.3531, -17.2222])\n",
      "Gradient: tensor([-0.0025,  0.0141])\n",
      "Epoch 3152, Loss 2.928243\n",
      "Params: tensor([  5.3531, -17.2223])\n",
      "Gradient: tensor([-0.0025,  0.0140])\n",
      "Epoch 3153, Loss 2.928242\n",
      "Params: tensor([  5.3532, -17.2224])\n",
      "Gradient: tensor([-0.0025,  0.0140])\n",
      "Epoch 3154, Loss 2.928240\n",
      "Params: tensor([  5.3532, -17.2226])\n",
      "Gradient: tensor([-0.0025,  0.0140])\n",
      "Epoch 3155, Loss 2.928239\n",
      "Params: tensor([  5.3532, -17.2227])\n",
      "Gradient: tensor([-0.0025,  0.0140])\n",
      "Epoch 3156, Loss 2.928236\n",
      "Params: tensor([  5.3532, -17.2229])\n",
      "Gradient: tensor([-0.0025,  0.0139])\n",
      "Epoch 3157, Loss 2.928234\n",
      "Params: tensor([  5.3533, -17.2230])\n",
      "Gradient: tensor([-0.0025,  0.0139])\n",
      "Epoch 3158, Loss 2.928232\n",
      "Params: tensor([  5.3533, -17.2231])\n",
      "Gradient: tensor([-0.0024,  0.0139])\n",
      "Epoch 3159, Loss 2.928230\n",
      "Params: tensor([  5.3533, -17.2233])\n",
      "Gradient: tensor([-0.0025,  0.0139])\n",
      "Epoch 3160, Loss 2.928228\n",
      "Params: tensor([  5.3533, -17.2234])\n",
      "Gradient: tensor([-0.0024,  0.0138])\n",
      "Epoch 3161, Loss 2.928226\n",
      "Params: tensor([  5.3534, -17.2236])\n",
      "Gradient: tensor([-0.0025,  0.0138])\n",
      "Epoch 3162, Loss 2.928224\n",
      "Params: tensor([  5.3534, -17.2237])\n",
      "Gradient: tensor([-0.0024,  0.0138])\n",
      "Epoch 3163, Loss 2.928222\n",
      "Params: tensor([  5.3534, -17.2238])\n",
      "Gradient: tensor([-0.0024,  0.0138])\n",
      "Epoch 3164, Loss 2.928222\n",
      "Params: tensor([  5.3534, -17.2240])\n",
      "Gradient: tensor([-0.0024,  0.0138])\n",
      "Epoch 3165, Loss 2.928216\n",
      "Params: tensor([  5.3535, -17.2241])\n",
      "Gradient: tensor([-0.0024,  0.0137])\n",
      "Epoch 3166, Loss 2.928216\n",
      "Params: tensor([  5.3535, -17.2242])\n",
      "Gradient: tensor([-0.0024,  0.0137])\n",
      "Epoch 3167, Loss 2.928214\n",
      "Params: tensor([  5.3535, -17.2244])\n",
      "Gradient: tensor([-0.0024,  0.0137])\n",
      "Epoch 3168, Loss 2.928213\n",
      "Params: tensor([  5.3535, -17.2245])\n",
      "Gradient: tensor([-0.0024,  0.0137])\n",
      "Epoch 3169, Loss 2.928211\n",
      "Params: tensor([  5.3536, -17.2246])\n",
      "Gradient: tensor([-0.0024,  0.0136])\n",
      "Epoch 3170, Loss 2.928209\n",
      "Params: tensor([  5.3536, -17.2248])\n",
      "Gradient: tensor([-0.0024,  0.0136])\n",
      "Epoch 3171, Loss 2.928206\n",
      "Params: tensor([  5.3536, -17.2249])\n",
      "Gradient: tensor([-0.0024,  0.0136])\n",
      "Epoch 3172, Loss 2.928205\n",
      "Params: tensor([  5.3536, -17.2251])\n",
      "Gradient: tensor([-0.0024,  0.0136])\n",
      "Epoch 3173, Loss 2.928203\n",
      "Params: tensor([  5.3537, -17.2252])\n",
      "Gradient: tensor([-0.0024,  0.0135])\n",
      "Epoch 3174, Loss 2.928201\n",
      "Params: tensor([  5.3537, -17.2253])\n",
      "Gradient: tensor([-0.0024,  0.0135])\n",
      "Epoch 3175, Loss 2.928198\n",
      "Params: tensor([  5.3537, -17.2255])\n",
      "Gradient: tensor([-0.0024,  0.0135])\n",
      "Epoch 3176, Loss 2.928197\n",
      "Params: tensor([  5.3537, -17.2256])\n",
      "Gradient: tensor([-0.0024,  0.0135])\n",
      "Epoch 3177, Loss 2.928195\n",
      "Params: tensor([  5.3538, -17.2257])\n",
      "Gradient: tensor([-0.0024,  0.0134])\n",
      "Epoch 3178, Loss 2.928194\n",
      "Params: tensor([  5.3538, -17.2259])\n",
      "Gradient: tensor([-0.0024,  0.0134])\n",
      "Epoch 3179, Loss 2.928192\n",
      "Params: tensor([  5.3538, -17.2260])\n",
      "Gradient: tensor([-0.0024,  0.0134])\n",
      "Epoch 3180, Loss 2.928190\n",
      "Params: tensor([  5.3538, -17.2261])\n",
      "Gradient: tensor([-0.0024,  0.0134])\n",
      "Epoch 3181, Loss 2.928187\n",
      "Params: tensor([  5.3538, -17.2263])\n",
      "Gradient: tensor([-0.0023,  0.0134])\n",
      "Epoch 3182, Loss 2.928186\n",
      "Params: tensor([  5.3539, -17.2264])\n",
      "Gradient: tensor([-0.0024,  0.0133])\n",
      "Epoch 3183, Loss 2.928185\n",
      "Params: tensor([  5.3539, -17.2265])\n",
      "Gradient: tensor([-0.0024,  0.0133])\n",
      "Epoch 3184, Loss 2.928182\n",
      "Params: tensor([  5.3539, -17.2267])\n",
      "Gradient: tensor([-0.0023,  0.0133])\n",
      "Epoch 3185, Loss 2.928181\n",
      "Params: tensor([  5.3539, -17.2268])\n",
      "Gradient: tensor([-0.0024,  0.0133])\n",
      "Epoch 3186, Loss 2.928178\n",
      "Params: tensor([  5.3540, -17.2269])\n",
      "Gradient: tensor([-0.0024,  0.0132])\n",
      "Epoch 3187, Loss 2.928177\n",
      "Params: tensor([  5.3540, -17.2271])\n",
      "Gradient: tensor([-0.0023,  0.0132])\n",
      "Epoch 3188, Loss 2.928177\n",
      "Params: tensor([  5.3540, -17.2272])\n",
      "Gradient: tensor([-0.0023,  0.0132])\n",
      "Epoch 3189, Loss 2.928174\n",
      "Params: tensor([  5.3540, -17.2273])\n",
      "Gradient: tensor([-0.0023,  0.0132])\n",
      "Epoch 3190, Loss 2.928171\n",
      "Params: tensor([  5.3541, -17.2275])\n",
      "Gradient: tensor([-0.0023,  0.0132])\n",
      "Epoch 3191, Loss 2.928170\n",
      "Params: tensor([  5.3541, -17.2276])\n",
      "Gradient: tensor([-0.0023,  0.0131])\n",
      "Epoch 3192, Loss 2.928168\n",
      "Params: tensor([  5.3541, -17.2277])\n",
      "Gradient: tensor([-0.0023,  0.0131])\n",
      "Epoch 3193, Loss 2.928166\n",
      "Params: tensor([  5.3541, -17.2279])\n",
      "Gradient: tensor([-0.0023,  0.0131])\n",
      "Epoch 3194, Loss 2.928164\n",
      "Params: tensor([  5.3542, -17.2280])\n",
      "Gradient: tensor([-0.0023,  0.0131])\n",
      "Epoch 3195, Loss 2.928162\n",
      "Params: tensor([  5.3542, -17.2281])\n",
      "Gradient: tensor([-0.0023,  0.0130])\n",
      "Epoch 3196, Loss 2.928160\n",
      "Params: tensor([  5.3542, -17.2282])\n",
      "Gradient: tensor([-0.0023,  0.0130])\n",
      "Epoch 3197, Loss 2.928159\n",
      "Params: tensor([  5.3542, -17.2284])\n",
      "Gradient: tensor([-0.0023,  0.0130])\n",
      "Epoch 3198, Loss 2.928158\n",
      "Params: tensor([  5.3542, -17.2285])\n",
      "Gradient: tensor([-0.0023,  0.0130])\n",
      "Epoch 3199, Loss 2.928156\n",
      "Params: tensor([  5.3543, -17.2286])\n",
      "Gradient: tensor([-0.0023,  0.0130])\n",
      "Epoch 3200, Loss 2.928153\n",
      "Params: tensor([  5.3543, -17.2288])\n",
      "Gradient: tensor([-0.0023,  0.0129])\n",
      "Epoch 3201, Loss 2.928153\n",
      "Params: tensor([  5.3543, -17.2289])\n",
      "Gradient: tensor([-0.0023,  0.0129])\n",
      "Epoch 3202, Loss 2.928151\n",
      "Params: tensor([  5.3543, -17.2290])\n",
      "Gradient: tensor([-0.0023,  0.0129])\n",
      "Epoch 3203, Loss 2.928149\n",
      "Params: tensor([  5.3544, -17.2292])\n",
      "Gradient: tensor([-0.0023,  0.0129])\n",
      "Epoch 3204, Loss 2.928146\n",
      "Params: tensor([  5.3544, -17.2293])\n",
      "Gradient: tensor([-0.0023,  0.0129])\n",
      "Epoch 3205, Loss 2.928146\n",
      "Params: tensor([  5.3544, -17.2294])\n",
      "Gradient: tensor([-0.0023,  0.0128])\n",
      "Epoch 3206, Loss 2.928144\n",
      "Params: tensor([  5.3544, -17.2295])\n",
      "Gradient: tensor([-0.0023,  0.0128])\n",
      "Epoch 3207, Loss 2.928142\n",
      "Params: tensor([  5.3544, -17.2297])\n",
      "Gradient: tensor([-0.0023,  0.0128])\n",
      "Epoch 3208, Loss 2.928140\n",
      "Params: tensor([  5.3545, -17.2298])\n",
      "Gradient: tensor([-0.0023,  0.0128])\n",
      "Epoch 3209, Loss 2.928139\n",
      "Params: tensor([  5.3545, -17.2299])\n",
      "Gradient: tensor([-0.0023,  0.0127])\n",
      "Epoch 3210, Loss 2.928137\n",
      "Params: tensor([  5.3545, -17.2300])\n",
      "Gradient: tensor([-0.0023,  0.0127])\n",
      "Epoch 3211, Loss 2.928137\n",
      "Params: tensor([  5.3545, -17.2302])\n",
      "Gradient: tensor([-0.0023,  0.0127])\n",
      "Epoch 3212, Loss 2.928133\n",
      "Params: tensor([  5.3546, -17.2303])\n",
      "Gradient: tensor([-0.0023,  0.0127])\n",
      "Epoch 3213, Loss 2.928130\n",
      "Params: tensor([  5.3546, -17.2304])\n",
      "Gradient: tensor([-0.0022,  0.0127])\n",
      "Epoch 3214, Loss 2.928131\n",
      "Params: tensor([  5.3546, -17.2306])\n",
      "Gradient: tensor([-0.0022,  0.0126])\n",
      "Epoch 3215, Loss 2.928128\n",
      "Params: tensor([  5.3546, -17.2307])\n",
      "Gradient: tensor([-0.0022,  0.0126])\n",
      "Epoch 3216, Loss 2.928128\n",
      "Params: tensor([  5.3546, -17.2308])\n",
      "Gradient: tensor([-0.0022,  0.0126])\n",
      "Epoch 3217, Loss 2.928126\n",
      "Params: tensor([  5.3547, -17.2309])\n",
      "Gradient: tensor([-0.0022,  0.0126])\n",
      "Epoch 3218, Loss 2.928123\n",
      "Params: tensor([  5.3547, -17.2311])\n",
      "Gradient: tensor([-0.0022,  0.0125])\n",
      "Epoch 3219, Loss 2.928123\n",
      "Params: tensor([  5.3547, -17.2312])\n",
      "Gradient: tensor([-0.0022,  0.0125])\n",
      "Epoch 3220, Loss 2.928121\n",
      "Params: tensor([  5.3547, -17.2313])\n",
      "Gradient: tensor([-0.0022,  0.0125])\n",
      "Epoch 3221, Loss 2.928118\n",
      "Params: tensor([  5.3548, -17.2314])\n",
      "Gradient: tensor([-0.0022,  0.0125])\n",
      "Epoch 3222, Loss 2.928118\n",
      "Params: tensor([  5.3548, -17.2316])\n",
      "Gradient: tensor([-0.0022,  0.0125])\n",
      "Epoch 3223, Loss 2.928116\n",
      "Params: tensor([  5.3548, -17.2317])\n",
      "Gradient: tensor([-0.0022,  0.0124])\n",
      "Epoch 3224, Loss 2.928115\n",
      "Params: tensor([  5.3548, -17.2318])\n",
      "Gradient: tensor([-0.0022,  0.0124])\n",
      "Epoch 3225, Loss 2.928111\n",
      "Params: tensor([  5.3548, -17.2319])\n",
      "Gradient: tensor([-0.0022,  0.0124])\n",
      "Epoch 3226, Loss 2.928111\n",
      "Params: tensor([  5.3549, -17.2320])\n",
      "Gradient: tensor([-0.0022,  0.0124])\n",
      "Epoch 3227, Loss 2.928108\n",
      "Params: tensor([  5.3549, -17.2322])\n",
      "Gradient: tensor([-0.0022,  0.0124])\n",
      "Epoch 3228, Loss 2.928108\n",
      "Params: tensor([  5.3549, -17.2323])\n",
      "Gradient: tensor([-0.0022,  0.0123])\n",
      "Epoch 3229, Loss 2.928107\n",
      "Params: tensor([  5.3549, -17.2324])\n",
      "Gradient: tensor([-0.0022,  0.0123])\n",
      "Epoch 3230, Loss 2.928104\n",
      "Params: tensor([  5.3550, -17.2325])\n",
      "Gradient: tensor([-0.0022,  0.0123])\n",
      "Epoch 3231, Loss 2.928103\n",
      "Params: tensor([  5.3550, -17.2327])\n",
      "Gradient: tensor([-0.0022,  0.0123])\n",
      "Epoch 3232, Loss 2.928101\n",
      "Params: tensor([  5.3550, -17.2328])\n",
      "Gradient: tensor([-0.0022,  0.0123])\n",
      "Epoch 3233, Loss 2.928100\n",
      "Params: tensor([  5.3550, -17.2329])\n",
      "Gradient: tensor([-0.0022,  0.0122])\n",
      "Epoch 3234, Loss 2.928099\n",
      "Params: tensor([  5.3550, -17.2330])\n",
      "Gradient: tensor([-0.0022,  0.0122])\n",
      "Epoch 3235, Loss 2.928097\n",
      "Params: tensor([  5.3551, -17.2332])\n",
      "Gradient: tensor([-0.0022,  0.0122])\n",
      "Epoch 3236, Loss 2.928095\n",
      "Params: tensor([  5.3551, -17.2333])\n",
      "Gradient: tensor([-0.0022,  0.0122])\n",
      "Epoch 3237, Loss 2.928094\n",
      "Params: tensor([  5.3551, -17.2334])\n",
      "Gradient: tensor([-0.0022,  0.0121])\n",
      "Epoch 3238, Loss 2.928092\n",
      "Params: tensor([  5.3551, -17.2335])\n",
      "Gradient: tensor([-0.0022,  0.0121])\n",
      "Epoch 3239, Loss 2.928091\n",
      "Params: tensor([  5.3552, -17.2336])\n",
      "Gradient: tensor([-0.0022,  0.0121])\n",
      "Epoch 3240, Loss 2.928089\n",
      "Params: tensor([  5.3552, -17.2338])\n",
      "Gradient: tensor([-0.0022,  0.0121])\n",
      "Epoch 3241, Loss 2.928088\n",
      "Params: tensor([  5.3552, -17.2339])\n",
      "Gradient: tensor([-0.0021,  0.0121])\n",
      "Epoch 3242, Loss 2.928086\n",
      "Params: tensor([  5.3552, -17.2340])\n",
      "Gradient: tensor([-0.0021,  0.0120])\n",
      "Epoch 3243, Loss 2.928083\n",
      "Params: tensor([  5.3552, -17.2341])\n",
      "Gradient: tensor([-0.0021,  0.0120])\n",
      "Epoch 3244, Loss 2.928084\n",
      "Params: tensor([  5.3553, -17.2342])\n",
      "Gradient: tensor([-0.0021,  0.0120])\n",
      "Epoch 3245, Loss 2.928082\n",
      "Params: tensor([  5.3553, -17.2344])\n",
      "Gradient: tensor([-0.0021,  0.0120])\n",
      "Epoch 3246, Loss 2.928082\n",
      "Params: tensor([  5.3553, -17.2345])\n",
      "Gradient: tensor([-0.0021,  0.0120])\n",
      "Epoch 3247, Loss 2.928079\n",
      "Params: tensor([  5.3553, -17.2346])\n",
      "Gradient: tensor([-0.0021,  0.0119])\n",
      "Epoch 3248, Loss 2.928077\n",
      "Params: tensor([  5.3553, -17.2347])\n",
      "Gradient: tensor([-0.0021,  0.0119])\n",
      "Epoch 3249, Loss 2.928077\n",
      "Params: tensor([  5.3554, -17.2348])\n",
      "Gradient: tensor([-0.0021,  0.0119])\n",
      "Epoch 3250, Loss 2.928075\n",
      "Params: tensor([  5.3554, -17.2350])\n",
      "Gradient: tensor([-0.0021,  0.0119])\n",
      "Epoch 3251, Loss 2.928073\n",
      "Params: tensor([  5.3554, -17.2351])\n",
      "Gradient: tensor([-0.0021,  0.0119])\n",
      "Epoch 3252, Loss 2.928072\n",
      "Params: tensor([  5.3554, -17.2352])\n",
      "Gradient: tensor([-0.0021,  0.0118])\n",
      "Epoch 3253, Loss 2.928070\n",
      "Params: tensor([  5.3554, -17.2353])\n",
      "Gradient: tensor([-0.0021,  0.0118])\n",
      "Epoch 3254, Loss 2.928068\n",
      "Params: tensor([  5.3555, -17.2354])\n",
      "Gradient: tensor([-0.0021,  0.0118])\n",
      "Epoch 3255, Loss 2.928068\n",
      "Params: tensor([  5.3555, -17.2355])\n",
      "Gradient: tensor([-0.0021,  0.0118])\n",
      "Epoch 3256, Loss 2.928067\n",
      "Params: tensor([  5.3555, -17.2357])\n",
      "Gradient: tensor([-0.0021,  0.0118])\n",
      "Epoch 3257, Loss 2.928064\n",
      "Params: tensor([  5.3555, -17.2358])\n",
      "Gradient: tensor([-0.0021,  0.0117])\n",
      "Epoch 3258, Loss 2.928062\n",
      "Params: tensor([  5.3556, -17.2359])\n",
      "Gradient: tensor([-0.0021,  0.0117])\n",
      "Epoch 3259, Loss 2.928061\n",
      "Params: tensor([  5.3556, -17.2360])\n",
      "Gradient: tensor([-0.0021,  0.0117])\n",
      "Epoch 3260, Loss 2.928059\n",
      "Params: tensor([  5.3556, -17.2361])\n",
      "Gradient: tensor([-0.0021,  0.0117])\n",
      "Epoch 3261, Loss 2.928059\n",
      "Params: tensor([  5.3556, -17.2362])\n",
      "Gradient: tensor([-0.0021,  0.0117])\n",
      "Epoch 3262, Loss 2.928057\n",
      "Params: tensor([  5.3556, -17.2364])\n",
      "Gradient: tensor([-0.0021,  0.0116])\n",
      "Epoch 3263, Loss 2.928056\n",
      "Params: tensor([  5.3557, -17.2365])\n",
      "Gradient: tensor([-0.0021,  0.0116])\n",
      "Epoch 3264, Loss 2.928053\n",
      "Params: tensor([  5.3557, -17.2366])\n",
      "Gradient: tensor([-0.0021,  0.0116])\n",
      "Epoch 3265, Loss 2.928053\n",
      "Params: tensor([  5.3557, -17.2367])\n",
      "Gradient: tensor([-0.0021,  0.0116])\n",
      "Epoch 3266, Loss 2.928051\n",
      "Params: tensor([  5.3557, -17.2368])\n",
      "Gradient: tensor([-0.0021,  0.0116])\n",
      "Epoch 3267, Loss 2.928050\n",
      "Params: tensor([  5.3557, -17.2369])\n",
      "Gradient: tensor([-0.0021,  0.0115])\n",
      "Epoch 3268, Loss 2.928049\n",
      "Params: tensor([  5.3558, -17.2371])\n",
      "Gradient: tensor([-0.0021,  0.0115])\n",
      "Epoch 3269, Loss 2.928048\n",
      "Params: tensor([  5.3558, -17.2372])\n",
      "Gradient: tensor([-0.0020,  0.0115])\n",
      "Epoch 3270, Loss 2.928046\n",
      "Params: tensor([  5.3558, -17.2373])\n",
      "Gradient: tensor([-0.0020,  0.0115])\n",
      "Epoch 3271, Loss 2.928045\n",
      "Params: tensor([  5.3558, -17.2374])\n",
      "Gradient: tensor([-0.0020,  0.0115])\n",
      "Epoch 3272, Loss 2.928043\n",
      "Params: tensor([  5.3558, -17.2375])\n",
      "Gradient: tensor([-0.0020,  0.0114])\n",
      "Epoch 3273, Loss 2.928042\n",
      "Params: tensor([  5.3559, -17.2376])\n",
      "Gradient: tensor([-0.0020,  0.0114])\n",
      "Epoch 3274, Loss 2.928042\n",
      "Params: tensor([  5.3559, -17.2377])\n",
      "Gradient: tensor([-0.0020,  0.0114])\n",
      "Epoch 3275, Loss 2.928039\n",
      "Params: tensor([  5.3559, -17.2379])\n",
      "Gradient: tensor([-0.0020,  0.0114])\n",
      "Epoch 3276, Loss 2.928040\n",
      "Params: tensor([  5.3559, -17.2380])\n",
      "Gradient: tensor([-0.0020,  0.0114])\n",
      "Epoch 3277, Loss 2.928036\n",
      "Params: tensor([  5.3559, -17.2381])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3278, Loss 2.928035\n",
      "Params: tensor([  5.3560, -17.2382])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3279, Loss 2.928035\n",
      "Params: tensor([  5.3560, -17.2383])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3280, Loss 2.928033\n",
      "Params: tensor([  5.3560, -17.2384])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3281, Loss 2.928032\n",
      "Params: tensor([  5.3560, -17.2385])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3282, Loss 2.928030\n",
      "Params: tensor([  5.3560, -17.2387])\n",
      "Gradient: tensor([-0.0020,  0.0113])\n",
      "Epoch 3283, Loss 2.928028\n",
      "Params: tensor([  5.3561, -17.2388])\n",
      "Gradient: tensor([-0.0020,  0.0112])\n",
      "Epoch 3284, Loss 2.928027\n",
      "Params: tensor([  5.3561, -17.2389])\n",
      "Gradient: tensor([-0.0020,  0.0112])\n",
      "Epoch 3285, Loss 2.928027\n",
      "Params: tensor([  5.3561, -17.2390])\n",
      "Gradient: tensor([-0.0020,  0.0112])\n",
      "Epoch 3286, Loss 2.928025\n",
      "Params: tensor([  5.3561, -17.2391])\n",
      "Gradient: tensor([-0.0020,  0.0112])\n",
      "Epoch 3287, Loss 2.928023\n",
      "Params: tensor([  5.3561, -17.2392])\n",
      "Gradient: tensor([-0.0020,  0.0112])\n",
      "Epoch 3288, Loss 2.928022\n",
      "Params: tensor([  5.3562, -17.2393])\n",
      "Gradient: tensor([-0.0019,  0.0111])\n",
      "Epoch 3289, Loss 2.928020\n",
      "Params: tensor([  5.3562, -17.2394])\n",
      "Gradient: tensor([-0.0019,  0.0111])\n",
      "Epoch 3290, Loss 2.928020\n",
      "Params: tensor([  5.3562, -17.2395])\n",
      "Gradient: tensor([-0.0019,  0.0111])\n",
      "Epoch 3291, Loss 2.928019\n",
      "Params: tensor([  5.3562, -17.2397])\n",
      "Gradient: tensor([-0.0019,  0.0111])\n",
      "Epoch 3292, Loss 2.928017\n",
      "Params: tensor([  5.3562, -17.2398])\n",
      "Gradient: tensor([-0.0019,  0.0111])\n",
      "Epoch 3293, Loss 2.928016\n",
      "Params: tensor([  5.3563, -17.2399])\n",
      "Gradient: tensor([-0.0019,  0.0110])\n",
      "Epoch 3294, Loss 2.928014\n",
      "Params: tensor([  5.3563, -17.2400])\n",
      "Gradient: tensor([-0.0020,  0.0110])\n",
      "Epoch 3295, Loss 2.928014\n",
      "Params: tensor([  5.3563, -17.2401])\n",
      "Gradient: tensor([-0.0020,  0.0110])\n",
      "Epoch 3296, Loss 2.928013\n",
      "Params: tensor([  5.3563, -17.2402])\n",
      "Gradient: tensor([-0.0019,  0.0110])\n",
      "Epoch 3297, Loss 2.928011\n",
      "Params: tensor([  5.3563, -17.2403])\n",
      "Gradient: tensor([-0.0020,  0.0110])\n",
      "Epoch 3298, Loss 2.928011\n",
      "Params: tensor([  5.3564, -17.2404])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3299, Loss 2.928010\n",
      "Params: tensor([  5.3564, -17.2405])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3300, Loss 2.928008\n",
      "Params: tensor([  5.3564, -17.2406])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3301, Loss 2.928006\n",
      "Params: tensor([  5.3564, -17.2408])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3302, Loss 2.928005\n",
      "Params: tensor([  5.3564, -17.2409])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3303, Loss 2.928003\n",
      "Params: tensor([  5.3564, -17.2410])\n",
      "Gradient: tensor([-0.0019,  0.0109])\n",
      "Epoch 3304, Loss 2.928004\n",
      "Params: tensor([  5.3565, -17.2411])\n",
      "Gradient: tensor([-0.0019,  0.0108])\n",
      "Epoch 3305, Loss 2.928001\n",
      "Params: tensor([  5.3565, -17.2412])\n",
      "Gradient: tensor([-0.0019,  0.0108])\n",
      "Epoch 3306, Loss 2.927999\n",
      "Params: tensor([  5.3565, -17.2413])\n",
      "Gradient: tensor([-0.0019,  0.0108])\n",
      "Epoch 3307, Loss 2.927999\n",
      "Params: tensor([  5.3565, -17.2414])\n",
      "Gradient: tensor([-0.0019,  0.0108])\n",
      "Epoch 3308, Loss 2.927997\n",
      "Params: tensor([  5.3565, -17.2415])\n",
      "Gradient: tensor([-0.0019,  0.0108])\n",
      "Epoch 3309, Loss 2.927997\n",
      "Params: tensor([  5.3566, -17.2416])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3310, Loss 2.927995\n",
      "Params: tensor([  5.3566, -17.2417])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3311, Loss 2.927994\n",
      "Params: tensor([  5.3566, -17.2418])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3312, Loss 2.927993\n",
      "Params: tensor([  5.3566, -17.2419])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3313, Loss 2.927992\n",
      "Params: tensor([  5.3566, -17.2420])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3314, Loss 2.927990\n",
      "Params: tensor([  5.3567, -17.2422])\n",
      "Gradient: tensor([-0.0019,  0.0107])\n",
      "Epoch 3315, Loss 2.927989\n",
      "Params: tensor([  5.3567, -17.2423])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3316, Loss 2.927988\n",
      "Params: tensor([  5.3567, -17.2424])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3317, Loss 2.927988\n",
      "Params: tensor([  5.3567, -17.2425])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3318, Loss 2.927985\n",
      "Params: tensor([  5.3567, -17.2426])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3319, Loss 2.927987\n",
      "Params: tensor([  5.3567, -17.2427])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3320, Loss 2.927984\n",
      "Params: tensor([  5.3568, -17.2428])\n",
      "Gradient: tensor([-0.0019,  0.0106])\n",
      "Epoch 3321, Loss 2.927981\n",
      "Params: tensor([  5.3568, -17.2429])\n",
      "Gradient: tensor([-0.0018,  0.0105])\n",
      "Epoch 3322, Loss 2.927982\n",
      "Params: tensor([  5.3568, -17.2430])\n",
      "Gradient: tensor([-0.0018,  0.0105])\n",
      "Epoch 3323, Loss 2.927981\n",
      "Params: tensor([  5.3568, -17.2431])\n",
      "Gradient: tensor([-0.0018,  0.0105])\n",
      "Epoch 3324, Loss 2.927980\n",
      "Params: tensor([  5.3568, -17.2432])\n",
      "Gradient: tensor([-0.0018,  0.0105])\n",
      "Epoch 3325, Loss 2.927979\n",
      "Params: tensor([  5.3569, -17.2433])\n",
      "Gradient: tensor([-0.0018,  0.0105])\n",
      "Epoch 3326, Loss 2.927977\n",
      "Params: tensor([  5.3569, -17.2434])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3327, Loss 2.927975\n",
      "Params: tensor([  5.3569, -17.2435])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3328, Loss 2.927974\n",
      "Params: tensor([  5.3569, -17.2436])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3329, Loss 2.927972\n",
      "Params: tensor([  5.3569, -17.2437])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3330, Loss 2.927973\n",
      "Params: tensor([  5.3570, -17.2438])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3331, Loss 2.927972\n",
      "Params: tensor([  5.3570, -17.2439])\n",
      "Gradient: tensor([-0.0018,  0.0104])\n",
      "Epoch 3332, Loss 2.927970\n",
      "Params: tensor([  5.3570, -17.2440])\n",
      "Gradient: tensor([-0.0018,  0.0103])\n",
      "Epoch 3333, Loss 2.927969\n",
      "Params: tensor([  5.3570, -17.2441])\n",
      "Gradient: tensor([-0.0018,  0.0103])\n",
      "Epoch 3334, Loss 2.927968\n",
      "Params: tensor([  5.3570, -17.2442])\n",
      "Gradient: tensor([-0.0018,  0.0103])\n",
      "Epoch 3335, Loss 2.927966\n",
      "Params: tensor([  5.3570, -17.2444])\n",
      "Gradient: tensor([-0.0018,  0.0103])\n",
      "Epoch 3336, Loss 2.927966\n",
      "Params: tensor([  5.3571, -17.2445])\n",
      "Gradient: tensor([-0.0018,  0.0103])\n",
      "Epoch 3337, Loss 2.927966\n",
      "Params: tensor([  5.3571, -17.2446])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3338, Loss 2.927962\n",
      "Params: tensor([  5.3571, -17.2447])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3339, Loss 2.927962\n",
      "Params: tensor([  5.3571, -17.2448])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3340, Loss 2.927960\n",
      "Params: tensor([  5.3571, -17.2449])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3341, Loss 2.927961\n",
      "Params: tensor([  5.3572, -17.2450])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3342, Loss 2.927959\n",
      "Params: tensor([  5.3572, -17.2451])\n",
      "Gradient: tensor([-0.0018,  0.0102])\n",
      "Epoch 3343, Loss 2.927960\n",
      "Params: tensor([  5.3572, -17.2452])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3344, Loss 2.927958\n",
      "Params: tensor([  5.3572, -17.2453])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3345, Loss 2.927957\n",
      "Params: tensor([  5.3572, -17.2454])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3346, Loss 2.927956\n",
      "Params: tensor([  5.3572, -17.2455])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3347, Loss 2.927954\n",
      "Params: tensor([  5.3573, -17.2456])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3348, Loss 2.927953\n",
      "Params: tensor([  5.3573, -17.2457])\n",
      "Gradient: tensor([-0.0018,  0.0101])\n",
      "Epoch 3349, Loss 2.927952\n",
      "Params: tensor([  5.3573, -17.2458])\n",
      "Gradient: tensor([-0.0018,  0.0100])\n",
      "Epoch 3350, Loss 2.927950\n",
      "Params: tensor([  5.3573, -17.2459])\n",
      "Gradient: tensor([-0.0018,  0.0100])\n",
      "Epoch 3351, Loss 2.927951\n",
      "Params: tensor([  5.3573, -17.2460])\n",
      "Gradient: tensor([-0.0018,  0.0100])\n",
      "Epoch 3352, Loss 2.927949\n",
      "Params: tensor([  5.3573, -17.2461])\n",
      "Gradient: tensor([-0.0018,  0.0100])\n",
      "Epoch 3353, Loss 2.927948\n",
      "Params: tensor([  5.3574, -17.2462])\n",
      "Gradient: tensor([-0.0017,  0.0100])\n",
      "Epoch 3354, Loss 2.927946\n",
      "Params: tensor([  5.3574, -17.2463])\n",
      "Gradient: tensor([-0.0017,  0.0100])\n",
      "Epoch 3355, Loss 2.927947\n",
      "Params: tensor([  5.3574, -17.2464])\n",
      "Gradient: tensor([-0.0017,  0.0099])\n",
      "Epoch 3356, Loss 2.927946\n",
      "Params: tensor([  5.3574, -17.2465])\n",
      "Gradient: tensor([-0.0017,  0.0099])\n",
      "Epoch 3357, Loss 2.927944\n",
      "Params: tensor([  5.3574, -17.2466])\n",
      "Gradient: tensor([-0.0018,  0.0099])\n",
      "Epoch 3358, Loss 2.927943\n",
      "Params: tensor([  5.3575, -17.2467])\n",
      "Gradient: tensor([-0.0018,  0.0099])\n",
      "Epoch 3359, Loss 2.927944\n",
      "Params: tensor([  5.3575, -17.2468])\n",
      "Gradient: tensor([-0.0017,  0.0099])\n",
      "Epoch 3360, Loss 2.927941\n",
      "Params: tensor([  5.3575, -17.2469])\n",
      "Gradient: tensor([-0.0018,  0.0099])\n",
      "Epoch 3361, Loss 2.927940\n",
      "Params: tensor([  5.3575, -17.2470])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3362, Loss 2.927940\n",
      "Params: tensor([  5.3575, -17.2471])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3363, Loss 2.927937\n",
      "Params: tensor([  5.3575, -17.2472])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3364, Loss 2.927937\n",
      "Params: tensor([  5.3576, -17.2473])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3365, Loss 2.927936\n",
      "Params: tensor([  5.3576, -17.2474])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3366, Loss 2.927934\n",
      "Params: tensor([  5.3576, -17.2475])\n",
      "Gradient: tensor([-0.0017,  0.0098])\n",
      "Epoch 3367, Loss 2.927936\n",
      "Params: tensor([  5.3576, -17.2475])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3368, Loss 2.927934\n",
      "Params: tensor([  5.3576, -17.2476])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3369, Loss 2.927932\n",
      "Params: tensor([  5.3576, -17.2477])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3370, Loss 2.927930\n",
      "Params: tensor([  5.3577, -17.2478])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3371, Loss 2.927930\n",
      "Params: tensor([  5.3577, -17.2479])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3372, Loss 2.927928\n",
      "Params: tensor([  5.3577, -17.2480])\n",
      "Gradient: tensor([-0.0017,  0.0097])\n",
      "Epoch 3373, Loss 2.927929\n",
      "Params: tensor([  5.3577, -17.2481])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3374, Loss 2.927928\n",
      "Params: tensor([  5.3577, -17.2482])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3375, Loss 2.927925\n",
      "Params: tensor([  5.3577, -17.2483])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3376, Loss 2.927924\n",
      "Params: tensor([  5.3578, -17.2484])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3377, Loss 2.927925\n",
      "Params: tensor([  5.3578, -17.2485])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3378, Loss 2.927922\n",
      "Params: tensor([  5.3578, -17.2486])\n",
      "Gradient: tensor([-0.0017,  0.0096])\n",
      "Epoch 3379, Loss 2.927923\n",
      "Params: tensor([  5.3578, -17.2487])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3380, Loss 2.927922\n",
      "Params: tensor([  5.3578, -17.2488])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3381, Loss 2.927920\n",
      "Params: tensor([  5.3578, -17.2489])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3382, Loss 2.927918\n",
      "Params: tensor([  5.3579, -17.2490])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3383, Loss 2.927917\n",
      "Params: tensor([  5.3579, -17.2491])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3384, Loss 2.927917\n",
      "Params: tensor([  5.3579, -17.2492])\n",
      "Gradient: tensor([-0.0017,  0.0095])\n",
      "Epoch 3385, Loss 2.927918\n",
      "Params: tensor([  5.3579, -17.2493])\n",
      "Gradient: tensor([-0.0017,  0.0094])\n",
      "Epoch 3386, Loss 2.927917\n",
      "Params: tensor([  5.3579, -17.2494])\n",
      "Gradient: tensor([-0.0017,  0.0094])\n",
      "Epoch 3387, Loss 2.927914\n",
      "Params: tensor([  5.3579, -17.2495])\n",
      "Gradient: tensor([-0.0017,  0.0094])\n",
      "Epoch 3388, Loss 2.927914\n",
      "Params: tensor([  5.3580, -17.2496])\n",
      "Gradient: tensor([-0.0016,  0.0094])\n",
      "Epoch 3389, Loss 2.927913\n",
      "Params: tensor([  5.3580, -17.2496])\n",
      "Gradient: tensor([-0.0016,  0.0094])\n",
      "Epoch 3390, Loss 2.927912\n",
      "Params: tensor([  5.3580, -17.2497])\n",
      "Gradient: tensor([-0.0016,  0.0094])\n",
      "Epoch 3391, Loss 2.927913\n",
      "Params: tensor([  5.3580, -17.2498])\n",
      "Gradient: tensor([-0.0016,  0.0094])\n",
      "Epoch 3392, Loss 2.927911\n",
      "Params: tensor([  5.3580, -17.2499])\n",
      "Gradient: tensor([-0.0016,  0.0093])\n",
      "Epoch 3393, Loss 2.927910\n",
      "Params: tensor([  5.3580, -17.2500])\n",
      "Gradient: tensor([-0.0017,  0.0093])\n",
      "Epoch 3394, Loss 2.927909\n",
      "Params: tensor([  5.3581, -17.2501])\n",
      "Gradient: tensor([-0.0016,  0.0093])\n",
      "Epoch 3395, Loss 2.927909\n",
      "Params: tensor([  5.3581, -17.2502])\n",
      "Gradient: tensor([-0.0016,  0.0093])\n",
      "Epoch 3396, Loss 2.927907\n",
      "Params: tensor([  5.3581, -17.2503])\n",
      "Gradient: tensor([-0.0016,  0.0093])\n",
      "Epoch 3397, Loss 2.927906\n",
      "Params: tensor([  5.3581, -17.2504])\n",
      "Gradient: tensor([-0.0017,  0.0093])\n",
      "Epoch 3398, Loss 2.927905\n",
      "Params: tensor([  5.3581, -17.2505])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3399, Loss 2.927905\n",
      "Params: tensor([  5.3581, -17.2506])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3400, Loss 2.927904\n",
      "Params: tensor([  5.3582, -17.2507])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3401, Loss 2.927902\n",
      "Params: tensor([  5.3582, -17.2508])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3402, Loss 2.927902\n",
      "Params: tensor([  5.3582, -17.2509])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3403, Loss 2.927902\n",
      "Params: tensor([  5.3582, -17.2509])\n",
      "Gradient: tensor([-0.0016,  0.0092])\n",
      "Epoch 3404, Loss 2.927899\n",
      "Params: tensor([  5.3582, -17.2510])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3405, Loss 2.927899\n",
      "Params: tensor([  5.3582, -17.2511])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3406, Loss 2.927898\n",
      "Params: tensor([  5.3583, -17.2512])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3407, Loss 2.927899\n",
      "Params: tensor([  5.3583, -17.2513])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3408, Loss 2.927896\n",
      "Params: tensor([  5.3583, -17.2514])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3409, Loss 2.927895\n",
      "Params: tensor([  5.3583, -17.2515])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3410, Loss 2.927896\n",
      "Params: tensor([  5.3583, -17.2516])\n",
      "Gradient: tensor([-0.0016,  0.0091])\n",
      "Epoch 3411, Loss 2.927893\n",
      "Params: tensor([  5.3583, -17.2517])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3412, Loss 2.927892\n",
      "Params: tensor([  5.3584, -17.2518])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3413, Loss 2.927892\n",
      "Params: tensor([  5.3584, -17.2519])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3414, Loss 2.927891\n",
      "Params: tensor([  5.3584, -17.2519])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3415, Loss 2.927891\n",
      "Params: tensor([  5.3584, -17.2520])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3416, Loss 2.927889\n",
      "Params: tensor([  5.3584, -17.2521])\n",
      "Gradient: tensor([-0.0016,  0.0090])\n",
      "Epoch 3417, Loss 2.927891\n",
      "Params: tensor([  5.3584, -17.2522])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3418, Loss 2.927887\n",
      "Params: tensor([  5.3584, -17.2523])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3419, Loss 2.927886\n",
      "Params: tensor([  5.3585, -17.2524])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3420, Loss 2.927886\n",
      "Params: tensor([  5.3585, -17.2525])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3421, Loss 2.927886\n",
      "Params: tensor([  5.3585, -17.2526])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3422, Loss 2.927885\n",
      "Params: tensor([  5.3585, -17.2527])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3423, Loss 2.927883\n",
      "Params: tensor([  5.3585, -17.2527])\n",
      "Gradient: tensor([-0.0016,  0.0089])\n",
      "Epoch 3424, Loss 2.927883\n",
      "Params: tensor([  5.3585, -17.2528])\n",
      "Gradient: tensor([-0.0016,  0.0088])\n",
      "Epoch 3425, Loss 2.927882\n",
      "Params: tensor([  5.3586, -17.2529])\n",
      "Gradient: tensor([-0.0015,  0.0088])\n",
      "Epoch 3426, Loss 2.927882\n",
      "Params: tensor([  5.3586, -17.2530])\n",
      "Gradient: tensor([-0.0016,  0.0088])\n",
      "Epoch 3427, Loss 2.927880\n",
      "Params: tensor([  5.3586, -17.2531])\n",
      "Gradient: tensor([-0.0015,  0.0088])\n",
      "Epoch 3428, Loss 2.927880\n",
      "Params: tensor([  5.3586, -17.2532])\n",
      "Gradient: tensor([-0.0015,  0.0088])\n",
      "Epoch 3429, Loss 2.927879\n",
      "Params: tensor([  5.3586, -17.2533])\n",
      "Gradient: tensor([-0.0016,  0.0088])\n",
      "Epoch 3430, Loss 2.927879\n",
      "Params: tensor([  5.3586, -17.2534])\n",
      "Gradient: tensor([-0.0015,  0.0088])\n",
      "Epoch 3431, Loss 2.927877\n",
      "Params: tensor([  5.3587, -17.2534])\n",
      "Gradient: tensor([-0.0016,  0.0087])\n",
      "Epoch 3432, Loss 2.927876\n",
      "Params: tensor([  5.3587, -17.2535])\n",
      "Gradient: tensor([-0.0015,  0.0087])\n",
      "Epoch 3433, Loss 2.927876\n",
      "Params: tensor([  5.3587, -17.2536])\n",
      "Gradient: tensor([-0.0016,  0.0087])\n",
      "Epoch 3434, Loss 2.927875\n",
      "Params: tensor([  5.3587, -17.2537])\n",
      "Gradient: tensor([-0.0015,  0.0087])\n",
      "Epoch 3435, Loss 2.927875\n",
      "Params: tensor([  5.3587, -17.2538])\n",
      "Gradient: tensor([-0.0015,  0.0087])\n",
      "Epoch 3436, Loss 2.927875\n",
      "Params: tensor([  5.3587, -17.2539])\n",
      "Gradient: tensor([-0.0015,  0.0087])\n",
      "Epoch 3437, Loss 2.927873\n",
      "Params: tensor([  5.3587, -17.2540])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3438, Loss 2.927872\n",
      "Params: tensor([  5.3588, -17.2541])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3439, Loss 2.927872\n",
      "Params: tensor([  5.3588, -17.2541])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3440, Loss 2.927870\n",
      "Params: tensor([  5.3588, -17.2542])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3441, Loss 2.927870\n",
      "Params: tensor([  5.3588, -17.2543])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3442, Loss 2.927869\n",
      "Params: tensor([  5.3588, -17.2544])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3443, Loss 2.927869\n",
      "Params: tensor([  5.3588, -17.2545])\n",
      "Gradient: tensor([-0.0015,  0.0086])\n",
      "Epoch 3444, Loss 2.927869\n",
      "Params: tensor([  5.3588, -17.2546])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3445, Loss 2.927865\n",
      "Params: tensor([  5.3589, -17.2547])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3446, Loss 2.927866\n",
      "Params: tensor([  5.3589, -17.2547])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3447, Loss 2.927865\n",
      "Params: tensor([  5.3589, -17.2548])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3448, Loss 2.927864\n",
      "Params: tensor([  5.3589, -17.2549])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3449, Loss 2.927863\n",
      "Params: tensor([  5.3589, -17.2550])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3450, Loss 2.927863\n",
      "Params: tensor([  5.3589, -17.2551])\n",
      "Gradient: tensor([-0.0015,  0.0085])\n",
      "Epoch 3451, Loss 2.927862\n",
      "Params: tensor([  5.3590, -17.2552])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3452, Loss 2.927863\n",
      "Params: tensor([  5.3590, -17.2552])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3453, Loss 2.927860\n",
      "Params: tensor([  5.3590, -17.2553])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3454, Loss 2.927860\n",
      "Params: tensor([  5.3590, -17.2554])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3455, Loss 2.927860\n",
      "Params: tensor([  5.3590, -17.2555])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3456, Loss 2.927859\n",
      "Params: tensor([  5.3590, -17.2556])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3457, Loss 2.927858\n",
      "Params: tensor([  5.3590, -17.2557])\n",
      "Gradient: tensor([-0.0015,  0.0084])\n",
      "Epoch 3458, Loss 2.927858\n",
      "Params: tensor([  5.3591, -17.2558])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3459, Loss 2.927855\n",
      "Params: tensor([  5.3591, -17.2558])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3460, Loss 2.927857\n",
      "Params: tensor([  5.3591, -17.2559])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3461, Loss 2.927854\n",
      "Params: tensor([  5.3591, -17.2560])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3462, Loss 2.927855\n",
      "Params: tensor([  5.3591, -17.2561])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3463, Loss 2.927853\n",
      "Params: tensor([  5.3591, -17.2562])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3464, Loss 2.927854\n",
      "Params: tensor([  5.3591, -17.2562])\n",
      "Gradient: tensor([-0.0015,  0.0083])\n",
      "Epoch 3465, Loss 2.927851\n",
      "Params: tensor([  5.3592, -17.2563])\n",
      "Gradient: tensor([-0.0015,  0.0082])\n",
      "Epoch 3466, Loss 2.927852\n",
      "Params: tensor([  5.3592, -17.2564])\n",
      "Gradient: tensor([-0.0014,  0.0082])\n",
      "Epoch 3467, Loss 2.927852\n",
      "Params: tensor([  5.3592, -17.2565])\n",
      "Gradient: tensor([-0.0014,  0.0082])\n",
      "Epoch 3468, Loss 2.927850\n",
      "Params: tensor([  5.3592, -17.2566])\n",
      "Gradient: tensor([-0.0015,  0.0082])\n",
      "Epoch 3469, Loss 2.927849\n",
      "Params: tensor([  5.3592, -17.2567])\n",
      "Gradient: tensor([-0.0015,  0.0082])\n",
      "Epoch 3470, Loss 2.927849\n",
      "Params: tensor([  5.3592, -17.2567])\n",
      "Gradient: tensor([-0.0014,  0.0082])\n",
      "Epoch 3471, Loss 2.927848\n",
      "Params: tensor([  5.3592, -17.2568])\n",
      "Gradient: tensor([-0.0015,  0.0082])\n",
      "Epoch 3472, Loss 2.927848\n",
      "Params: tensor([  5.3593, -17.2569])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3473, Loss 2.927847\n",
      "Params: tensor([  5.3593, -17.2570])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3474, Loss 2.927846\n",
      "Params: tensor([  5.3593, -17.2571])\n",
      "Gradient: tensor([-0.0015,  0.0081])\n",
      "Epoch 3475, Loss 2.927845\n",
      "Params: tensor([  5.3593, -17.2571])\n",
      "Gradient: tensor([-0.0015,  0.0081])\n",
      "Epoch 3476, Loss 2.927844\n",
      "Params: tensor([  5.3593, -17.2572])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3477, Loss 2.927844\n",
      "Params: tensor([  5.3593, -17.2573])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3478, Loss 2.927843\n",
      "Params: tensor([  5.3593, -17.2574])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3479, Loss 2.927842\n",
      "Params: tensor([  5.3594, -17.2575])\n",
      "Gradient: tensor([-0.0014,  0.0081])\n",
      "Epoch 3480, Loss 2.927841\n",
      "Params: tensor([  5.3594, -17.2575])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3481, Loss 2.927842\n",
      "Params: tensor([  5.3594, -17.2576])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3482, Loss 2.927840\n",
      "Params: tensor([  5.3594, -17.2577])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3483, Loss 2.927842\n",
      "Params: tensor([  5.3594, -17.2578])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3484, Loss 2.927839\n",
      "Params: tensor([  5.3594, -17.2579])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3485, Loss 2.927839\n",
      "Params: tensor([  5.3594, -17.2579])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3486, Loss 2.927839\n",
      "Params: tensor([  5.3595, -17.2580])\n",
      "Gradient: tensor([-0.0014,  0.0080])\n",
      "Epoch 3487, Loss 2.927838\n",
      "Params: tensor([  5.3595, -17.2581])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3488, Loss 2.927836\n",
      "Params: tensor([  5.3595, -17.2582])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3489, Loss 2.927836\n",
      "Params: tensor([  5.3595, -17.2583])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3490, Loss 2.927837\n",
      "Params: tensor([  5.3595, -17.2583])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3491, Loss 2.927834\n",
      "Params: tensor([  5.3595, -17.2584])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3492, Loss 2.927834\n",
      "Params: tensor([  5.3595, -17.2585])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3493, Loss 2.927833\n",
      "Params: tensor([  5.3596, -17.2586])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3494, Loss 2.927833\n",
      "Params: tensor([  5.3596, -17.2587])\n",
      "Gradient: tensor([-0.0014,  0.0079])\n",
      "Epoch 3495, Loss 2.927833\n",
      "Params: tensor([  5.3596, -17.2587])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3496, Loss 2.927831\n",
      "Params: tensor([  5.3596, -17.2588])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3497, Loss 2.927831\n",
      "Params: tensor([  5.3596, -17.2589])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3498, Loss 2.927830\n",
      "Params: tensor([  5.3596, -17.2590])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3499, Loss 2.927830\n",
      "Params: tensor([  5.3596, -17.2591])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3500, Loss 2.927831\n",
      "Params: tensor([  5.3597, -17.2591])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3501, Loss 2.927830\n",
      "Params: tensor([  5.3597, -17.2592])\n",
      "Gradient: tensor([-0.0014,  0.0078])\n",
      "Epoch 3502, Loss 2.927828\n",
      "Params: tensor([  5.3597, -17.2593])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3503, Loss 2.927826\n",
      "Params: tensor([  5.3597, -17.2594])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3504, Loss 2.927826\n",
      "Params: tensor([  5.3597, -17.2594])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3505, Loss 2.927825\n",
      "Params: tensor([  5.3597, -17.2595])\n",
      "Gradient: tensor([-0.0013,  0.0077])\n",
      "Epoch 3506, Loss 2.927825\n",
      "Params: tensor([  5.3597, -17.2596])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3507, Loss 2.927826\n",
      "Params: tensor([  5.3597, -17.2597])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3508, Loss 2.927824\n",
      "Params: tensor([  5.3598, -17.2597])\n",
      "Gradient: tensor([-0.0014,  0.0077])\n",
      "Epoch 3509, Loss 2.927824\n",
      "Params: tensor([  5.3598, -17.2598])\n",
      "Gradient: tensor([-0.0014,  0.0076])\n",
      "Epoch 3510, Loss 2.927823\n",
      "Params: tensor([  5.3598, -17.2599])\n",
      "Gradient: tensor([-0.0013,  0.0076])\n",
      "Epoch 3511, Loss 2.927822\n",
      "Params: tensor([  5.3598, -17.2600])\n",
      "Gradient: tensor([-0.0013,  0.0076])\n",
      "Epoch 3512, Loss 2.927822\n",
      "Params: tensor([  5.3598, -17.2600])\n",
      "Gradient: tensor([-0.0014,  0.0076])\n",
      "Epoch 3513, Loss 2.927822\n",
      "Params: tensor([  5.3598, -17.2601])\n",
      "Gradient: tensor([-0.0014,  0.0076])\n",
      "Epoch 3514, Loss 2.927820\n",
      "Params: tensor([  5.3598, -17.2602])\n",
      "Gradient: tensor([-0.0014,  0.0076])\n",
      "Epoch 3515, Loss 2.927819\n",
      "Params: tensor([  5.3599, -17.2603])\n",
      "Gradient: tensor([-0.0014,  0.0076])\n",
      "Epoch 3516, Loss 2.927820\n",
      "Params: tensor([  5.3599, -17.2604])\n",
      "Gradient: tensor([-0.0013,  0.0076])\n",
      "Epoch 3517, Loss 2.927818\n",
      "Params: tensor([  5.3599, -17.2604])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3518, Loss 2.927819\n",
      "Params: tensor([  5.3599, -17.2605])\n",
      "Gradient: tensor([-0.0014,  0.0075])\n",
      "Epoch 3519, Loss 2.927818\n",
      "Params: tensor([  5.3599, -17.2606])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3520, Loss 2.927817\n",
      "Params: tensor([  5.3599, -17.2607])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3521, Loss 2.927817\n",
      "Params: tensor([  5.3599, -17.2607])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3522, Loss 2.927816\n",
      "Params: tensor([  5.3599, -17.2608])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3523, Loss 2.927817\n",
      "Params: tensor([  5.3600, -17.2609])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3524, Loss 2.927816\n",
      "Params: tensor([  5.3600, -17.2610])\n",
      "Gradient: tensor([-0.0013,  0.0075])\n",
      "Epoch 3525, Loss 2.927814\n",
      "Params: tensor([  5.3600, -17.2610])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3526, Loss 2.927813\n",
      "Params: tensor([  5.3600, -17.2611])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3527, Loss 2.927814\n",
      "Params: tensor([  5.3600, -17.2612])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3528, Loss 2.927812\n",
      "Params: tensor([  5.3600, -17.2612])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3529, Loss 2.927812\n",
      "Params: tensor([  5.3600, -17.2613])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3530, Loss 2.927811\n",
      "Params: tensor([  5.3601, -17.2614])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3531, Loss 2.927811\n",
      "Params: tensor([  5.3601, -17.2615])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3532, Loss 2.927809\n",
      "Params: tensor([  5.3601, -17.2615])\n",
      "Gradient: tensor([-0.0013,  0.0074])\n",
      "Epoch 3533, Loss 2.927810\n",
      "Params: tensor([  5.3601, -17.2616])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3534, Loss 2.927810\n",
      "Params: tensor([  5.3601, -17.2617])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3535, Loss 2.927809\n",
      "Params: tensor([  5.3601, -17.2618])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3536, Loss 2.927808\n",
      "Params: tensor([  5.3601, -17.2618])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3537, Loss 2.927807\n",
      "Params: tensor([  5.3601, -17.2619])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3538, Loss 2.927808\n",
      "Params: tensor([  5.3602, -17.2620])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3539, Loss 2.927805\n",
      "Params: tensor([  5.3602, -17.2621])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3540, Loss 2.927805\n",
      "Params: tensor([  5.3602, -17.2621])\n",
      "Gradient: tensor([-0.0013,  0.0073])\n",
      "Epoch 3541, Loss 2.927806\n",
      "Params: tensor([  5.3602, -17.2622])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3542, Loss 2.927806\n",
      "Params: tensor([  5.3602, -17.2623])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3543, Loss 2.927804\n",
      "Params: tensor([  5.3602, -17.2623])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3544, Loss 2.927805\n",
      "Params: tensor([  5.3602, -17.2624])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3545, Loss 2.927803\n",
      "Params: tensor([  5.3602, -17.2625])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3546, Loss 2.927802\n",
      "Params: tensor([  5.3603, -17.2626])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3547, Loss 2.927803\n",
      "Params: tensor([  5.3603, -17.2626])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3548, Loss 2.927800\n",
      "Params: tensor([  5.3603, -17.2627])\n",
      "Gradient: tensor([-0.0013,  0.0072])\n",
      "Epoch 3549, Loss 2.927802\n",
      "Params: tensor([  5.3603, -17.2628])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3550, Loss 2.927800\n",
      "Params: tensor([  5.3603, -17.2628])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3551, Loss 2.927800\n",
      "Params: tensor([  5.3603, -17.2629])\n",
      "Gradient: tensor([-0.0012,  0.0071])\n",
      "Epoch 3552, Loss 2.927799\n",
      "Params: tensor([  5.3603, -17.2630])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3553, Loss 2.927799\n",
      "Params: tensor([  5.3603, -17.2631])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3554, Loss 2.927798\n",
      "Params: tensor([  5.3604, -17.2631])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3555, Loss 2.927798\n",
      "Params: tensor([  5.3604, -17.2632])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3556, Loss 2.927797\n",
      "Params: tensor([  5.3604, -17.2633])\n",
      "Gradient: tensor([-0.0013,  0.0071])\n",
      "Epoch 3557, Loss 2.927799\n",
      "Params: tensor([  5.3604, -17.2633])\n",
      "Gradient: tensor([-0.0012,  0.0071])\n",
      "Epoch 3558, Loss 2.927795\n",
      "Params: tensor([  5.3604, -17.2634])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3559, Loss 2.927796\n",
      "Params: tensor([  5.3604, -17.2635])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3560, Loss 2.927795\n",
      "Params: tensor([  5.3604, -17.2636])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3561, Loss 2.927795\n",
      "Params: tensor([  5.3604, -17.2636])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3562, Loss 2.927794\n",
      "Params: tensor([  5.3605, -17.2637])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3563, Loss 2.927793\n",
      "Params: tensor([  5.3605, -17.2638])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3564, Loss 2.927794\n",
      "Params: tensor([  5.3605, -17.2638])\n",
      "Gradient: tensor([-0.0012,  0.0070])\n",
      "Epoch 3565, Loss 2.927792\n",
      "Params: tensor([  5.3605, -17.2639])\n",
      "Gradient: tensor([-0.0013,  0.0070])\n",
      "Epoch 3566, Loss 2.927793\n",
      "Params: tensor([  5.3605, -17.2640])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3567, Loss 2.927791\n",
      "Params: tensor([  5.3605, -17.2640])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3568, Loss 2.927793\n",
      "Params: tensor([  5.3605, -17.2641])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3569, Loss 2.927791\n",
      "Params: tensor([  5.3605, -17.2642])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3570, Loss 2.927789\n",
      "Params: tensor([  5.3606, -17.2642])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3571, Loss 2.927790\n",
      "Params: tensor([  5.3606, -17.2643])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3572, Loss 2.927790\n",
      "Params: tensor([  5.3606, -17.2644])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3573, Loss 2.927788\n",
      "Params: tensor([  5.3606, -17.2645])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3574, Loss 2.927788\n",
      "Params: tensor([  5.3606, -17.2645])\n",
      "Gradient: tensor([-0.0012,  0.0069])\n",
      "Epoch 3575, Loss 2.927789\n",
      "Params: tensor([  5.3606, -17.2646])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3576, Loss 2.927787\n",
      "Params: tensor([  5.3606, -17.2647])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3577, Loss 2.927787\n",
      "Params: tensor([  5.3606, -17.2647])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3578, Loss 2.927787\n",
      "Params: tensor([  5.3607, -17.2648])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3579, Loss 2.927786\n",
      "Params: tensor([  5.3607, -17.2649])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3580, Loss 2.927786\n",
      "Params: tensor([  5.3607, -17.2649])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3581, Loss 2.927785\n",
      "Params: tensor([  5.3607, -17.2650])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3582, Loss 2.927784\n",
      "Params: tensor([  5.3607, -17.2651])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3583, Loss 2.927785\n",
      "Params: tensor([  5.3607, -17.2651])\n",
      "Gradient: tensor([-0.0012,  0.0068])\n",
      "Epoch 3584, Loss 2.927784\n",
      "Params: tensor([  5.3607, -17.2652])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3585, Loss 2.927784\n",
      "Params: tensor([  5.3607, -17.2653])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3586, Loss 2.927783\n",
      "Params: tensor([  5.3608, -17.2653])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3587, Loss 2.927782\n",
      "Params: tensor([  5.3608, -17.2654])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3588, Loss 2.927782\n",
      "Params: tensor([  5.3608, -17.2655])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3589, Loss 2.927782\n",
      "Params: tensor([  5.3608, -17.2655])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3590, Loss 2.927781\n",
      "Params: tensor([  5.3608, -17.2656])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3591, Loss 2.927781\n",
      "Params: tensor([  5.3608, -17.2657])\n",
      "Gradient: tensor([-0.0012,  0.0067])\n",
      "Epoch 3592, Loss 2.927779\n",
      "Params: tensor([  5.3608, -17.2657])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3593, Loss 2.927779\n",
      "Params: tensor([  5.3608, -17.2658])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3594, Loss 2.927779\n",
      "Params: tensor([  5.3608, -17.2659])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3595, Loss 2.927780\n",
      "Params: tensor([  5.3609, -17.2659])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3596, Loss 2.927779\n",
      "Params: tensor([  5.3609, -17.2660])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3597, Loss 2.927778\n",
      "Params: tensor([  5.3609, -17.2661])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3598, Loss 2.927778\n",
      "Params: tensor([  5.3609, -17.2661])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3599, Loss 2.927776\n",
      "Params: tensor([  5.3609, -17.2662])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3600, Loss 2.927777\n",
      "Params: tensor([  5.3609, -17.2663])\n",
      "Gradient: tensor([-0.0012,  0.0066])\n",
      "Epoch 3601, Loss 2.927775\n",
      "Params: tensor([  5.3609, -17.2663])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3602, Loss 2.927776\n",
      "Params: tensor([  5.3609, -17.2664])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3603, Loss 2.927774\n",
      "Params: tensor([  5.3609, -17.2665])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3604, Loss 2.927774\n",
      "Params: tensor([  5.3610, -17.2665])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3605, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2666])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3606, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2667])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3607, Loss 2.927774\n",
      "Params: tensor([  5.3610, -17.2667])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3608, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2668])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3609, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2668])\n",
      "Gradient: tensor([-0.0012,  0.0065])\n",
      "Epoch 3610, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2669])\n",
      "Gradient: tensor([-0.0012,  0.0064])\n",
      "Epoch 3611, Loss 2.927773\n",
      "Params: tensor([  5.3610, -17.2670])\n",
      "Gradient: tensor([-0.0012,  0.0064])\n",
      "Epoch 3612, Loss 2.927772\n",
      "Params: tensor([  5.3611, -17.2670])\n",
      "Gradient: tensor([-0.0012,  0.0064])\n",
      "Epoch 3613, Loss 2.927770\n",
      "Params: tensor([  5.3611, -17.2671])\n",
      "Gradient: tensor([-0.0011,  0.0064])\n",
      "Epoch 3614, Loss 2.927770\n",
      "Params: tensor([  5.3611, -17.2672])\n",
      "Gradient: tensor([-0.0012,  0.0064])\n",
      "Epoch 3615, Loss 2.927770\n",
      "Params: tensor([  5.3611, -17.2672])\n",
      "Gradient: tensor([-0.0011,  0.0064])\n",
      "Epoch 3616, Loss 2.927771\n",
      "Params: tensor([  5.3611, -17.2673])\n",
      "Gradient: tensor([-0.0011,  0.0064])\n",
      "Epoch 3617, Loss 2.927769\n",
      "Params: tensor([  5.3611, -17.2674])\n",
      "Gradient: tensor([-0.0011,  0.0064])\n",
      "Epoch 3618, Loss 2.927769\n",
      "Params: tensor([  5.3611, -17.2674])\n",
      "Gradient: tensor([-0.0011,  0.0064])\n",
      "Epoch 3619, Loss 2.927769\n",
      "Params: tensor([  5.3611, -17.2675])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3620, Loss 2.927768\n",
      "Params: tensor([  5.3611, -17.2676])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3621, Loss 2.927767\n",
      "Params: tensor([  5.3612, -17.2676])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3622, Loss 2.927766\n",
      "Params: tensor([  5.3612, -17.2677])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3623, Loss 2.927767\n",
      "Params: tensor([  5.3612, -17.2677])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3624, Loss 2.927767\n",
      "Params: tensor([  5.3612, -17.2678])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3625, Loss 2.927766\n",
      "Params: tensor([  5.3612, -17.2679])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3626, Loss 2.927767\n",
      "Params: tensor([  5.3612, -17.2679])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3627, Loss 2.927766\n",
      "Params: tensor([  5.3612, -17.2680])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3628, Loss 2.927765\n",
      "Params: tensor([  5.3612, -17.2681])\n",
      "Gradient: tensor([-0.0011,  0.0063])\n",
      "Epoch 3629, Loss 2.927763\n",
      "Params: tensor([  5.3612, -17.2681])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3630, Loss 2.927765\n",
      "Params: tensor([  5.3613, -17.2682])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3631, Loss 2.927762\n",
      "Params: tensor([  5.3613, -17.2682])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3632, Loss 2.927763\n",
      "Params: tensor([  5.3613, -17.2683])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3633, Loss 2.927763\n",
      "Params: tensor([  5.3613, -17.2684])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3634, Loss 2.927762\n",
      "Params: tensor([  5.3613, -17.2684])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3635, Loss 2.927761\n",
      "Params: tensor([  5.3613, -17.2685])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3636, Loss 2.927763\n",
      "Params: tensor([  5.3613, -17.2686])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3637, Loss 2.927759\n",
      "Params: tensor([  5.3613, -17.2686])\n",
      "Gradient: tensor([-0.0011,  0.0062])\n",
      "Epoch 3638, Loss 2.927762\n",
      "Params: tensor([  5.3613, -17.2687])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3639, Loss 2.927761\n",
      "Params: tensor([  5.3614, -17.2687])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3640, Loss 2.927761\n",
      "Params: tensor([  5.3614, -17.2688])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3641, Loss 2.927759\n",
      "Params: tensor([  5.3614, -17.2689])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3642, Loss 2.927760\n",
      "Params: tensor([  5.3614, -17.2689])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3643, Loss 2.927760\n",
      "Params: tensor([  5.3614, -17.2690])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3644, Loss 2.927758\n",
      "Params: tensor([  5.3614, -17.2690])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3645, Loss 2.927758\n",
      "Params: tensor([  5.3614, -17.2691])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3646, Loss 2.927759\n",
      "Params: tensor([  5.3614, -17.2692])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3647, Loss 2.927758\n",
      "Params: tensor([  5.3614, -17.2692])\n",
      "Gradient: tensor([-0.0011,  0.0061])\n",
      "Epoch 3648, Loss 2.927757\n",
      "Params: tensor([  5.3614, -17.2693])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3649, Loss 2.927757\n",
      "Params: tensor([  5.3615, -17.2693])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3650, Loss 2.927758\n",
      "Params: tensor([  5.3615, -17.2694])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3651, Loss 2.927756\n",
      "Params: tensor([  5.3615, -17.2695])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3652, Loss 2.927754\n",
      "Params: tensor([  5.3615, -17.2695])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3653, Loss 2.927754\n",
      "Params: tensor([  5.3615, -17.2696])\n",
      "Gradient: tensor([-0.0011,  0.0060])\n",
      "Epoch 3654, Loss 2.927756\n",
      "Params: tensor([  5.3615, -17.2696])\n",
      "Gradient: tensor([-0.0010,  0.0060])\n",
      "Epoch 3655, Loss 2.927754\n",
      "Params: tensor([  5.3615, -17.2697])\n",
      "Gradient: tensor([-0.0010,  0.0060])\n",
      "Epoch 3656, Loss 2.927754\n",
      "Params: tensor([  5.3615, -17.2698])\n",
      "Gradient: tensor([-0.0010,  0.0060])\n",
      "Epoch 3657, Loss 2.927752\n",
      "Params: tensor([  5.3615, -17.2698])\n",
      "Gradient: tensor([-0.0010,  0.0060])\n",
      "Epoch 3658, Loss 2.927752\n",
      "Params: tensor([  5.3616, -17.2699])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3659, Loss 2.927753\n",
      "Params: tensor([  5.3616, -17.2699])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3660, Loss 2.927752\n",
      "Params: tensor([  5.3616, -17.2700])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3661, Loss 2.927752\n",
      "Params: tensor([  5.3616, -17.2701])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3662, Loss 2.927752\n",
      "Params: tensor([  5.3616, -17.2701])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3663, Loss 2.927751\n",
      "Params: tensor([  5.3616, -17.2702])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3664, Loss 2.927751\n",
      "Params: tensor([  5.3616, -17.2702])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3665, Loss 2.927749\n",
      "Params: tensor([  5.3616, -17.2703])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3666, Loss 2.927751\n",
      "Params: tensor([  5.3616, -17.2704])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3667, Loss 2.927750\n",
      "Params: tensor([  5.3616, -17.2704])\n",
      "Gradient: tensor([-0.0010,  0.0059])\n",
      "Epoch 3668, Loss 2.927750\n",
      "Params: tensor([  5.3617, -17.2705])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3669, Loss 2.927749\n",
      "Params: tensor([  5.3617, -17.2705])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3670, Loss 2.927749\n",
      "Params: tensor([  5.3617, -17.2706])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3671, Loss 2.927748\n",
      "Params: tensor([  5.3617, -17.2706])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3672, Loss 2.927749\n",
      "Params: tensor([  5.3617, -17.2707])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3673, Loss 2.927748\n",
      "Params: tensor([  5.3617, -17.2708])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3674, Loss 2.927748\n",
      "Params: tensor([  5.3617, -17.2708])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3675, Loss 2.927747\n",
      "Params: tensor([  5.3617, -17.2709])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3676, Loss 2.927747\n",
      "Params: tensor([  5.3617, -17.2709])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3677, Loss 2.927746\n",
      "Params: tensor([  5.3617, -17.2710])\n",
      "Gradient: tensor([-0.0010,  0.0058])\n",
      "Epoch 3678, Loss 2.927746\n",
      "Params: tensor([  5.3618, -17.2710])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3679, Loss 2.927745\n",
      "Params: tensor([  5.3618, -17.2711])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3680, Loss 2.927747\n",
      "Params: tensor([  5.3618, -17.2712])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3681, Loss 2.927743\n",
      "Params: tensor([  5.3618, -17.2712])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3682, Loss 2.927744\n",
      "Params: tensor([  5.3618, -17.2713])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3683, Loss 2.927743\n",
      "Params: tensor([  5.3618, -17.2713])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3684, Loss 2.927744\n",
      "Params: tensor([  5.3618, -17.2714])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3685, Loss 2.927744\n",
      "Params: tensor([  5.3618, -17.2714])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3686, Loss 2.927743\n",
      "Params: tensor([  5.3618, -17.2715])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3687, Loss 2.927743\n",
      "Params: tensor([  5.3618, -17.2716])\n",
      "Gradient: tensor([-0.0010,  0.0057])\n",
      "Epoch 3688, Loss 2.927743\n",
      "Params: tensor([  5.3619, -17.2716])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3689, Loss 2.927743\n",
      "Params: tensor([  5.3619, -17.2717])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3690, Loss 2.927743\n",
      "Params: tensor([  5.3619, -17.2717])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3691, Loss 2.927742\n",
      "Params: tensor([  5.3619, -17.2718])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3692, Loss 2.927742\n",
      "Params: tensor([  5.3619, -17.2718])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3693, Loss 2.927739\n",
      "Params: tensor([  5.3619, -17.2719])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3694, Loss 2.927741\n",
      "Params: tensor([  5.3619, -17.2720])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3695, Loss 2.927740\n",
      "Params: tensor([  5.3619, -17.2720])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3696, Loss 2.927742\n",
      "Params: tensor([  5.3619, -17.2721])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3697, Loss 2.927740\n",
      "Params: tensor([  5.3619, -17.2721])\n",
      "Gradient: tensor([-0.0010,  0.0056])\n",
      "Epoch 3698, Loss 2.927740\n",
      "Params: tensor([  5.3620, -17.2722])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3699, Loss 2.927738\n",
      "Params: tensor([  5.3620, -17.2722])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3700, Loss 2.927738\n",
      "Params: tensor([  5.3620, -17.2723])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3701, Loss 2.927740\n",
      "Params: tensor([  5.3620, -17.2723])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3702, Loss 2.927737\n",
      "Params: tensor([  5.3620, -17.2724])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3703, Loss 2.927737\n",
      "Params: tensor([  5.3620, -17.2725])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3704, Loss 2.927738\n",
      "Params: tensor([  5.3620, -17.2725])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3705, Loss 2.927737\n",
      "Params: tensor([  5.3620, -17.2726])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3706, Loss 2.927737\n",
      "Params: tensor([  5.3620, -17.2726])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3707, Loss 2.927735\n",
      "Params: tensor([  5.3620, -17.2727])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3708, Loss 2.927735\n",
      "Params: tensor([  5.3621, -17.2727])\n",
      "Gradient: tensor([-0.0010,  0.0055])\n",
      "Epoch 3709, Loss 2.927735\n",
      "Params: tensor([  5.3621, -17.2728])\n",
      "Gradient: tensor([-0.0010,  0.0054])\n",
      "Epoch 3710, Loss 2.927734\n",
      "Params: tensor([  5.3621, -17.2728])\n",
      "Gradient: tensor([-0.0010,  0.0054])\n",
      "Epoch 3711, Loss 2.927734\n",
      "Params: tensor([  5.3621, -17.2729])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3712, Loss 2.927734\n",
      "Params: tensor([  5.3621, -17.2729])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3713, Loss 2.927736\n",
      "Params: tensor([  5.3621, -17.2730])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3714, Loss 2.927734\n",
      "Params: tensor([  5.3621, -17.2731])\n",
      "Gradient: tensor([-0.0010,  0.0054])\n",
      "Epoch 3715, Loss 2.927735\n",
      "Params: tensor([  5.3621, -17.2731])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3716, Loss 2.927734\n",
      "Params: tensor([  5.3621, -17.2732])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3717, Loss 2.927735\n",
      "Params: tensor([  5.3621, -17.2732])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3718, Loss 2.927734\n",
      "Params: tensor([  5.3622, -17.2733])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3719, Loss 2.927735\n",
      "Params: tensor([  5.3622, -17.2733])\n",
      "Gradient: tensor([-0.0009,  0.0054])\n",
      "Epoch 3720, Loss 2.927733\n",
      "Params: tensor([  5.3622, -17.2734])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3721, Loss 2.927733\n",
      "Params: tensor([  5.3622, -17.2734])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3722, Loss 2.927732\n",
      "Params: tensor([  5.3622, -17.2735])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3723, Loss 2.927732\n",
      "Params: tensor([  5.3622, -17.2735])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3724, Loss 2.927732\n",
      "Params: tensor([  5.3622, -17.2736])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3725, Loss 2.927731\n",
      "Params: tensor([  5.3622, -17.2736])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3726, Loss 2.927731\n",
      "Params: tensor([  5.3622, -17.2737])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3727, Loss 2.927731\n",
      "Params: tensor([  5.3622, -17.2737])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3728, Loss 2.927730\n",
      "Params: tensor([  5.3622, -17.2738])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3729, Loss 2.927730\n",
      "Params: tensor([  5.3623, -17.2739])\n",
      "Gradient: tensor([-0.0010,  0.0053])\n",
      "Epoch 3730, Loss 2.927729\n",
      "Params: tensor([  5.3623, -17.2739])\n",
      "Gradient: tensor([-0.0009,  0.0053])\n",
      "Epoch 3731, Loss 2.927731\n",
      "Params: tensor([  5.3623, -17.2740])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3732, Loss 2.927731\n",
      "Params: tensor([  5.3623, -17.2740])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3733, Loss 2.927729\n",
      "Params: tensor([  5.3623, -17.2741])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3734, Loss 2.927729\n",
      "Params: tensor([  5.3623, -17.2741])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3735, Loss 2.927729\n",
      "Params: tensor([  5.3623, -17.2742])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3736, Loss 2.927728\n",
      "Params: tensor([  5.3623, -17.2742])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3737, Loss 2.927727\n",
      "Params: tensor([  5.3623, -17.2743])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3738, Loss 2.927728\n",
      "Params: tensor([  5.3623, -17.2743])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3739, Loss 2.927728\n",
      "Params: tensor([  5.3623, -17.2744])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3740, Loss 2.927727\n",
      "Params: tensor([  5.3624, -17.2744])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3741, Loss 2.927728\n",
      "Params: tensor([  5.3624, -17.2745])\n",
      "Gradient: tensor([-0.0009,  0.0052])\n",
      "Epoch 3742, Loss 2.927727\n",
      "Params: tensor([  5.3624, -17.2745])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3743, Loss 2.927727\n",
      "Params: tensor([  5.3624, -17.2746])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3744, Loss 2.927727\n",
      "Params: tensor([  5.3624, -17.2746])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3745, Loss 2.927725\n",
      "Params: tensor([  5.3624, -17.2747])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3746, Loss 2.927724\n",
      "Params: tensor([  5.3624, -17.2747])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3747, Loss 2.927726\n",
      "Params: tensor([  5.3624, -17.2748])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3748, Loss 2.927725\n",
      "Params: tensor([  5.3624, -17.2748])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3749, Loss 2.927723\n",
      "Params: tensor([  5.3624, -17.2749])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3750, Loss 2.927725\n",
      "Params: tensor([  5.3624, -17.2749])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3751, Loss 2.927724\n",
      "Params: tensor([  5.3625, -17.2750])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3752, Loss 2.927725\n",
      "Params: tensor([  5.3625, -17.2750])\n",
      "Gradient: tensor([-0.0009,  0.0051])\n",
      "Epoch 3753, Loss 2.927724\n",
      "Params: tensor([  5.3625, -17.2751])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3754, Loss 2.927724\n",
      "Params: tensor([  5.3625, -17.2751])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3755, Loss 2.927723\n",
      "Params: tensor([  5.3625, -17.2752])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3756, Loss 2.927723\n",
      "Params: tensor([  5.3625, -17.2752])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3757, Loss 2.927721\n",
      "Params: tensor([  5.3625, -17.2753])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3758, Loss 2.927723\n",
      "Params: tensor([  5.3625, -17.2753])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3759, Loss 2.927723\n",
      "Params: tensor([  5.3625, -17.2754])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3760, Loss 2.927721\n",
      "Params: tensor([  5.3625, -17.2754])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3761, Loss 2.927722\n",
      "Params: tensor([  5.3625, -17.2755])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3762, Loss 2.927721\n",
      "Params: tensor([  5.3626, -17.2755])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3763, Loss 2.927721\n",
      "Params: tensor([  5.3626, -17.2756])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3764, Loss 2.927720\n",
      "Params: tensor([  5.3626, -17.2756])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3765, Loss 2.927720\n",
      "Params: tensor([  5.3626, -17.2757])\n",
      "Gradient: tensor([-0.0009,  0.0050])\n",
      "Epoch 3766, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2757])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3767, Loss 2.927720\n",
      "Params: tensor([  5.3626, -17.2758])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3768, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2758])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3769, Loss 2.927720\n",
      "Params: tensor([  5.3626, -17.2759])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3770, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2759])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3771, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2760])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3772, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2760])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3773, Loss 2.927719\n",
      "Params: tensor([  5.3626, -17.2761])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3774, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2761])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3775, Loss 2.927718\n",
      "Params: tensor([  5.3627, -17.2762])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3776, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2762])\n",
      "Gradient: tensor([-0.0009,  0.0049])\n",
      "Epoch 3777, Loss 2.927718\n",
      "Params: tensor([  5.3627, -17.2763])\n",
      "Gradient: tensor([-0.0008,  0.0049])\n",
      "Epoch 3778, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2763])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3779, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2764])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3780, Loss 2.927716\n",
      "Params: tensor([  5.3627, -17.2764])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3781, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2765])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3782, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2765])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3783, Loss 2.927717\n",
      "Params: tensor([  5.3627, -17.2766])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3784, Loss 2.927716\n",
      "Params: tensor([  5.3627, -17.2766])\n",
      "Gradient: tensor([-0.0009,  0.0048])\n",
      "Epoch 3785, Loss 2.927715\n",
      "Params: tensor([  5.3627, -17.2767])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3786, Loss 2.927715\n",
      "Params: tensor([  5.3628, -17.2767])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3787, Loss 2.927715\n",
      "Params: tensor([  5.3628, -17.2767])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3788, Loss 2.927715\n",
      "Params: tensor([  5.3628, -17.2768])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3789, Loss 2.927715\n",
      "Params: tensor([  5.3628, -17.2768])\n",
      "Gradient: tensor([-0.0008,  0.0048])\n",
      "Epoch 3790, Loss 2.927715\n",
      "Params: tensor([  5.3628, -17.2769])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3791, Loss 2.927714\n",
      "Params: tensor([  5.3628, -17.2769])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3792, Loss 2.927714\n",
      "Params: tensor([  5.3628, -17.2770])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3793, Loss 2.927714\n",
      "Params: tensor([  5.3628, -17.2770])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3794, Loss 2.927713\n",
      "Params: tensor([  5.3628, -17.2771])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3795, Loss 2.927713\n",
      "Params: tensor([  5.3628, -17.2771])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3796, Loss 2.927714\n",
      "Params: tensor([  5.3628, -17.2772])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3797, Loss 2.927713\n",
      "Params: tensor([  5.3629, -17.2772])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3798, Loss 2.927712\n",
      "Params: tensor([  5.3629, -17.2773])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3799, Loss 2.927712\n",
      "Params: tensor([  5.3629, -17.2773])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3800, Loss 2.927713\n",
      "Params: tensor([  5.3629, -17.2774])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3801, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2774])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3802, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2775])\n",
      "Gradient: tensor([-0.0008,  0.0047])\n",
      "Epoch 3803, Loss 2.927713\n",
      "Params: tensor([  5.3629, -17.2775])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3804, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2775])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3805, Loss 2.927712\n",
      "Params: tensor([  5.3629, -17.2776])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3806, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2776])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3807, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2777])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3808, Loss 2.927711\n",
      "Params: tensor([  5.3629, -17.2777])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3809, Loss 2.927709\n",
      "Params: tensor([  5.3629, -17.2778])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3810, Loss 2.927711\n",
      "Params: tensor([  5.3630, -17.2778])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3811, Loss 2.927710\n",
      "Params: tensor([  5.3630, -17.2779])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3812, Loss 2.927708\n",
      "Params: tensor([  5.3630, -17.2779])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3813, Loss 2.927708\n",
      "Params: tensor([  5.3630, -17.2780])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3814, Loss 2.927709\n",
      "Params: tensor([  5.3630, -17.2780])\n",
      "Gradient: tensor([-0.0008,  0.0046])\n",
      "Epoch 3815, Loss 2.927709\n",
      "Params: tensor([  5.3630, -17.2781])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3816, Loss 2.927710\n",
      "Params: tensor([  5.3630, -17.2781])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3817, Loss 2.927708\n",
      "Params: tensor([  5.3630, -17.2781])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3818, Loss 2.927708\n",
      "Params: tensor([  5.3630, -17.2782])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3819, Loss 2.927707\n",
      "Params: tensor([  5.3630, -17.2782])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3820, Loss 2.927707\n",
      "Params: tensor([  5.3630, -17.2783])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3821, Loss 2.927708\n",
      "Params: tensor([  5.3630, -17.2783])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3822, Loss 2.927707\n",
      "Params: tensor([  5.3631, -17.2784])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3823, Loss 2.927707\n",
      "Params: tensor([  5.3631, -17.2784])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3824, Loss 2.927707\n",
      "Params: tensor([  5.3631, -17.2785])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3825, Loss 2.927708\n",
      "Params: tensor([  5.3631, -17.2785])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3826, Loss 2.927707\n",
      "Params: tensor([  5.3631, -17.2786])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3827, Loss 2.927706\n",
      "Params: tensor([  5.3631, -17.2786])\n",
      "Gradient: tensor([-0.0008,  0.0045])\n",
      "Epoch 3828, Loss 2.927707\n",
      "Params: tensor([  5.3631, -17.2786])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3829, Loss 2.927706\n",
      "Params: tensor([  5.3631, -17.2787])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3830, Loss 2.927706\n",
      "Params: tensor([  5.3631, -17.2787])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3831, Loss 2.927706\n",
      "Params: tensor([  5.3631, -17.2788])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3832, Loss 2.927706\n",
      "Params: tensor([  5.3631, -17.2788])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3833, Loss 2.927705\n",
      "Params: tensor([  5.3631, -17.2789])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3834, Loss 2.927705\n",
      "Params: tensor([  5.3631, -17.2789])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3835, Loss 2.927705\n",
      "Params: tensor([  5.3632, -17.2789])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3836, Loss 2.927705\n",
      "Params: tensor([  5.3632, -17.2790])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3837, Loss 2.927705\n",
      "Params: tensor([  5.3632, -17.2790])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3838, Loss 2.927704\n",
      "Params: tensor([  5.3632, -17.2791])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3839, Loss 2.927704\n",
      "Params: tensor([  5.3632, -17.2791])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3840, Loss 2.927704\n",
      "Params: tensor([  5.3632, -17.2792])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3841, Loss 2.927704\n",
      "Params: tensor([  5.3632, -17.2792])\n",
      "Gradient: tensor([-0.0008,  0.0044])\n",
      "Epoch 3842, Loss 2.927702\n",
      "Params: tensor([  5.3632, -17.2793])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3843, Loss 2.927703\n",
      "Params: tensor([  5.3632, -17.2793])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3844, Loss 2.927703\n",
      "Params: tensor([  5.3632, -17.2793])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3845, Loss 2.927704\n",
      "Params: tensor([  5.3632, -17.2794])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3846, Loss 2.927702\n",
      "Params: tensor([  5.3632, -17.2794])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3847, Loss 2.927701\n",
      "Params: tensor([  5.3632, -17.2795])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3848, Loss 2.927703\n",
      "Params: tensor([  5.3633, -17.2795])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3849, Loss 2.927702\n",
      "Params: tensor([  5.3633, -17.2796])\n",
      "Gradient: tensor([-0.0008,  0.0043])\n",
      "Epoch 3850, Loss 2.927701\n",
      "Params: tensor([  5.3633, -17.2796])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3851, Loss 2.927701\n",
      "Params: tensor([  5.3633, -17.2796])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3852, Loss 2.927703\n",
      "Params: tensor([  5.3633, -17.2797])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3853, Loss 2.927700\n",
      "Params: tensor([  5.3633, -17.2797])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3854, Loss 2.927701\n",
      "Params: tensor([  5.3633, -17.2798])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3855, Loss 2.927701\n",
      "Params: tensor([  5.3633, -17.2798])\n",
      "Gradient: tensor([-0.0007,  0.0043])\n",
      "Epoch 3856, Loss 2.927700\n",
      "Params: tensor([  5.3633, -17.2799])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3857, Loss 2.927700\n",
      "Params: tensor([  5.3633, -17.2799])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3858, Loss 2.927700\n",
      "Params: tensor([  5.3633, -17.2799])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3859, Loss 2.927701\n",
      "Params: tensor([  5.3633, -17.2800])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3860, Loss 2.927699\n",
      "Params: tensor([  5.3633, -17.2800])\n",
      "Gradient: tensor([-0.0008,  0.0042])\n",
      "Epoch 3861, Loss 2.927699\n",
      "Params: tensor([  5.3634, -17.2801])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3862, Loss 2.927700\n",
      "Params: tensor([  5.3634, -17.2801])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3863, Loss 2.927699\n",
      "Params: tensor([  5.3634, -17.2801])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3864, Loss 2.927698\n",
      "Params: tensor([  5.3634, -17.2802])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3865, Loss 2.927700\n",
      "Params: tensor([  5.3634, -17.2802])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3866, Loss 2.927697\n",
      "Params: tensor([  5.3634, -17.2803])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3867, Loss 2.927700\n",
      "Params: tensor([  5.3634, -17.2803])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3868, Loss 2.927700\n",
      "Params: tensor([  5.3634, -17.2804])\n",
      "Gradient: tensor([-0.0008,  0.0042])\n",
      "Epoch 3869, Loss 2.927698\n",
      "Params: tensor([  5.3634, -17.2804])\n",
      "Gradient: tensor([-0.0007,  0.0042])\n",
      "Epoch 3870, Loss 2.927697\n",
      "Params: tensor([  5.3634, -17.2804])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3871, Loss 2.927698\n",
      "Params: tensor([  5.3634, -17.2805])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3872, Loss 2.927696\n",
      "Params: tensor([  5.3634, -17.2805])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3873, Loss 2.927699\n",
      "Params: tensor([  5.3634, -17.2806])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3874, Loss 2.927697\n",
      "Params: tensor([  5.3634, -17.2806])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3875, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2806])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3876, Loss 2.927698\n",
      "Params: tensor([  5.3635, -17.2807])\n",
      "Gradient: tensor([-0.0008,  0.0041])\n",
      "Epoch 3877, Loss 2.927697\n",
      "Params: tensor([  5.3635, -17.2807])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3878, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2808])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3879, Loss 2.927698\n",
      "Params: tensor([  5.3635, -17.2808])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3880, Loss 2.927697\n",
      "Params: tensor([  5.3635, -17.2808])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3881, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2809])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3882, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2809])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3883, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2810])\n",
      "Gradient: tensor([-0.0007,  0.0041])\n",
      "Epoch 3884, Loss 2.927695\n",
      "Params: tensor([  5.3635, -17.2810])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3885, Loss 2.927695\n",
      "Params: tensor([  5.3635, -17.2810])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3886, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2811])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3887, Loss 2.927696\n",
      "Params: tensor([  5.3635, -17.2811])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3888, Loss 2.927695\n",
      "Params: tensor([  5.3635, -17.2812])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3889, Loss 2.927695\n",
      "Params: tensor([  5.3636, -17.2812])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3890, Loss 2.927694\n",
      "Params: tensor([  5.3636, -17.2812])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3891, Loss 2.927694\n",
      "Params: tensor([  5.3636, -17.2813])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3892, Loss 2.927693\n",
      "Params: tensor([  5.3636, -17.2813])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3893, Loss 2.927695\n",
      "Params: tensor([  5.3636, -17.2814])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3894, Loss 2.927695\n",
      "Params: tensor([  5.3636, -17.2814])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3895, Loss 2.927694\n",
      "Params: tensor([  5.3636, -17.2815])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3896, Loss 2.927696\n",
      "Params: tensor([  5.3636, -17.2815])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3897, Loss 2.927693\n",
      "Params: tensor([  5.3636, -17.2815])\n",
      "Gradient: tensor([-0.0007,  0.0040])\n",
      "Epoch 3898, Loss 2.927693\n",
      "Params: tensor([  5.3636, -17.2816])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3899, Loss 2.927694\n",
      "Params: tensor([  5.3636, -17.2816])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3900, Loss 2.927693\n",
      "Params: tensor([  5.3636, -17.2817])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3901, Loss 2.927692\n",
      "Params: tensor([  5.3636, -17.2817])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3902, Loss 2.927694\n",
      "Params: tensor([  5.3636, -17.2817])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3903, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2818])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3904, Loss 2.927693\n",
      "Params: tensor([  5.3637, -17.2818])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3905, Loss 2.927691\n",
      "Params: tensor([  5.3637, -17.2818])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3906, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2819])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3907, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2819])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3908, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2820])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3909, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2820])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3910, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2820])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3911, Loss 2.927690\n",
      "Params: tensor([  5.3637, -17.2821])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3912, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2821])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3913, Loss 2.927691\n",
      "Params: tensor([  5.3637, -17.2822])\n",
      "Gradient: tensor([-0.0007,  0.0039])\n",
      "Epoch 3914, Loss 2.927692\n",
      "Params: tensor([  5.3637, -17.2822])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3915, Loss 2.927690\n",
      "Params: tensor([  5.3637, -17.2822])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3916, Loss 2.927691\n",
      "Params: tensor([  5.3637, -17.2823])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3917, Loss 2.927691\n",
      "Params: tensor([  5.3637, -17.2823])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3918, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2823])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3919, Loss 2.927690\n",
      "Params: tensor([  5.3638, -17.2824])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3920, Loss 2.927690\n",
      "Params: tensor([  5.3638, -17.2824])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3921, Loss 2.927690\n",
      "Params: tensor([  5.3638, -17.2825])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3922, Loss 2.927690\n",
      "Params: tensor([  5.3638, -17.2825])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3923, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2825])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3924, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2826])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3925, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2826])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3926, Loss 2.927688\n",
      "Params: tensor([  5.3638, -17.2826])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3927, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2827])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3928, Loss 2.927690\n",
      "Params: tensor([  5.3638, -17.2827])\n",
      "Gradient: tensor([-0.0007,  0.0038])\n",
      "Epoch 3929, Loss 2.927689\n",
      "Params: tensor([  5.3638, -17.2828])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3930, Loss 2.927688\n",
      "Params: tensor([  5.3638, -17.2828])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3931, Loss 2.927688\n",
      "Params: tensor([  5.3638, -17.2828])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3932, Loss 2.927688\n",
      "Params: tensor([  5.3638, -17.2829])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3933, Loss 2.927687\n",
      "Params: tensor([  5.3639, -17.2829])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3934, Loss 2.927689\n",
      "Params: tensor([  5.3639, -17.2829])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3935, Loss 2.927688\n",
      "Params: tensor([  5.3639, -17.2830])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3936, Loss 2.927687\n",
      "Params: tensor([  5.3639, -17.2830])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3937, Loss 2.927687\n",
      "Params: tensor([  5.3639, -17.2831])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3938, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2831])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3939, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2831])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3940, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2832])\n",
      "Gradient: tensor([-0.0007,  0.0037])\n",
      "Epoch 3941, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2832])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3942, Loss 2.927687\n",
      "Params: tensor([  5.3639, -17.2832])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3943, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2833])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3944, Loss 2.927687\n",
      "Params: tensor([  5.3639, -17.2833])\n",
      "Gradient: tensor([-0.0006,  0.0037])\n",
      "Epoch 3945, Loss 2.927686\n",
      "Params: tensor([  5.3639, -17.2833])\n",
      "Gradient: tensor([-0.0007,  0.0036])\n",
      "Epoch 3946, Loss 2.927685\n",
      "Params: tensor([  5.3639, -17.2834])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3947, Loss 2.927685\n",
      "Params: tensor([  5.3639, -17.2834])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3948, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2835])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3949, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2835])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3950, Loss 2.927686\n",
      "Params: tensor([  5.3640, -17.2835])\n",
      "Gradient: tensor([-0.0007,  0.0036])\n",
      "Epoch 3951, Loss 2.927686\n",
      "Params: tensor([  5.3640, -17.2836])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3952, Loss 2.927686\n",
      "Params: tensor([  5.3640, -17.2836])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3953, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2836])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3954, Loss 2.927686\n",
      "Params: tensor([  5.3640, -17.2837])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3955, Loss 2.927686\n",
      "Params: tensor([  5.3640, -17.2837])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3956, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2837])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3957, Loss 2.927683\n",
      "Params: tensor([  5.3640, -17.2838])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3958, Loss 2.927684\n",
      "Params: tensor([  5.3640, -17.2838])\n",
      "Gradient: tensor([-0.0007,  0.0036])\n",
      "Epoch 3959, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2839])\n",
      "Gradient: tensor([-0.0006,  0.0036])\n",
      "Epoch 3960, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2839])\n",
      "Gradient: tensor([-0.0007,  0.0036])\n",
      "Epoch 3961, Loss 2.927684\n",
      "Params: tensor([  5.3640, -17.2839])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3962, Loss 2.927684\n",
      "Params: tensor([  5.3640, -17.2840])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3963, Loss 2.927685\n",
      "Params: tensor([  5.3640, -17.2840])\n",
      "Gradient: tensor([-0.0007,  0.0035])\n",
      "Epoch 3964, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2840])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3965, Loss 2.927685\n",
      "Params: tensor([  5.3641, -17.2841])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3966, Loss 2.927685\n",
      "Params: tensor([  5.3641, -17.2841])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3967, Loss 2.927684\n",
      "Params: tensor([  5.3641, -17.2841])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3968, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2842])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3969, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2842])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3970, Loss 2.927682\n",
      "Params: tensor([  5.3641, -17.2842])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3971, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2843])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3972, Loss 2.927684\n",
      "Params: tensor([  5.3641, -17.2843])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3973, Loss 2.927682\n",
      "Params: tensor([  5.3641, -17.2843])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3974, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2844])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3975, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2844])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3976, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2844])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3977, Loss 2.927683\n",
      "Params: tensor([  5.3641, -17.2845])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3978, Loss 2.927682\n",
      "Params: tensor([  5.3641, -17.2845])\n",
      "Gradient: tensor([-0.0006,  0.0035])\n",
      "Epoch 3979, Loss 2.927682\n",
      "Params: tensor([  5.3641, -17.2845])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3980, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2846])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3981, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2846])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3982, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2847])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3983, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2847])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3984, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2847])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3985, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2848])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3986, Loss 2.927683\n",
      "Params: tensor([  5.3642, -17.2848])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3987, Loss 2.927680\n",
      "Params: tensor([  5.3642, -17.2848])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3988, Loss 2.927682\n",
      "Params: tensor([  5.3642, -17.2849])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3989, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2849])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3990, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2849])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3991, Loss 2.927680\n",
      "Params: tensor([  5.3642, -17.2850])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3992, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2850])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3993, Loss 2.927680\n",
      "Params: tensor([  5.3642, -17.2850])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3994, Loss 2.927680\n",
      "Params: tensor([  5.3642, -17.2851])\n",
      "Gradient: tensor([-0.0006,  0.0034])\n",
      "Epoch 3995, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2851])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 3996, Loss 2.927681\n",
      "Params: tensor([  5.3642, -17.2851])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 3997, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2852])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 3998, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2852])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 3999, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2852])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4000, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2853])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4001, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2853])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4002, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2853])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4003, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2854])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4004, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2854])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4005, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2854])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4006, Loss 2.927680\n",
      "Params: tensor([  5.3643, -17.2855])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4007, Loss 2.927677\n",
      "Params: tensor([  5.3643, -17.2855])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4008, Loss 2.927678\n",
      "Params: tensor([  5.3643, -17.2855])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4009, Loss 2.927678\n",
      "Params: tensor([  5.3643, -17.2856])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4010, Loss 2.927678\n",
      "Params: tensor([  5.3643, -17.2856])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4011, Loss 2.927678\n",
      "Params: tensor([  5.3643, -17.2856])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4012, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2857])\n",
      "Gradient: tensor([-0.0006,  0.0033])\n",
      "Epoch 4013, Loss 2.927679\n",
      "Params: tensor([  5.3643, -17.2857])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4014, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2857])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4015, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2857])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4016, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2858])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4017, Loss 2.927679\n",
      "Params: tensor([  5.3644, -17.2858])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4018, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2858])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4019, Loss 2.927678\n",
      "Params: tensor([  5.3644, -17.2859])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4020, Loss 2.927678\n",
      "Params: tensor([  5.3644, -17.2859])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4021, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2859])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4022, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2860])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4023, Loss 2.927678\n",
      "Params: tensor([  5.3644, -17.2860])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4024, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2860])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4025, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2861])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4026, Loss 2.927676\n",
      "Params: tensor([  5.3644, -17.2861])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4027, Loss 2.927676\n",
      "Params: tensor([  5.3644, -17.2861])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4028, Loss 2.927675\n",
      "Params: tensor([  5.3644, -17.2862])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4029, Loss 2.927677\n",
      "Params: tensor([  5.3644, -17.2862])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4030, Loss 2.927674\n",
      "Params: tensor([  5.3644, -17.2862])\n",
      "Gradient: tensor([-0.0006,  0.0032])\n",
      "Epoch 4031, Loss 2.927676\n",
      "Params: tensor([  5.3644, -17.2863])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4032, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2863])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4033, Loss 2.927676\n",
      "Params: tensor([  5.3645, -17.2863])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4034, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2864])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4035, Loss 2.927676\n",
      "Params: tensor([  5.3645, -17.2864])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4036, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2864])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4037, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2865])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4038, Loss 2.927677\n",
      "Params: tensor([  5.3645, -17.2865])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4039, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2865])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4040, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2865])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4041, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2866])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4042, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2866])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4043, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2866])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4044, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2867])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4045, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2867])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4046, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2867])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4047, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2868])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4048, Loss 2.927674\n",
      "Params: tensor([  5.3645, -17.2868])\n",
      "Gradient: tensor([-0.0006,  0.0031])\n",
      "Epoch 4049, Loss 2.927675\n",
      "Params: tensor([  5.3645, -17.2868])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4050, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2868])\n",
      "Gradient: tensor([-0.0005,  0.0031])\n",
      "Epoch 4051, Loss 2.927674\n",
      "Params: tensor([  5.3646, -17.2869])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4052, Loss 2.927675\n",
      "Params: tensor([  5.3646, -17.2869])\n",
      "Gradient: tensor([-0.0006,  0.0030])\n",
      "Epoch 4053, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2869])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4054, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2870])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4055, Loss 2.927674\n",
      "Params: tensor([  5.3646, -17.2870])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4056, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2870])\n",
      "Gradient: tensor([-0.0006,  0.0030])\n",
      "Epoch 4057, Loss 2.927674\n",
      "Params: tensor([  5.3646, -17.2871])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4058, Loss 2.927672\n",
      "Params: tensor([  5.3646, -17.2871])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4059, Loss 2.927674\n",
      "Params: tensor([  5.3646, -17.2871])\n",
      "Gradient: tensor([-0.0006,  0.0030])\n",
      "Epoch 4060, Loss 2.927675\n",
      "Params: tensor([  5.3646, -17.2872])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4061, Loss 2.927672\n",
      "Params: tensor([  5.3646, -17.2872])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4062, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2872])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4063, Loss 2.927674\n",
      "Params: tensor([  5.3646, -17.2872])\n",
      "Gradient: tensor([-0.0006,  0.0030])\n",
      "Epoch 4064, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2873])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4065, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2873])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4066, Loss 2.927672\n",
      "Params: tensor([  5.3646, -17.2873])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4067, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2874])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4068, Loss 2.927673\n",
      "Params: tensor([  5.3646, -17.2874])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4069, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2874])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4070, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2875])\n",
      "Gradient: tensor([-0.0005,  0.0030])\n",
      "Epoch 4071, Loss 2.927674\n",
      "Params: tensor([  5.3647, -17.2875])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4072, Loss 2.927673\n",
      "Params: tensor([  5.3647, -17.2875])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4073, Loss 2.927670\n",
      "Params: tensor([  5.3647, -17.2875])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4074, Loss 2.927673\n",
      "Params: tensor([  5.3647, -17.2876])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4075, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2876])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4076, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2876])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4077, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2877])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4078, Loss 2.927671\n",
      "Params: tensor([  5.3647, -17.2877])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4079, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2877])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4080, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2877])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4081, Loss 2.927671\n",
      "Params: tensor([  5.3647, -17.2878])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4082, Loss 2.927670\n",
      "Params: tensor([  5.3647, -17.2878])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4083, Loss 2.927673\n",
      "Params: tensor([  5.3647, -17.2878])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4084, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2879])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4085, Loss 2.927670\n",
      "Params: tensor([  5.3647, -17.2879])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4086, Loss 2.927670\n",
      "Params: tensor([  5.3647, -17.2879])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4087, Loss 2.927670\n",
      "Params: tensor([  5.3647, -17.2879])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4088, Loss 2.927672\n",
      "Params: tensor([  5.3647, -17.2880])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4089, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2880])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4090, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2880])\n",
      "Gradient: tensor([-0.0005,  0.0029])\n",
      "Epoch 4091, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2881])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4092, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2881])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4093, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2881])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4094, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2881])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4095, Loss 2.927669\n",
      "Params: tensor([  5.3648, -17.2882])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4096, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2882])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4097, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2882])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4098, Loss 2.927671\n",
      "Params: tensor([  5.3648, -17.2883])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4099, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2883])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4100, Loss 2.927671\n",
      "Params: tensor([  5.3648, -17.2883])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4101, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2883])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4102, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2884])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4103, Loss 2.927671\n",
      "Params: tensor([  5.3648, -17.2884])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4104, Loss 2.927669\n",
      "Params: tensor([  5.3648, -17.2884])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4105, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2885])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4106, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2885])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4107, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2885])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4108, Loss 2.927670\n",
      "Params: tensor([  5.3648, -17.2885])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4109, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2886])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4110, Loss 2.927669\n",
      "Params: tensor([  5.3649, -17.2886])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4111, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2886])\n",
      "Gradient: tensor([-0.0005,  0.0028])\n",
      "Epoch 4112, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2886])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4113, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2887])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4114, Loss 2.927669\n",
      "Params: tensor([  5.3649, -17.2887])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4115, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2887])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4116, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2887])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4117, Loss 2.927667\n",
      "Params: tensor([  5.3649, -17.2888])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4118, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2888])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4119, Loss 2.927669\n",
      "Params: tensor([  5.3649, -17.2888])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4120, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2889])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4121, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2889])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4122, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2889])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4123, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2889])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4124, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2890])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4125, Loss 2.927670\n",
      "Params: tensor([  5.3649, -17.2890])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4126, Loss 2.927666\n",
      "Params: tensor([  5.3649, -17.2890])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4127, Loss 2.927669\n",
      "Params: tensor([  5.3649, -17.2890])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4128, Loss 2.927668\n",
      "Params: tensor([  5.3649, -17.2891])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4129, Loss 2.927669\n",
      "Params: tensor([  5.3649, -17.2891])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4130, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2891])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4131, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2892])\n",
      "Gradient: tensor([-0.0004,  0.0027])\n",
      "Epoch 4132, Loss 2.927668\n",
      "Params: tensor([  5.3650, -17.2892])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4133, Loss 2.927668\n",
      "Params: tensor([  5.3650, -17.2892])\n",
      "Gradient: tensor([-0.0005,  0.0027])\n",
      "Epoch 4134, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2892])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4135, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2893])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4136, Loss 2.927666\n",
      "Params: tensor([  5.3650, -17.2893])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4137, Loss 2.927669\n",
      "Params: tensor([  5.3650, -17.2893])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4138, Loss 2.927666\n",
      "Params: tensor([  5.3650, -17.2893])\n",
      "Gradient: tensor([-0.0004,  0.0026])\n",
      "Epoch 4139, Loss 2.927668\n",
      "Params: tensor([  5.3650, -17.2894])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4140, Loss 2.927666\n",
      "Params: tensor([  5.3650, -17.2894])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4141, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2894])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4142, Loss 2.927668\n",
      "Params: tensor([  5.3650, -17.2894])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4143, Loss 2.927666\n",
      "Params: tensor([  5.3650, -17.2895])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4144, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2895])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4145, Loss 2.927666\n",
      "Params: tensor([  5.3650, -17.2895])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4146, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2896])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4147, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2896])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4148, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2896])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4149, Loss 2.927667\n",
      "Params: tensor([  5.3650, -17.2896])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4150, Loss 2.927665\n",
      "Params: tensor([  5.3650, -17.2897])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4151, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2897])\n",
      "Gradient: tensor([-0.0004,  0.0026])\n",
      "Epoch 4152, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2897])\n",
      "Gradient: tensor([-0.0004,  0.0026])\n",
      "Epoch 4153, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2897])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4154, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2898])\n",
      "Gradient: tensor([-0.0005,  0.0026])\n",
      "Epoch 4155, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2898])\n",
      "Gradient: tensor([-0.0004,  0.0026])\n",
      "Epoch 4156, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2898])\n",
      "Gradient: tensor([-0.0004,  0.0026])\n",
      "Epoch 4157, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2898])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4158, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2899])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4159, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2899])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4160, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2899])\n",
      "Gradient: tensor([-0.0005,  0.0025])\n",
      "Epoch 4161, Loss 2.927664\n",
      "Params: tensor([  5.3651, -17.2899])\n",
      "Gradient: tensor([-0.0005,  0.0025])\n",
      "Epoch 4162, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2900])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4163, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2900])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4164, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2900])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4165, Loss 2.927664\n",
      "Params: tensor([  5.3651, -17.2900])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4166, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2901])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4167, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2901])\n",
      "Gradient: tensor([-0.0005,  0.0025])\n",
      "Epoch 4168, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2901])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4169, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2901])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4170, Loss 2.927664\n",
      "Params: tensor([  5.3651, -17.2902])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4171, Loss 2.927665\n",
      "Params: tensor([  5.3651, -17.2902])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4172, Loss 2.927666\n",
      "Params: tensor([  5.3651, -17.2902])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4173, Loss 2.927663\n",
      "Params: tensor([  5.3651, -17.2902])\n",
      "Gradient: tensor([-0.0005,  0.0025])\n",
      "Epoch 4174, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2903])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4175, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2903])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4176, Loss 2.927665\n",
      "Params: tensor([  5.3652, -17.2903])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4177, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2903])\n",
      "Gradient: tensor([-0.0004,  0.0025])\n",
      "Epoch 4178, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2903])\n",
      "Gradient: tensor([-0.0005,  0.0025])\n",
      "Epoch 4179, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2904])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4180, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2904])\n",
      "Gradient: tensor([-0.0005,  0.0024])\n",
      "Epoch 4181, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2904])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4182, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2904])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4183, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2905])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4184, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2905])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4185, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2905])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4186, Loss 2.927662\n",
      "Params: tensor([  5.3652, -17.2905])\n",
      "Gradient: tensor([-0.0005,  0.0024])\n",
      "Epoch 4187, Loss 2.927665\n",
      "Params: tensor([  5.3652, -17.2906])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4188, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2906])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4189, Loss 2.927662\n",
      "Params: tensor([  5.3652, -17.2906])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4190, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2906])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4191, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2907])\n",
      "Gradient: tensor([-0.0005,  0.0024])\n",
      "Epoch 4192, Loss 2.927664\n",
      "Params: tensor([  5.3652, -17.2907])\n",
      "Gradient: tensor([-0.0005,  0.0024])\n",
      "Epoch 4193, Loss 2.927662\n",
      "Params: tensor([  5.3652, -17.2907])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4194, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2907])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4195, Loss 2.927663\n",
      "Params: tensor([  5.3652, -17.2908])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4196, Loss 2.927665\n",
      "Params: tensor([  5.3652, -17.2908])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4197, Loss 2.927664\n",
      "Params: tensor([  5.3653, -17.2908])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4198, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2908])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4199, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2909])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4200, Loss 2.927664\n",
      "Params: tensor([  5.3653, -17.2909])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4201, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2909])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4202, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2909])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4203, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2910])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4204, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2910])\n",
      "Gradient: tensor([-0.0004,  0.0024])\n",
      "Epoch 4205, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2910])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4206, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2910])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4207, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2910])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4208, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2911])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4209, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2911])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4210, Loss 2.927664\n",
      "Params: tensor([  5.3653, -17.2911])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4211, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2911])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4212, Loss 2.927660\n",
      "Params: tensor([  5.3653, -17.2912])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4213, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2912])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4214, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2912])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4215, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2912])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4216, Loss 2.927661\n",
      "Params: tensor([  5.3653, -17.2913])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4217, Loss 2.927660\n",
      "Params: tensor([  5.3653, -17.2913])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4218, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2913])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4219, Loss 2.927662\n",
      "Params: tensor([  5.3653, -17.2913])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4220, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2913])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4221, Loss 2.927663\n",
      "Params: tensor([  5.3653, -17.2914])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4222, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2914])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4223, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2914])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4224, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2914])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4225, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2915])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4226, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2915])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4227, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2915])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4228, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2915])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4229, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2915])\n",
      "Gradient: tensor([-0.0004,  0.0023])\n",
      "Epoch 4230, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2916])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4231, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2916])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4232, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2916])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4233, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2916])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4234, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2917])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4235, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2917])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4236, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2917])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4237, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2917])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4238, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2918])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4239, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2918])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4240, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2918])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4241, Loss 2.927662\n",
      "Params: tensor([  5.3654, -17.2918])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4242, Loss 2.927660\n",
      "Params: tensor([  5.3654, -17.2918])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4243, Loss 2.927659\n",
      "Params: tensor([  5.3654, -17.2919])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4244, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2919])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4245, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2919])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4246, Loss 2.927661\n",
      "Params: tensor([  5.3654, -17.2919])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4247, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2920])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4248, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2920])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4249, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2920])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4250, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2920])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4251, Loss 2.927662\n",
      "Params: tensor([  5.3655, -17.2920])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4252, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2921])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4253, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2921])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4254, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2921])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4255, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2921])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4256, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2921])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4257, Loss 2.927661\n",
      "Params: tensor([  5.3655, -17.2922])\n",
      "Gradient: tensor([-0.0004,  0.0022])\n",
      "Epoch 4258, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2922])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4259, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2922])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4260, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2922])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4261, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2922])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4262, Loss 2.927662\n",
      "Params: tensor([  5.3655, -17.2923])\n",
      "Gradient: tensor([-0.0003,  0.0021])\n",
      "Epoch 4263, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2923])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4264, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2923])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4265, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2923])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4266, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2924])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4267, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2924])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4268, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2924])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4269, Loss 2.927659\n",
      "Params: tensor([  5.3655, -17.2924])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4270, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2924])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4271, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2925])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4272, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2925])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4273, Loss 2.927660\n",
      "Params: tensor([  5.3655, -17.2925])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4274, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2925])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4275, Loss 2.927660\n",
      "Params: tensor([  5.3656, -17.2925])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4276, Loss 2.927660\n",
      "Params: tensor([  5.3656, -17.2926])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4277, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2926])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4278, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2926])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4279, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2926])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4280, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2926])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4281, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2927])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4282, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2927])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4283, Loss 2.927660\n",
      "Params: tensor([  5.3656, -17.2927])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4284, Loss 2.927660\n",
      "Params: tensor([  5.3656, -17.2927])\n",
      "Gradient: tensor([-0.0004,  0.0021])\n",
      "Epoch 4285, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2927])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4286, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2928])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4287, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2928])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4288, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2928])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4289, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2928])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4290, Loss 2.927657\n",
      "Params: tensor([  5.3656, -17.2929])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4291, Loss 2.927660\n",
      "Params: tensor([  5.3656, -17.2929])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4292, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2929])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4293, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2929])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4294, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2929])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4295, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2930])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4296, Loss 2.927657\n",
      "Params: tensor([  5.3656, -17.2930])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4297, Loss 2.927657\n",
      "Params: tensor([  5.3656, -17.2930])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4298, Loss 2.927657\n",
      "Params: tensor([  5.3656, -17.2930])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4299, Loss 2.927658\n",
      "Params: tensor([  5.3656, -17.2930])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4300, Loss 2.927659\n",
      "Params: tensor([  5.3656, -17.2931])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4301, Loss 2.927660\n",
      "Params: tensor([  5.3657, -17.2931])\n",
      "Gradient: tensor([-0.0004,  0.0020])\n",
      "Epoch 4302, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2931])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4303, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2931])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4304, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2931])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4305, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4306, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4307, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4308, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4309, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4310, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2932])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4311, Loss 2.927659\n",
      "Params: tensor([  5.3657, -17.2933])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4312, Loss 2.927659\n",
      "Params: tensor([  5.3657, -17.2933])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4313, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2933])\n",
      "Gradient: tensor([-0.0003,  0.0020])\n",
      "Epoch 4314, Loss 2.927656\n",
      "Params: tensor([  5.3657, -17.2933])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4315, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2933])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4316, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2934])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4317, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2934])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4318, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2934])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4319, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2934])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4320, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2934])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4321, Loss 2.927655\n",
      "Params: tensor([  5.3657, -17.2935])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4322, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2935])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4323, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2935])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4324, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2935])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4325, Loss 2.927658\n",
      "Params: tensor([  5.3657, -17.2935])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4326, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4327, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4328, Loss 2.927657\n",
      "Params: tensor([  5.3657, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4329, Loss 2.927656\n",
      "Params: tensor([  5.3657, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4330, Loss 2.927656\n",
      "Params: tensor([  5.3657, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4331, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2936])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4332, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2937])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4333, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2937])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4334, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2937])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4335, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2937])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4336, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2937])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4337, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2938])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4338, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2938])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4339, Loss 2.927655\n",
      "Params: tensor([  5.3658, -17.2938])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4340, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2938])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4341, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2938])\n",
      "Gradient: tensor([-0.0003,  0.0019])\n",
      "Epoch 4342, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2939])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4343, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2939])\n",
      "Gradient: tensor([-0.0004,  0.0019])\n",
      "Epoch 4344, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2939])\n",
      "Gradient: tensor([-0.0004,  0.0018])\n",
      "Epoch 4345, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2939])\n",
      "Gradient: tensor([-0.0004,  0.0018])\n",
      "Epoch 4346, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2939])\n",
      "Gradient: tensor([-0.0004,  0.0018])\n",
      "Epoch 4347, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2940])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4348, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2940])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4349, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2940])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4350, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2940])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4351, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2940])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4352, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4353, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4354, Loss 2.927655\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4355, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4356, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4357, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2941])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4358, Loss 2.927657\n",
      "Params: tensor([  5.3658, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4359, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4360, Loss 2.927656\n",
      "Params: tensor([  5.3658, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4361, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4362, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4363, Loss 2.927657\n",
      "Params: tensor([  5.3659, -17.2942])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4364, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4365, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4366, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4367, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4368, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4369, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2943])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4370, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2944])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4371, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2944])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4372, Loss 2.927657\n",
      "Params: tensor([  5.3659, -17.2944])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4373, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2944])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4374, Loss 2.927657\n",
      "Params: tensor([  5.3659, -17.2944])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4375, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4376, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4377, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4378, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0018])\n",
      "Epoch 4379, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4380, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2945])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4381, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4382, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4383, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4384, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4385, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4386, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2946])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4387, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4388, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4389, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4390, Loss 2.927654\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4391, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4392, Loss 2.927656\n",
      "Params: tensor([  5.3659, -17.2947])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4393, Loss 2.927655\n",
      "Params: tensor([  5.3659, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4394, Loss 2.927656\n",
      "Params: tensor([  5.3660, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4395, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4396, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4397, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4398, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2948])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4399, Loss 2.927656\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4400, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4401, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4402, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4403, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4404, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2949])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4405, Loss 2.927656\n",
      "Params: tensor([  5.3660, -17.2950])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4406, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2950])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4407, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2950])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4408, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2950])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4409, Loss 2.927653\n",
      "Params: tensor([  5.3660, -17.2950])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4410, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4411, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0017])\n",
      "Epoch 4412, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4413, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4414, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4415, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2951])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4416, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4417, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4418, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4419, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4420, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4421, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2952])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4422, Loss 2.927653\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4423, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4424, Loss 2.927653\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4425, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4426, Loss 2.927655\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4427, Loss 2.927654\n",
      "Params: tensor([  5.3660, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4428, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2953])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4429, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4430, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4431, Loss 2.927652\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4432, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4433, Loss 2.927655\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4434, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2954])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4435, Loss 2.927655\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4436, Loss 2.927652\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4437, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4438, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4439, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4440, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4441, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2955])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4442, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4443, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0002,  0.0016])\n",
      "Epoch 4444, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4445, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4446, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4447, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2956])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4448, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4449, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0016])\n",
      "Epoch 4450, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4451, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4452, Loss 2.927651\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4453, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4454, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2957])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4455, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4456, Loss 2.927655\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4457, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4458, Loss 2.927652\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4459, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4460, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2958])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4461, Loss 2.927654\n",
      "Params: tensor([  5.3661, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4462, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4463, Loss 2.927652\n",
      "Params: tensor([  5.3661, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4464, Loss 2.927653\n",
      "Params: tensor([  5.3661, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4465, Loss 2.927654\n",
      "Params: tensor([  5.3662, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4466, Loss 2.927654\n",
      "Params: tensor([  5.3662, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4467, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2959])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4468, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4469, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4470, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4471, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4472, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4473, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4474, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2960])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4475, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4476, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4477, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4478, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4479, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4480, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2961])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4481, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4482, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4483, Loss 2.927654\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4484, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4485, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4486, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4487, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2962])\n",
      "Gradient: tensor([-0.0003,  0.0015])\n",
      "Epoch 4488, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4489, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4490, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4491, Loss 2.927651\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4492, Loss 2.927651\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4493, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2963])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4494, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4495, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4496, Loss 2.927651\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0003,  0.0014])\n",
      "Epoch 4497, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4498, Loss 2.927652\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4499, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4500, Loss 2.927651\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4501, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2964])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4502, Loss 2.927653\n",
      "Params: tensor([  5.3662, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4503, Loss 2.927653\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4504, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4505, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4506, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4507, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4508, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2965])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4509, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4510, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4511, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4512, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4513, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4514, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4515, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4516, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2966])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4517, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4518, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4519, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4520, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4521, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4522, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4523, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2967])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4524, Loss 2.927653\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4525, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4526, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4527, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4528, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4529, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4530, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2968])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4531, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0014])\n",
      "Epoch 4532, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4533, Loss 2.927652\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4534, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4535, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4536, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4537, Loss 2.927653\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4538, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2969])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4539, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4540, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4541, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4542, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4543, Loss 2.927651\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4544, Loss 2.927650\n",
      "Params: tensor([  5.3663, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4545, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2970])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4546, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4547, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4548, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4549, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4550, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4551, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4552, Loss 2.927653\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4553, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2971])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4554, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4555, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4556, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4557, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4558, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4559, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4560, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2972])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4561, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4562, Loss 2.927653\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4563, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4564, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4565, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4566, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4567, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4568, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2973])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4569, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4570, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4571, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4572, Loss 2.927649\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4573, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0013])\n",
      "Epoch 4574, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4575, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2974])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4576, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4577, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4578, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4579, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4580, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4581, Loss 2.927649\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4582, Loss 2.927652\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4583, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2975])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4584, Loss 2.927649\n",
      "Params: tensor([  5.3664, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4585, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4586, Loss 2.927650\n",
      "Params: tensor([  5.3664, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4587, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4588, Loss 2.927651\n",
      "Params: tensor([  5.3664, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4589, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4590, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4591, Loss 2.927652\n",
      "Params: tensor([  5.3665, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4592, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2976])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4593, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4594, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4595, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4596, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4597, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4598, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4599, Loss 2.927652\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4600, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4601, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2977])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4602, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4603, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4604, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4605, Loss 2.927652\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4606, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4607, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4608, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4609, Loss 2.927652\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4610, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2978])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4611, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4612, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4613, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4614, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4615, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4616, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4617, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4618, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2979])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4619, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4620, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4621, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4622, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4623, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4624, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4625, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4626, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0012])\n",
      "Epoch 4627, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2980])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4628, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4629, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4630, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4631, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4632, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4633, Loss 2.927651\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4634, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4635, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4636, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2981])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4637, Loss 2.927649\n",
      "Params: tensor([  5.3665, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4638, Loss 2.927650\n",
      "Params: tensor([  5.3665, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4639, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4640, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4641, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4642, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4643, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4644, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4645, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2982])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4646, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4647, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4648, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4649, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4650, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4651, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4652, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4653, Loss 2.927651\n",
      "Params: tensor([  5.3666, -17.2983])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4654, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4655, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4656, Loss 2.927651\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4657, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4658, Loss 2.927651\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4659, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4660, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4661, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4662, Loss 2.927648\n",
      "Params: tensor([  5.3666, -17.2984])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4663, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4664, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4665, Loss 2.927648\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4666, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4667, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4668, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4669, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4670, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4671, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2985])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4672, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4673, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4674, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4675, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0011])\n",
      "Epoch 4676, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4677, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4678, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4679, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4680, Loss 2.927648\n",
      "Params: tensor([  5.3666, -17.2986])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4681, Loss 2.927648\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4682, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4683, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4684, Loss 2.927648\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4685, Loss 2.927650\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4686, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4687, Loss 2.927651\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4688, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4689, Loss 2.927649\n",
      "Params: tensor([  5.3666, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4690, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4691, Loss 2.927651\n",
      "Params: tensor([  5.3667, -17.2987])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4692, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4693, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4694, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4695, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4696, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4697, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4698, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4699, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4700, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4701, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2988])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4702, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4703, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4704, Loss 2.927647\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4705, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4706, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4707, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4708, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4709, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4710, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4711, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4712, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2989])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4713, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4714, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4715, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4716, Loss 2.927647\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4717, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4718, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4719, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4720, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4721, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4722, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2990])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4723, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4724, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4725, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4726, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4727, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4728, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4729, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4730, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4731, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4732, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4733, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2991])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4734, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4735, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4736, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4737, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4738, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0010])\n",
      "Epoch 4739, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4740, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4741, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4742, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4743, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2992])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4744, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4745, Loss 2.927650\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4746, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4747, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4748, Loss 2.927648\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4749, Loss 2.927649\n",
      "Params: tensor([  5.3667, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4750, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4751, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4752, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4753, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4754, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2993])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4755, Loss 2.927647\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4756, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4757, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4758, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4759, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4760, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4761, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4762, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4763, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4764, Loss 2.927647\n",
      "Params: tensor([  5.3668, -17.2994])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4765, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4766, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4767, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4768, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4769, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4770, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4771, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4772, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4773, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4774, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4775, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2995])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4776, Loss 2.927647\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4777, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4778, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4779, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4780, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4781, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4782, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4783, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4784, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4785, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2996])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4786, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4787, Loss 2.927650\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4788, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4789, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4790, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4791, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4792, Loss 2.927647\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4793, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4794, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4795, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4796, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4797, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2997])\n",
      "Gradient: tensor([-0.0002,  0.0009])\n",
      "Epoch 4798, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4799, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4800, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4801, Loss 2.927646\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4802, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4803, Loss 2.927647\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0009])\n",
      "Epoch 4804, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4805, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4806, Loss 2.927646\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4807, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4808, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4809, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4810, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2998])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4811, Loss 2.927648\n",
      "Params: tensor([  5.3668, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4812, Loss 2.927649\n",
      "Params: tensor([  5.3668, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4813, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4814, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4815, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4816, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4817, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4818, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4819, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4820, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4821, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4822, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4823, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.2999])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4824, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4825, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4826, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4827, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4828, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4829, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4830, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4831, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4832, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4833, Loss 2.927646\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4834, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4835, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4836, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3000])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4837, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4838, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4839, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4840, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4841, Loss 2.927650\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4842, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4843, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4844, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4845, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4846, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4847, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4848, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4849, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3001])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4850, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4851, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4852, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4853, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4854, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4855, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4856, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4857, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4858, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4859, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4860, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4861, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4862, Loss 2.927646\n",
      "Params: tensor([  5.3669, -17.3002])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4863, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4864, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4865, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4866, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4867, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4868, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4869, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4870, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4871, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4872, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4873, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4874, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4875, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3003])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4876, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4877, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4878, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4879, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4880, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4881, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0008])\n",
      "Epoch 4882, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4883, Loss 2.927648\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4884, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4885, Loss 2.927647\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4886, Loss 2.927649\n",
      "Params: tensor([  5.3669, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4887, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4888, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3004])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4889, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4890, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4891, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4892, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4893, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4894, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4895, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4896, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4897, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4898, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4899, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4900, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4901, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3005])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4902, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4903, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4904, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4905, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4906, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4907, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4908, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4909, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4910, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4911, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4912, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4913, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4914, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4915, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3006])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4916, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4917, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4918, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4919, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4920, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4921, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4922, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4923, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4924, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4925, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4926, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4927, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4928, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3007])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4929, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4930, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4931, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4932, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4933, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4934, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4935, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4936, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4937, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4938, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4939, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4940, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4941, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3008])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4942, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4943, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4944, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4945, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4946, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4947, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4948, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4949, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4950, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-9.9182e-05,  6.6358e-04])\n",
      "Epoch 4951, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-9.5367e-05,  6.6286e-04])\n",
      "Epoch 4952, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-9.6798e-05,  6.6179e-04])\n",
      "Epoch 4953, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4954, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4955, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4956, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4957, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4958, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3009])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4959, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-9.8228e-05,  6.5470e-04])\n",
      "Epoch 4960, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4961, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-9.7990e-05,  6.5273e-04])\n",
      "Epoch 4962, Loss 2.927646\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0007])\n",
      "Epoch 4963, Loss 2.927649\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4964, Loss 2.927647\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4965, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4966, Loss 2.927648\n",
      "Params: tensor([  5.3670, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4967, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-9.7752e-05,  6.4695e-04])\n",
      "Epoch 4968, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-9.7513e-05,  6.4576e-04])\n",
      "Epoch 4969, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4970, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4971, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4972, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4973, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4974, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4975, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3010])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4976, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4977, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-9.5844e-05,  6.3682e-04])\n",
      "Epoch 4978, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-9.7752e-05,  6.3533e-04])\n",
      "Epoch 4979, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4980, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4981, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4982, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4983, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4984, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4985, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4986, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-9.6798e-05,  6.2823e-04])\n",
      "Epoch 4987, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4988, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4989, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4990, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4991, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4992, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4993, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3011])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4994, Loss 2.927646\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-9.2506e-05,  6.2042e-04])\n",
      "Epoch 4995, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-9.9182e-05,  6.1846e-04])\n",
      "Epoch 4996, Loss 2.927648\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4997, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4998, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 4999, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n",
      "Epoch 5000, Loss 2.927647\n",
      "Params: tensor([  5.3671, -17.3012])\n",
      "Gradient: tensor([-0.0001,  0.0006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_un = 0.1 * t_u\n",
    "\n",
    "params = training_loop(n_epochs=5000, learning_rate=1e-2, params=torch.tensor([1.0, 0.0]), t_u = t_un, t_c=t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "341a25b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x149088690>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as pyplot\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4), dpi=600)\n",
    "plt.xlabel(\"Temprature (Fehrenheit)\")\n",
    "plt.ylabel(\"Temprature (Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
